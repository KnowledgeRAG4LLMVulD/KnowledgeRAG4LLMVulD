[
    {
        "cve_id": "CVE-2014-0181",
        "code_before_change": "static int crypto_user_rcv_msg(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct nlattr *attrs[CRYPTOCFGA_MAX+1];\n\tconst struct crypto_link *link;\n\tint type, err;\n\n\ttype = nlh->nlmsg_type;\n\tif (type > CRYPTO_MSG_MAX)\n\t\treturn -EINVAL;\n\n\ttype -= CRYPTO_MSG_BASE;\n\tlink = &crypto_dispatch[type];\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((type == (CRYPTO_MSG_GETALG - CRYPTO_MSG_BASE) &&\n\t    (nlh->nlmsg_flags & NLM_F_DUMP))) {\n\t\tstruct crypto_alg *alg;\n\t\tu16 dump_alloc = 0;\n\n\t\tif (link->dump == NULL)\n\t\t\treturn -EINVAL;\n\n\t\tlist_for_each_entry(alg, &crypto_alg_list, cra_list)\n\t\t\tdump_alloc += CRYPTO_REPORT_MAXSIZE;\n\n\t\t{\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.dump = link->dump,\n\t\t\t\t.done = link->done,\n\t\t\t\t.min_dump_alloc = dump_alloc,\n\t\t\t};\n\t\t\treturn netlink_dump_start(crypto_nlsk, skb, nlh, &c);\n\t\t}\n\t}\n\n\terr = nlmsg_parse(nlh, crypto_msg_min[type], attrs, CRYPTOCFGA_MAX,\n\t\t\t  crypto_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (link->doit == NULL)\n\t\treturn -EINVAL;\n\n\treturn link->doit(skb, nlh, attrs);\n}",
        "code_after_change": "static int crypto_user_rcv_msg(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct nlattr *attrs[CRYPTOCFGA_MAX+1];\n\tconst struct crypto_link *link;\n\tint type, err;\n\n\ttype = nlh->nlmsg_type;\n\tif (type > CRYPTO_MSG_MAX)\n\t\treturn -EINVAL;\n\n\ttype -= CRYPTO_MSG_BASE;\n\tlink = &crypto_dispatch[type];\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((type == (CRYPTO_MSG_GETALG - CRYPTO_MSG_BASE) &&\n\t    (nlh->nlmsg_flags & NLM_F_DUMP))) {\n\t\tstruct crypto_alg *alg;\n\t\tu16 dump_alloc = 0;\n\n\t\tif (link->dump == NULL)\n\t\t\treturn -EINVAL;\n\n\t\tlist_for_each_entry(alg, &crypto_alg_list, cra_list)\n\t\t\tdump_alloc += CRYPTO_REPORT_MAXSIZE;\n\n\t\t{\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.dump = link->dump,\n\t\t\t\t.done = link->done,\n\t\t\t\t.min_dump_alloc = dump_alloc,\n\t\t\t};\n\t\t\treturn netlink_dump_start(crypto_nlsk, skb, nlh, &c);\n\t\t}\n\t}\n\n\terr = nlmsg_parse(nlh, crypto_msg_min[type], attrs, CRYPTOCFGA_MAX,\n\t\t\t  crypto_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (link->doit == NULL)\n\t\treturn -EINVAL;\n\n\treturn link->doit(skb, nlh, attrs);\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,7 @@\n \ttype -= CRYPTO_MSG_BASE;\n \tlink = &crypto_dispatch[type];\n \n-\tif (!capable(CAP_NET_ADMIN))\n+\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n \t\treturn -EPERM;\n \n \tif ((type == (CRYPTO_MSG_GETALG - CRYPTO_MSG_BASE) &&",
        "function_modified_lines": {
            "added": [
                "\tif (!netlink_capable(skb, CAP_NET_ADMIN))"
            ],
            "deleted": [
                "\tif (!capable(CAP_NET_ADMIN))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Netlink implementation in the Linux kernel through 3.14.1 does not provide a mechanism for authorizing socket operations based on the opener of a socket, which allows local users to bypass intended access restrictions and modify network configurations by using a Netlink socket for the (1) stdout or (2) stderr of a setuid program.",
        "id": 433
    },
    {
        "cve_id": "CVE-2014-1438",
        "code_before_change": "static inline int restore_fpu_checking(struct task_struct *tsk)\n{\n\t/* AMD K7/K8 CPUs don't save/restore FDP/FIP/FOP unless an exception\n\t   is pending.  Clear the x87 state here by setting it to fixed\n\t   values. \"m\" is a random variable that should be in L1 */\n\talternative_input(\n\t\tASM_NOP8 ASM_NOP2,\n\t\t\"emms\\n\\t\"\t\t/* clear stack tags */\n\t\t\"fildl %P[addr]\",\t/* set F?P to defined value */\n\t\tX86_FEATURE_FXSAVE_LEAK,\n\t\t[addr] \"m\" (tsk->thread.fpu.has_fpu));\n\n\treturn fpu_restore_checking(&tsk->thread.fpu);\n}",
        "code_after_change": "static inline int restore_fpu_checking(struct task_struct *tsk)\n{\n\t/* AMD K7/K8 CPUs don't save/restore FDP/FIP/FOP unless an exception\n\t   is pending.  Clear the x87 state here by setting it to fixed\n\t   values. \"m\" is a random variable that should be in L1 */\n\tif (unlikely(static_cpu_has(X86_FEATURE_FXSAVE_LEAK))) {\n\t\tasm volatile(\n\t\t\t\"fnclex\\n\\t\"\n\t\t\t\"emms\\n\\t\"\n\t\t\t\"fildl %P[addr]\"\t/* set F?P to defined value */\n\t\t\t: : [addr] \"m\" (tsk->thread.fpu.has_fpu));\n\t}\n\n\treturn fpu_restore_checking(&tsk->thread.fpu);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,12 +3,13 @@\n \t/* AMD K7/K8 CPUs don't save/restore FDP/FIP/FOP unless an exception\n \t   is pending.  Clear the x87 state here by setting it to fixed\n \t   values. \"m\" is a random variable that should be in L1 */\n-\talternative_input(\n-\t\tASM_NOP8 ASM_NOP2,\n-\t\t\"emms\\n\\t\"\t\t/* clear stack tags */\n-\t\t\"fildl %P[addr]\",\t/* set F?P to defined value */\n-\t\tX86_FEATURE_FXSAVE_LEAK,\n-\t\t[addr] \"m\" (tsk->thread.fpu.has_fpu));\n+\tif (unlikely(static_cpu_has(X86_FEATURE_FXSAVE_LEAK))) {\n+\t\tasm volatile(\n+\t\t\t\"fnclex\\n\\t\"\n+\t\t\t\"emms\\n\\t\"\n+\t\t\t\"fildl %P[addr]\"\t/* set F?P to defined value */\n+\t\t\t: : [addr] \"m\" (tsk->thread.fpu.has_fpu));\n+\t}\n \n \treturn fpu_restore_checking(&tsk->thread.fpu);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (unlikely(static_cpu_has(X86_FEATURE_FXSAVE_LEAK))) {",
                "\t\tasm volatile(",
                "\t\t\t\"fnclex\\n\\t\"",
                "\t\t\t\"emms\\n\\t\"",
                "\t\t\t\"fildl %P[addr]\"\t/* set F?P to defined value */",
                "\t\t\t: : [addr] \"m\" (tsk->thread.fpu.has_fpu));",
                "\t}"
            ],
            "deleted": [
                "\talternative_input(",
                "\t\tASM_NOP8 ASM_NOP2,",
                "\t\t\"emms\\n\\t\"\t\t/* clear stack tags */",
                "\t\t\"fildl %P[addr]\",\t/* set F?P to defined value */",
                "\t\tX86_FEATURE_FXSAVE_LEAK,",
                "\t\t[addr] \"m\" (tsk->thread.fpu.has_fpu));"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The restore_fpu_checking function in arch/x86/include/asm/fpu-internal.h in the Linux kernel before 3.12.8 on the AMD K7 and K8 platforms does not clear pending exceptions before proceeding to an EMMS instruction, which allows local users to cause a denial of service (task kill) or possibly gain privileges via a crafted application.",
        "id": 470
    },
    {
        "cve_id": "CVE-2015-8709",
        "code_before_change": "void __mmdrop(struct mm_struct *mm)\n{\n\tBUG_ON(mm == &init_mm);\n\tmm_free_pgd(mm);\n\tdestroy_context(mm);\n\tmmu_notifier_mm_destroy(mm);\n\tcheck_mm(mm);\n\tfree_mm(mm);\n}",
        "code_after_change": "void __mmdrop(struct mm_struct *mm)\n{\n\tBUG_ON(mm == &init_mm);\n\tmm_free_pgd(mm);\n\tdestroy_context(mm);\n\tmmu_notifier_mm_destroy(mm);\n\tcheck_mm(mm);\n\tput_user_ns(mm->user_ns);\n\tfree_mm(mm);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,5 +5,6 @@\n \tdestroy_context(mm);\n \tmmu_notifier_mm_destroy(mm);\n \tcheck_mm(mm);\n+\tput_user_ns(mm->user_ns);\n \tfree_mm(mm);\n }",
        "function_modified_lines": {
            "added": [
                "\tput_user_ns(mm->user_ns);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "kernel/ptrace.c in the Linux kernel through 4.4.1 mishandles uid and gid mappings, which allows local users to gain privileges by establishing a user namespace, waiting for a root process to enter that namespace with an unsafe uid or gid, and then using the ptrace system call.  NOTE: the vendor states \"there is no kernel bug here.",
        "id": 835
    },
    {
        "cve_id": "CVE-2016-6786",
        "code_before_change": "void perf_event_enable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct task_struct *task = ctx->task;\n\n\tif (!task) {\n\t\t/*\n\t\t * Enable the event on the cpu that it's on\n\t\t */\n\t\tcpu_function_call(event->cpu, __perf_event_enable, event);\n\t\treturn;\n\t}\n\n\traw_spin_lock_irq(&ctx->lock);\n\tif (event->state >= PERF_EVENT_STATE_INACTIVE)\n\t\tgoto out;\n\n\t/*\n\t * If the event is in error state, clear that first.\n\t * That way, if we see the event in error state below, we\n\t * know that it has gone back into error state, as distinct\n\t * from the task having been scheduled away before the\n\t * cross-call arrived.\n\t */\n\tif (event->state == PERF_EVENT_STATE_ERROR)\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\nretry:\n\tif (!ctx->is_active) {\n\t\t__perf_event_mark_enabled(event);\n\t\tgoto out;\n\t}\n\n\traw_spin_unlock_irq(&ctx->lock);\n\n\tif (!task_function_call(task, __perf_event_enable, event))\n\t\treturn;\n\n\traw_spin_lock_irq(&ctx->lock);\n\n\t/*\n\t * If the context is active and the event is still off,\n\t * we need to retry the cross-call.\n\t */\n\tif (ctx->is_active && event->state == PERF_EVENT_STATE_OFF) {\n\t\t/*\n\t\t * task could have been flipped by a concurrent\n\t\t * perf_event_context_sched_out()\n\t\t */\n\t\ttask = ctx->task;\n\t\tgoto retry;\n\t}\n\nout:\n\traw_spin_unlock_irq(&ctx->lock);\n}",
        "code_after_change": "void perf_event_enable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx;\n\n\tctx = perf_event_ctx_lock(event);\n\t_perf_event_enable(event);\n\tperf_event_ctx_unlock(event, ctx);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,56 +1,8 @@\n void perf_event_enable(struct perf_event *event)\n {\n-\tstruct perf_event_context *ctx = event->ctx;\n-\tstruct task_struct *task = ctx->task;\n+\tstruct perf_event_context *ctx;\n \n-\tif (!task) {\n-\t\t/*\n-\t\t * Enable the event on the cpu that it's on\n-\t\t */\n-\t\tcpu_function_call(event->cpu, __perf_event_enable, event);\n-\t\treturn;\n-\t}\n-\n-\traw_spin_lock_irq(&ctx->lock);\n-\tif (event->state >= PERF_EVENT_STATE_INACTIVE)\n-\t\tgoto out;\n-\n-\t/*\n-\t * If the event is in error state, clear that first.\n-\t * That way, if we see the event in error state below, we\n-\t * know that it has gone back into error state, as distinct\n-\t * from the task having been scheduled away before the\n-\t * cross-call arrived.\n-\t */\n-\tif (event->state == PERF_EVENT_STATE_ERROR)\n-\t\tevent->state = PERF_EVENT_STATE_OFF;\n-\n-retry:\n-\tif (!ctx->is_active) {\n-\t\t__perf_event_mark_enabled(event);\n-\t\tgoto out;\n-\t}\n-\n-\traw_spin_unlock_irq(&ctx->lock);\n-\n-\tif (!task_function_call(task, __perf_event_enable, event))\n-\t\treturn;\n-\n-\traw_spin_lock_irq(&ctx->lock);\n-\n-\t/*\n-\t * If the context is active and the event is still off,\n-\t * we need to retry the cross-call.\n-\t */\n-\tif (ctx->is_active && event->state == PERF_EVENT_STATE_OFF) {\n-\t\t/*\n-\t\t * task could have been flipped by a concurrent\n-\t\t * perf_event_context_sched_out()\n-\t\t */\n-\t\ttask = ctx->task;\n-\t\tgoto retry;\n-\t}\n-\n-out:\n-\traw_spin_unlock_irq(&ctx->lock);\n+\tctx = perf_event_ctx_lock(event);\n+\t_perf_event_enable(event);\n+\tperf_event_ctx_unlock(event, ctx);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct perf_event_context *ctx;",
                "\tctx = perf_event_ctx_lock(event);",
                "\t_perf_event_enable(event);",
                "\tperf_event_ctx_unlock(event, ctx);"
            ],
            "deleted": [
                "\tstruct perf_event_context *ctx = event->ctx;",
                "\tstruct task_struct *task = ctx->task;",
                "\tif (!task) {",
                "\t\t/*",
                "\t\t * Enable the event on the cpu that it's on",
                "\t\t */",
                "\t\tcpu_function_call(event->cpu, __perf_event_enable, event);",
                "\t\treturn;",
                "\t}",
                "",
                "\traw_spin_lock_irq(&ctx->lock);",
                "\tif (event->state >= PERF_EVENT_STATE_INACTIVE)",
                "\t\tgoto out;",
                "",
                "\t/*",
                "\t * If the event is in error state, clear that first.",
                "\t * That way, if we see the event in error state below, we",
                "\t * know that it has gone back into error state, as distinct",
                "\t * from the task having been scheduled away before the",
                "\t * cross-call arrived.",
                "\t */",
                "\tif (event->state == PERF_EVENT_STATE_ERROR)",
                "\t\tevent->state = PERF_EVENT_STATE_OFF;",
                "",
                "retry:",
                "\tif (!ctx->is_active) {",
                "\t\t__perf_event_mark_enabled(event);",
                "\t\tgoto out;",
                "\t}",
                "",
                "\traw_spin_unlock_irq(&ctx->lock);",
                "",
                "\tif (!task_function_call(task, __perf_event_enable, event))",
                "\t\treturn;",
                "",
                "\traw_spin_lock_irq(&ctx->lock);",
                "",
                "\t/*",
                "\t * If the context is active and the event is still off,",
                "\t * we need to retry the cross-call.",
                "\t */",
                "\tif (ctx->is_active && event->state == PERF_EVENT_STATE_OFF) {",
                "\t\t/*",
                "\t\t * task could have been flipped by a concurrent",
                "\t\t * perf_event_context_sched_out()",
                "\t\t */",
                "\t\ttask = ctx->task;",
                "\t\tgoto retry;",
                "\t}",
                "",
                "out:",
                "\traw_spin_unlock_irq(&ctx->lock);"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "kernel/events/core.c in the performance subsystem in the Linux kernel before 4.0 mismanages locks during certain migrations, which allows local users to gain privileges via a crafted application, aka Android internal bug 30955111.",
        "id": 1086
    },
    {
        "cve_id": "CVE-2016-3857",
        "code_before_change": "asmlinkage long sys_oabi_epoll_wait(int epfd,\n\t\t\t\t    struct oabi_epoll_event __user *events,\n\t\t\t\t    int maxevents, int timeout)\n{\n\tstruct epoll_event *kbuf;\n\tmm_segment_t fs;\n\tlong ret, err, i;\n\n\tif (maxevents <= 0 || maxevents > (INT_MAX/sizeof(struct epoll_event)))\n\t\treturn -EINVAL;\n\tkbuf = kmalloc(sizeof(*kbuf) * maxevents, GFP_KERNEL);\n\tif (!kbuf)\n\t\treturn -ENOMEM;\n\tfs = get_fs();\n\tset_fs(KERNEL_DS);\n\tret = sys_epoll_wait(epfd, kbuf, maxevents, timeout);\n\tset_fs(fs);\n\terr = 0;\n\tfor (i = 0; i < ret; i++) {\n\t\t__put_user_error(kbuf[i].events, &events->events, err);\n\t\t__put_user_error(kbuf[i].data,   &events->data,   err);\n\t\tevents++;\n\t}\n\tkfree(kbuf);\n\treturn err ? -EFAULT : ret;\n}",
        "code_after_change": "asmlinkage long sys_oabi_epoll_wait(int epfd,\n\t\t\t\t    struct oabi_epoll_event __user *events,\n\t\t\t\t    int maxevents, int timeout)\n{\n\tstruct epoll_event *kbuf;\n\tmm_segment_t fs;\n\tlong ret, err, i;\n\n\tif (maxevents <= 0 ||\n\t\t\tmaxevents > (INT_MAX/sizeof(*kbuf)) ||\n\t\t\tmaxevents > (INT_MAX/sizeof(*events)))\n\t\treturn -EINVAL;\n\tif (!access_ok(VERIFY_WRITE, events, sizeof(*events) * maxevents))\n\t\treturn -EFAULT;\n\tkbuf = kmalloc(sizeof(*kbuf) * maxevents, GFP_KERNEL);\n\tif (!kbuf)\n\t\treturn -ENOMEM;\n\tfs = get_fs();\n\tset_fs(KERNEL_DS);\n\tret = sys_epoll_wait(epfd, kbuf, maxevents, timeout);\n\tset_fs(fs);\n\terr = 0;\n\tfor (i = 0; i < ret; i++) {\n\t\t__put_user_error(kbuf[i].events, &events->events, err);\n\t\t__put_user_error(kbuf[i].data,   &events->data,   err);\n\t\tevents++;\n\t}\n\tkfree(kbuf);\n\treturn err ? -EFAULT : ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,8 +6,12 @@\n \tmm_segment_t fs;\n \tlong ret, err, i;\n \n-\tif (maxevents <= 0 || maxevents > (INT_MAX/sizeof(struct epoll_event)))\n+\tif (maxevents <= 0 ||\n+\t\t\tmaxevents > (INT_MAX/sizeof(*kbuf)) ||\n+\t\t\tmaxevents > (INT_MAX/sizeof(*events)))\n \t\treturn -EINVAL;\n+\tif (!access_ok(VERIFY_WRITE, events, sizeof(*events) * maxevents))\n+\t\treturn -EFAULT;\n \tkbuf = kmalloc(sizeof(*kbuf) * maxevents, GFP_KERNEL);\n \tif (!kbuf)\n \t\treturn -ENOMEM;",
        "function_modified_lines": {
            "added": [
                "\tif (maxevents <= 0 ||",
                "\t\t\tmaxevents > (INT_MAX/sizeof(*kbuf)) ||",
                "\t\t\tmaxevents > (INT_MAX/sizeof(*events)))",
                "\tif (!access_ok(VERIFY_WRITE, events, sizeof(*events) * maxevents))",
                "\t\treturn -EFAULT;"
            ],
            "deleted": [
                "\tif (maxevents <= 0 || maxevents > (INT_MAX/sizeof(struct epoll_event)))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The kernel in Android before 2016-08-05 on Nexus 7 (2013) devices allows attackers to gain privileges via a crafted application, aka internal bug 28522518.",
        "id": 1010
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static int dccp_v6_send_response(const struct sock *sk, struct request_sock *req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct in6_addr *final_p, final;\n\tstruct flowi6 fl6;\n\tint err = -1;\n\tstruct dst_entry *dst;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\tfl6.saddr = ireq->ir_v6_loc_addr;\n\tfl6.flowlabel = 0;\n\tfl6.flowi6_oif = ireq->ir_iif;\n\tfl6.fl6_dport = ireq->ir_rmt_port;\n\tfl6.fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto done;\n\t}\n\n\tskb = dccp_make_response(sk, dst, req);\n\tif (skb != NULL) {\n\t\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\t\tdh->dccph_checksum = dccp_v6_csum_finish(skb,\n\t\t\t\t\t\t\t &ireq->ir_v6_loc_addr,\n\t\t\t\t\t\t\t &ireq->ir_v6_rmt_addr);\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\terr = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\tdst_release(dst);\n\treturn err;\n}",
        "code_after_change": "static int dccp_v6_send_response(const struct sock *sk, struct request_sock *req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct in6_addr *final_p, final;\n\tstruct flowi6 fl6;\n\tint err = -1;\n\tstruct dst_entry *dst;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\tfl6.saddr = ireq->ir_v6_loc_addr;\n\tfl6.flowlabel = 0;\n\tfl6.flowi6_oif = ireq->ir_iif;\n\tfl6.fl6_dport = ireq->ir_rmt_port;\n\tfl6.fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto done;\n\t}\n\n\tskb = dccp_make_response(sk, dst, req);\n\tif (skb != NULL) {\n\t\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\t\tdh->dccph_checksum = dccp_v6_csum_finish(skb,\n\t\t\t\t\t\t\t &ireq->ir_v6_loc_addr,\n\t\t\t\t\t\t\t &ireq->ir_v6_rmt_addr);\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\trcu_read_lock();\n\t\terr = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),\n\t\t\t       np->tclass);\n\t\trcu_read_unlock();\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\tdst_release(dst);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,7 +19,9 @@\n \tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n \n \n-\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n+\trcu_read_lock();\n+\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n+\trcu_read_unlock();\n \n \tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n \tif (IS_ERR(dst)) {\n@@ -36,7 +38,10 @@\n \t\t\t\t\t\t\t &ireq->ir_v6_loc_addr,\n \t\t\t\t\t\t\t &ireq->ir_v6_rmt_addr);\n \t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n-\t\terr = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);\n+\t\trcu_read_lock();\n+\t\terr = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),\n+\t\t\t       np->tclass);\n+\t\trcu_read_unlock();\n \t\terr = net_xmit_eval(err);\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\trcu_read_lock();",
                "\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);",
                "\trcu_read_unlock();",
                "\t\trcu_read_lock();",
                "\t\terr = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),",
                "\t\t\t       np->tclass);",
                "\t\trcu_read_unlock();"
            ],
            "deleted": [
                "\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);",
                "\t\terr = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 991
    },
    {
        "cve_id": "CVE-2016-6786",
        "code_before_change": "void perf_event_disable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct task_struct *task = ctx->task;\n\n\tif (!task) {\n\t\t/*\n\t\t * Disable the event on the cpu that it's on\n\t\t */\n\t\tcpu_function_call(event->cpu, __perf_event_disable, event);\n\t\treturn;\n\t}\n\nretry:\n\tif (!task_function_call(task, __perf_event_disable, event))\n\t\treturn;\n\n\traw_spin_lock_irq(&ctx->lock);\n\t/*\n\t * If the event is still active, we need to retry the cross-call.\n\t */\n\tif (event->state == PERF_EVENT_STATE_ACTIVE) {\n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\t/*\n\t\t * Reload the task pointer, it might have been changed by\n\t\t * a concurrent perf_event_context_sched_out().\n\t\t */\n\t\ttask = ctx->task;\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * Since we have the lock this context can't be scheduled\n\t * in, so we can change the state safely.\n\t */\n\tif (event->state == PERF_EVENT_STATE_INACTIVE) {\n\t\tupdate_group_times(event);\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\t}\n\traw_spin_unlock_irq(&ctx->lock);\n}",
        "code_after_change": "void perf_event_disable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx;\n\n\tctx = perf_event_ctx_lock(event);\n\t_perf_event_disable(event);\n\tperf_event_ctx_unlock(event, ctx);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,41 +1,8 @@\n void perf_event_disable(struct perf_event *event)\n {\n-\tstruct perf_event_context *ctx = event->ctx;\n-\tstruct task_struct *task = ctx->task;\n+\tstruct perf_event_context *ctx;\n \n-\tif (!task) {\n-\t\t/*\n-\t\t * Disable the event on the cpu that it's on\n-\t\t */\n-\t\tcpu_function_call(event->cpu, __perf_event_disable, event);\n-\t\treturn;\n-\t}\n-\n-retry:\n-\tif (!task_function_call(task, __perf_event_disable, event))\n-\t\treturn;\n-\n-\traw_spin_lock_irq(&ctx->lock);\n-\t/*\n-\t * If the event is still active, we need to retry the cross-call.\n-\t */\n-\tif (event->state == PERF_EVENT_STATE_ACTIVE) {\n-\t\traw_spin_unlock_irq(&ctx->lock);\n-\t\t/*\n-\t\t * Reload the task pointer, it might have been changed by\n-\t\t * a concurrent perf_event_context_sched_out().\n-\t\t */\n-\t\ttask = ctx->task;\n-\t\tgoto retry;\n-\t}\n-\n-\t/*\n-\t * Since we have the lock this context can't be scheduled\n-\t * in, so we can change the state safely.\n-\t */\n-\tif (event->state == PERF_EVENT_STATE_INACTIVE) {\n-\t\tupdate_group_times(event);\n-\t\tevent->state = PERF_EVENT_STATE_OFF;\n-\t}\n-\traw_spin_unlock_irq(&ctx->lock);\n+\tctx = perf_event_ctx_lock(event);\n+\t_perf_event_disable(event);\n+\tperf_event_ctx_unlock(event, ctx);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct perf_event_context *ctx;",
                "\tctx = perf_event_ctx_lock(event);",
                "\t_perf_event_disable(event);",
                "\tperf_event_ctx_unlock(event, ctx);"
            ],
            "deleted": [
                "\tstruct perf_event_context *ctx = event->ctx;",
                "\tstruct task_struct *task = ctx->task;",
                "\tif (!task) {",
                "\t\t/*",
                "\t\t * Disable the event on the cpu that it's on",
                "\t\t */",
                "\t\tcpu_function_call(event->cpu, __perf_event_disable, event);",
                "\t\treturn;",
                "\t}",
                "",
                "retry:",
                "\tif (!task_function_call(task, __perf_event_disable, event))",
                "\t\treturn;",
                "",
                "\traw_spin_lock_irq(&ctx->lock);",
                "\t/*",
                "\t * If the event is still active, we need to retry the cross-call.",
                "\t */",
                "\tif (event->state == PERF_EVENT_STATE_ACTIVE) {",
                "\t\traw_spin_unlock_irq(&ctx->lock);",
                "\t\t/*",
                "\t\t * Reload the task pointer, it might have been changed by",
                "\t\t * a concurrent perf_event_context_sched_out().",
                "\t\t */",
                "\t\ttask = ctx->task;",
                "\t\tgoto retry;",
                "\t}",
                "",
                "\t/*",
                "\t * Since we have the lock this context can't be scheduled",
                "\t * in, so we can change the state safely.",
                "\t */",
                "\tif (event->state == PERF_EVENT_STATE_INACTIVE) {",
                "\t\tupdate_group_times(event);",
                "\t\tevent->state = PERF_EVENT_STATE_OFF;",
                "\t}",
                "\traw_spin_unlock_irq(&ctx->lock);"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "kernel/events/core.c in the performance subsystem in the Linux kernel before 4.0 mismanages locks during certain migrations, which allows local users to gain privileges via a crafted application, aka Android internal bug 30955111.",
        "id": 1079
    },
    {
        "cve_id": "CVE-2014-4014",
        "code_before_change": "static inline int check_sticky(struct inode *dir, struct inode *inode)\n{\n\tkuid_t fsuid = current_fsuid();\n\n\tif (!(dir->i_mode & S_ISVTX))\n\t\treturn 0;\n\tif (uid_eq(inode->i_uid, fsuid))\n\t\treturn 0;\n\tif (uid_eq(dir->i_uid, fsuid))\n\t\treturn 0;\n\treturn !inode_capable(inode, CAP_FOWNER);\n}",
        "code_after_change": "static inline int check_sticky(struct inode *dir, struct inode *inode)\n{\n\tkuid_t fsuid = current_fsuid();\n\n\tif (!(dir->i_mode & S_ISVTX))\n\t\treturn 0;\n\tif (uid_eq(inode->i_uid, fsuid))\n\t\treturn 0;\n\tif (uid_eq(dir->i_uid, fsuid))\n\t\treturn 0;\n\treturn !capable_wrt_inode_uidgid(inode, CAP_FOWNER);\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,5 +8,5 @@\n \t\treturn 0;\n \tif (uid_eq(dir->i_uid, fsuid))\n \t\treturn 0;\n-\treturn !inode_capable(inode, CAP_FOWNER);\n+\treturn !capable_wrt_inode_uidgid(inode, CAP_FOWNER);\n }",
        "function_modified_lines": {
            "added": [
                "\treturn !capable_wrt_inode_uidgid(inode, CAP_FOWNER);"
            ],
            "deleted": [
                "\treturn !inode_capable(inode, CAP_FOWNER);"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The capabilities implementation in the Linux kernel before 3.14.8 does not properly consider that namespaces are inapplicable to inodes, which allows local users to bypass intended chmod restrictions by first creating a user namespace, as demonstrated by setting the setgid bit on a file with group ownership of root.",
        "id": 553
    },
    {
        "cve_id": "CVE-2015-1593",
        "code_before_change": "static unsigned int stack_maxrandom_size(void)\n{\n\tunsigned int max = 0;\n\tif ((current->flags & PF_RANDOMIZE) &&\n\t\t!(current->personality & ADDR_NO_RANDOMIZE)) {\n\t\tmax = ((-1U) & STACK_RND_MASK) << PAGE_SHIFT;\n\t}\n\n\treturn max;\n}",
        "code_after_change": "static unsigned long stack_maxrandom_size(void)\n{\n\tunsigned long max = 0;\n\tif ((current->flags & PF_RANDOMIZE) &&\n\t\t!(current->personality & ADDR_NO_RANDOMIZE)) {\n\t\tmax = ((-1UL) & STACK_RND_MASK) << PAGE_SHIFT;\n\t}\n\n\treturn max;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,9 +1,9 @@\n-static unsigned int stack_maxrandom_size(void)\n+static unsigned long stack_maxrandom_size(void)\n {\n-\tunsigned int max = 0;\n+\tunsigned long max = 0;\n \tif ((current->flags & PF_RANDOMIZE) &&\n \t\t!(current->personality & ADDR_NO_RANDOMIZE)) {\n-\t\tmax = ((-1U) & STACK_RND_MASK) << PAGE_SHIFT;\n+\t\tmax = ((-1UL) & STACK_RND_MASK) << PAGE_SHIFT;\n \t}\n \n \treturn max;",
        "function_modified_lines": {
            "added": [
                "static unsigned long stack_maxrandom_size(void)",
                "\tunsigned long max = 0;",
                "\t\tmax = ((-1UL) & STACK_RND_MASK) << PAGE_SHIFT;"
            ],
            "deleted": [
                "static unsigned int stack_maxrandom_size(void)",
                "\tunsigned int max = 0;",
                "\t\tmax = ((-1U) & STACK_RND_MASK) << PAGE_SHIFT;"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The stack randomization feature in the Linux kernel before 3.19.1 on 64-bit platforms uses incorrect data types for the results of bitwise left-shift operations, which makes it easier for attackers to bypass the ASLR protection mechanism by predicting the address of the top of the stack, related to the randomize_stack_top function in fs/binfmt_elf.c and the stack_maxrandom_size function in arch/x86/mm/mmap.c.",
        "id": 738
    },
    {
        "cve_id": "CVE-2015-9016",
        "code_before_change": "struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n{\n\tstruct request *rq = tags->rqs[tag];\n\t/* mq_ctx of flush rq is always cloned from the corresponding req */\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(rq->q, rq->mq_ctx);\n\n\tif (!is_flush_request(rq, fq, tag))\n\t\treturn rq;\n\n\treturn fq->flush_rq;\n}",
        "code_after_change": "struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n{\n\treturn tags->rqs[tag];\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,11 +1,4 @@\n struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n {\n-\tstruct request *rq = tags->rqs[tag];\n-\t/* mq_ctx of flush rq is always cloned from the corresponding req */\n-\tstruct blk_flush_queue *fq = blk_get_flush_queue(rq->q, rq->mq_ctx);\n-\n-\tif (!is_flush_request(rq, fq, tag))\n-\t\treturn rq;\n-\n-\treturn fq->flush_rq;\n+\treturn tags->rqs[tag];\n }",
        "function_modified_lines": {
            "added": [
                "\treturn tags->rqs[tag];"
            ],
            "deleted": [
                "\tstruct request *rq = tags->rqs[tag];",
                "\t/* mq_ctx of flush rq is always cloned from the corresponding req */",
                "\tstruct blk_flush_queue *fq = blk_get_flush_queue(rq->q, rq->mq_ctx);",
                "",
                "\tif (!is_flush_request(rq, fq, tag))",
                "\t\treturn rq;",
                "",
                "\treturn fq->flush_rq;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-362"
        ],
        "cve_description": "In blk_mq_tag_to_rq in blk-mq.c in the upstream kernel, there is a possible use after free due to a race condition when a request has been previously freed by blk_mq_complete_request. This could lead to local escalation of privilege. Product: Android. Versions: Android kernel. Android ID: A-63083046.",
        "id": 885
    },
    {
        "cve_id": "CVE-2014-0181",
        "code_before_change": "static int rtnetlink_rcv_msg(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\trtnl_doit_func doit;\n\tint sz_idx, kind;\n\tint family;\n\tint type;\n\tint err;\n\n\ttype = nlh->nlmsg_type;\n\tif (type > RTM_MAX)\n\t\treturn -EOPNOTSUPP;\n\n\ttype -= RTM_BASE;\n\n\t/* All the messages must have at least 1 byte length */\n\tif (nlmsg_len(nlh) < sizeof(struct rtgenmsg))\n\t\treturn 0;\n\n\tfamily = ((struct rtgenmsg *)nlmsg_data(nlh))->rtgen_family;\n\tsz_idx = type>>2;\n\tkind = type&3;\n\n\tif (kind != 2 && !ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (kind == 2 && nlh->nlmsg_flags&NLM_F_DUMP) {\n\t\tstruct sock *rtnl;\n\t\trtnl_dumpit_func dumpit;\n\t\trtnl_calcit_func calcit;\n\t\tu16 min_dump_alloc = 0;\n\n\t\tdumpit = rtnl_get_dumpit(family, type);\n\t\tif (dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\t\tcalcit = rtnl_get_calcit(family, type);\n\t\tif (calcit)\n\t\t\tmin_dump_alloc = calcit(skb, nlh);\n\n\t\t__rtnl_unlock();\n\t\trtnl = net->rtnl;\n\t\t{\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.dump\t\t= dumpit,\n\t\t\t\t.min_dump_alloc\t= min_dump_alloc,\n\t\t\t};\n\t\t\terr = netlink_dump_start(rtnl, skb, nlh, &c);\n\t\t}\n\t\trtnl_lock();\n\t\treturn err;\n\t}\n\n\tdoit = rtnl_get_doit(family, type);\n\tif (doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\treturn doit(skb, nlh);\n}",
        "code_after_change": "static int rtnetlink_rcv_msg(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\trtnl_doit_func doit;\n\tint sz_idx, kind;\n\tint family;\n\tint type;\n\tint err;\n\n\ttype = nlh->nlmsg_type;\n\tif (type > RTM_MAX)\n\t\treturn -EOPNOTSUPP;\n\n\ttype -= RTM_BASE;\n\n\t/* All the messages must have at least 1 byte length */\n\tif (nlmsg_len(nlh) < sizeof(struct rtgenmsg))\n\t\treturn 0;\n\n\tfamily = ((struct rtgenmsg *)nlmsg_data(nlh))->rtgen_family;\n\tsz_idx = type>>2;\n\tkind = type&3;\n\n\tif (kind != 2 && !netlink_net_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (kind == 2 && nlh->nlmsg_flags&NLM_F_DUMP) {\n\t\tstruct sock *rtnl;\n\t\trtnl_dumpit_func dumpit;\n\t\trtnl_calcit_func calcit;\n\t\tu16 min_dump_alloc = 0;\n\n\t\tdumpit = rtnl_get_dumpit(family, type);\n\t\tif (dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\t\tcalcit = rtnl_get_calcit(family, type);\n\t\tif (calcit)\n\t\t\tmin_dump_alloc = calcit(skb, nlh);\n\n\t\t__rtnl_unlock();\n\t\trtnl = net->rtnl;\n\t\t{\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.dump\t\t= dumpit,\n\t\t\t\t.min_dump_alloc\t= min_dump_alloc,\n\t\t\t};\n\t\t\terr = netlink_dump_start(rtnl, skb, nlh, &c);\n\t\t}\n\t\trtnl_lock();\n\t\treturn err;\n\t}\n\n\tdoit = rtnl_get_doit(family, type);\n\tif (doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\treturn doit(skb, nlh);\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,7 +21,7 @@\n \tsz_idx = type>>2;\n \tkind = type&3;\n \n-\tif (kind != 2 && !ns_capable(net->user_ns, CAP_NET_ADMIN))\n+\tif (kind != 2 && !netlink_net_capable(skb, CAP_NET_ADMIN))\n \t\treturn -EPERM;\n \n \tif (kind == 2 && nlh->nlmsg_flags&NLM_F_DUMP) {",
        "function_modified_lines": {
            "added": [
                "\tif (kind != 2 && !netlink_net_capable(skb, CAP_NET_ADMIN))"
            ],
            "deleted": [
                "\tif (kind != 2 && !ns_capable(net->user_ns, CAP_NET_ADMIN))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Netlink implementation in the Linux kernel through 3.14.1 does not provide a mechanism for authorizing socket operations based on the opener of a socket, which allows local users to bypass intended access restrictions and modify network configurations by using a Netlink socket for the (1) stdout or (2) stderr of a setuid program.",
        "id": 440
    },
    {
        "cve_id": "CVE-2016-9644",
        "code_before_change": "void sort_extable(struct exception_table_entry *start,\n\t\t  struct exception_table_entry *finish)\n{\n\tstruct exception_table_entry *p;\n\tint i;\n\n\t/* Convert all entries to being relative to the start of the section */\n\ti = 0;\n\tfor (p = start; p < finish; p++) {\n\t\tp->insn += i;\n\t\ti += 4;\n\t\tp->fixup += i;\n\t\ti += 4;\n\t}\n\n\tsort(start, finish - start, sizeof(struct exception_table_entry),\n\t     cmp_ex, NULL);\n\n\t/* Denormalize all entries */\n\ti = 0;\n\tfor (p = start; p < finish; p++) {\n\t\tp->insn -= i;\n\t\ti += 4;\n\t\tp->fixup -= i;\n\t\ti += 4;\n\t}\n}",
        "code_after_change": "void sort_extable(struct exception_table_entry *start,\n\t\t  struct exception_table_entry *finish)\n{\n\tstruct exception_table_entry *p;\n\tint i;\n\n\t/* Convert all entries to being relative to the start of the section */\n\ti = 0;\n\tfor (p = start; p < finish; p++) {\n\t\tp->insn += i;\n\t\ti += 4;\n\t\tp->fixup += i;\n\t\ti += 4;\n\t\tp->handler += i;\n\t\ti += 4;\n\t}\n\n\tsort(start, finish - start, sizeof(struct exception_table_entry),\n\t     cmp_ex, NULL);\n\n\t/* Denormalize all entries */\n\ti = 0;\n\tfor (p = start; p < finish; p++) {\n\t\tp->insn -= i;\n\t\ti += 4;\n\t\tp->fixup -= i;\n\t\ti += 4;\n\t\tp->handler -= i;\n\t\ti += 4;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,6 +11,8 @@\n \t\ti += 4;\n \t\tp->fixup += i;\n \t\ti += 4;\n+\t\tp->handler += i;\n+\t\ti += 4;\n \t}\n \n \tsort(start, finish - start, sizeof(struct exception_table_entry),\n@@ -23,5 +25,7 @@\n \t\ti += 4;\n \t\tp->fixup -= i;\n \t\ti += 4;\n+\t\tp->handler -= i;\n+\t\ti += 4;\n \t}\n }",
        "function_modified_lines": {
            "added": [
                "\t\tp->handler += i;",
                "\t\ti += 4;",
                "\t\tp->handler -= i;",
                "\t\ti += 4;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The __get_user_asm_ex macro in arch/x86/include/asm/uaccess.h in the Linux kernel 4.4.22 through 4.4.28 contains extended asm statements that are incompatible with the exception table, which allows local users to obtain root access on non-SMEP platforms via a crafted application.  NOTE: this vulnerability exists because of incorrect backporting of the CVE-2016-9178 patch to older kernels.",
        "id": 1156
    },
    {
        "cve_id": "CVE-2013-1979",
        "code_before_change": "static __inline__ void scm_set_cred(struct scm_cookie *scm,\n\t\t\t\t    struct pid *pid, const struct cred *cred)\n{\n\tscm->pid  = get_pid(pid);\n\tscm->cred = cred ? get_cred(cred) : NULL;\n\tscm->creds.pid = pid_vnr(pid);\n\tscm->creds.uid = cred ? cred->euid : INVALID_UID;\n\tscm->creds.gid = cred ? cred->egid : INVALID_GID;\n}",
        "code_after_change": "static __inline__ void scm_set_cred(struct scm_cookie *scm,\n\t\t\t\t    struct pid *pid, const struct cred *cred)\n{\n\tscm->pid  = get_pid(pid);\n\tscm->cred = cred ? get_cred(cred) : NULL;\n\tscm->creds.pid = pid_vnr(pid);\n\tscm->creds.uid = cred ? cred->uid : INVALID_UID;\n\tscm->creds.gid = cred ? cred->gid : INVALID_GID;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,6 @@\n \tscm->pid  = get_pid(pid);\n \tscm->cred = cred ? get_cred(cred) : NULL;\n \tscm->creds.pid = pid_vnr(pid);\n-\tscm->creds.uid = cred ? cred->euid : INVALID_UID;\n-\tscm->creds.gid = cred ? cred->egid : INVALID_GID;\n+\tscm->creds.uid = cred ? cred->uid : INVALID_UID;\n+\tscm->creds.gid = cred ? cred->gid : INVALID_GID;\n }",
        "function_modified_lines": {
            "added": [
                "\tscm->creds.uid = cred ? cred->uid : INVALID_UID;",
                "\tscm->creds.gid = cred ? cred->gid : INVALID_GID;"
            ],
            "deleted": [
                "\tscm->creds.uid = cred ? cred->euid : INVALID_UID;",
                "\tscm->creds.gid = cred ? cred->egid : INVALID_GID;"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The scm_set_cred function in include/net/scm.h in the Linux kernel before 3.8.11 uses incorrect uid and gid values during credentials passing, which allows local users to gain privileges via a crafted application.",
        "id": 212
    },
    {
        "cve_id": "CVE-2015-9004",
        "code_before_change": "int perf_pmu_register(struct pmu *pmu, const char *name, int type)\n{\n\tint cpu, ret;\n\n\tmutex_lock(&pmus_lock);\n\tret = -ENOMEM;\n\tpmu->pmu_disable_count = alloc_percpu(int);\n\tif (!pmu->pmu_disable_count)\n\t\tgoto unlock;\n\n\tpmu->type = -1;\n\tif (!name)\n\t\tgoto skip_type;\n\tpmu->name = name;\n\n\tif (type < 0) {\n\t\ttype = idr_alloc(&pmu_idr, pmu, PERF_TYPE_MAX, 0, GFP_KERNEL);\n\t\tif (type < 0) {\n\t\t\tret = type;\n\t\t\tgoto free_pdc;\n\t\t}\n\t}\n\tpmu->type = type;\n\n\tif (pmu_bus_running) {\n\t\tret = pmu_dev_alloc(pmu);\n\t\tif (ret)\n\t\t\tgoto free_idr;\n\t}\n\nskip_type:\n\tpmu->pmu_cpu_context = find_pmu_context(pmu->task_ctx_nr);\n\tif (pmu->pmu_cpu_context)\n\t\tgoto got_cpu_context;\n\n\tret = -ENOMEM;\n\tpmu->pmu_cpu_context = alloc_percpu(struct perf_cpu_context);\n\tif (!pmu->pmu_cpu_context)\n\t\tgoto free_dev;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct perf_cpu_context *cpuctx;\n\n\t\tcpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);\n\t\t__perf_event_init_context(&cpuctx->ctx);\n\t\tlockdep_set_class(&cpuctx->ctx.mutex, &cpuctx_mutex);\n\t\tlockdep_set_class(&cpuctx->ctx.lock, &cpuctx_lock);\n\t\tcpuctx->ctx.type = cpu_context;\n\t\tcpuctx->ctx.pmu = pmu;\n\n\t\t__perf_cpu_hrtimer_init(cpuctx, cpu);\n\n\t\tINIT_LIST_HEAD(&cpuctx->rotation_list);\n\t\tcpuctx->unique_pmu = pmu;\n\t}\n\ngot_cpu_context:\n\tif (!pmu->start_txn) {\n\t\tif (pmu->pmu_enable) {\n\t\t\t/*\n\t\t\t * If we have pmu_enable/pmu_disable calls, install\n\t\t\t * transaction stubs that use that to try and batch\n\t\t\t * hardware accesses.\n\t\t\t */\n\t\t\tpmu->start_txn  = perf_pmu_start_txn;\n\t\t\tpmu->commit_txn = perf_pmu_commit_txn;\n\t\t\tpmu->cancel_txn = perf_pmu_cancel_txn;\n\t\t} else {\n\t\t\tpmu->start_txn  = perf_pmu_nop_void;\n\t\t\tpmu->commit_txn = perf_pmu_nop_int;\n\t\t\tpmu->cancel_txn = perf_pmu_nop_void;\n\t\t}\n\t}\n\n\tif (!pmu->pmu_enable) {\n\t\tpmu->pmu_enable  = perf_pmu_nop_void;\n\t\tpmu->pmu_disable = perf_pmu_nop_void;\n\t}\n\n\tif (!pmu->event_idx)\n\t\tpmu->event_idx = perf_event_idx_default;\n\n\tlist_add_rcu(&pmu->entry, &pmus);\n\tret = 0;\nunlock:\n\tmutex_unlock(&pmus_lock);\n\n\treturn ret;\n\nfree_dev:\n\tdevice_del(pmu->dev);\n\tput_device(pmu->dev);\n\nfree_idr:\n\tif (pmu->type >= PERF_TYPE_MAX)\n\t\tidr_remove(&pmu_idr, pmu->type);\n\nfree_pdc:\n\tfree_percpu(pmu->pmu_disable_count);\n\tgoto unlock;\n}",
        "code_after_change": "int perf_pmu_register(struct pmu *pmu, const char *name, int type)\n{\n\tint cpu, ret;\n\n\tmutex_lock(&pmus_lock);\n\tret = -ENOMEM;\n\tpmu->pmu_disable_count = alloc_percpu(int);\n\tif (!pmu->pmu_disable_count)\n\t\tgoto unlock;\n\n\tpmu->type = -1;\n\tif (!name)\n\t\tgoto skip_type;\n\tpmu->name = name;\n\n\tif (type < 0) {\n\t\ttype = idr_alloc(&pmu_idr, pmu, PERF_TYPE_MAX, 0, GFP_KERNEL);\n\t\tif (type < 0) {\n\t\t\tret = type;\n\t\t\tgoto free_pdc;\n\t\t}\n\t}\n\tpmu->type = type;\n\n\tif (pmu_bus_running) {\n\t\tret = pmu_dev_alloc(pmu);\n\t\tif (ret)\n\t\t\tgoto free_idr;\n\t}\n\nskip_type:\n\tpmu->pmu_cpu_context = find_pmu_context(pmu->task_ctx_nr);\n\tif (pmu->pmu_cpu_context)\n\t\tgoto got_cpu_context;\n\n\tret = -ENOMEM;\n\tpmu->pmu_cpu_context = alloc_percpu(struct perf_cpu_context);\n\tif (!pmu->pmu_cpu_context)\n\t\tgoto free_dev;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct perf_cpu_context *cpuctx;\n\n\t\tcpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);\n\t\t__perf_event_init_context(&cpuctx->ctx);\n\t\tlockdep_set_class(&cpuctx->ctx.mutex, &cpuctx_mutex);\n\t\tlockdep_set_class(&cpuctx->ctx.lock, &cpuctx_lock);\n\t\tcpuctx->ctx.pmu = pmu;\n\n\t\t__perf_cpu_hrtimer_init(cpuctx, cpu);\n\n\t\tINIT_LIST_HEAD(&cpuctx->rotation_list);\n\t\tcpuctx->unique_pmu = pmu;\n\t}\n\ngot_cpu_context:\n\tif (!pmu->start_txn) {\n\t\tif (pmu->pmu_enable) {\n\t\t\t/*\n\t\t\t * If we have pmu_enable/pmu_disable calls, install\n\t\t\t * transaction stubs that use that to try and batch\n\t\t\t * hardware accesses.\n\t\t\t */\n\t\t\tpmu->start_txn  = perf_pmu_start_txn;\n\t\t\tpmu->commit_txn = perf_pmu_commit_txn;\n\t\t\tpmu->cancel_txn = perf_pmu_cancel_txn;\n\t\t} else {\n\t\t\tpmu->start_txn  = perf_pmu_nop_void;\n\t\t\tpmu->commit_txn = perf_pmu_nop_int;\n\t\t\tpmu->cancel_txn = perf_pmu_nop_void;\n\t\t}\n\t}\n\n\tif (!pmu->pmu_enable) {\n\t\tpmu->pmu_enable  = perf_pmu_nop_void;\n\t\tpmu->pmu_disable = perf_pmu_nop_void;\n\t}\n\n\tif (!pmu->event_idx)\n\t\tpmu->event_idx = perf_event_idx_default;\n\n\tlist_add_rcu(&pmu->entry, &pmus);\n\tret = 0;\nunlock:\n\tmutex_unlock(&pmus_lock);\n\n\treturn ret;\n\nfree_dev:\n\tdevice_del(pmu->dev);\n\tput_device(pmu->dev);\n\nfree_idr:\n\tif (pmu->type >= PERF_TYPE_MAX)\n\t\tidr_remove(&pmu_idr, pmu->type);\n\nfree_pdc:\n\tfree_percpu(pmu->pmu_disable_count);\n\tgoto unlock;\n}",
        "patch": "--- code before\n+++ code after\n@@ -45,7 +45,6 @@\n \t\t__perf_event_init_context(&cpuctx->ctx);\n \t\tlockdep_set_class(&cpuctx->ctx.mutex, &cpuctx_mutex);\n \t\tlockdep_set_class(&cpuctx->ctx.lock, &cpuctx_lock);\n-\t\tcpuctx->ctx.type = cpu_context;\n \t\tcpuctx->ctx.pmu = pmu;\n \n \t\t__perf_cpu_hrtimer_init(cpuctx, cpu);",
        "function_modified_lines": {
            "added": [],
            "deleted": [
                "\t\tcpuctx->ctx.type = cpu_context;"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "kernel/events/core.c in the Linux kernel before 3.19 mishandles counter grouping, which allows local users to gain privileges via a crafted application, related to the perf_pmu_register and perf_event_open functions.",
        "id": 880
    },
    {
        "cve_id": "CVE-2014-8133",
        "code_before_change": "int regset_tls_set(struct task_struct *target, const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   const void *kbuf, const void __user *ubuf)\n{\n\tstruct user_desc infobuf[GDT_ENTRY_TLS_ENTRIES];\n\tconst struct user_desc *info;\n\n\tif (pos >= GDT_ENTRY_TLS_ENTRIES * sizeof(struct user_desc) ||\n\t    (pos % sizeof(struct user_desc)) != 0 ||\n\t    (count % sizeof(struct user_desc)) != 0)\n\t\treturn -EINVAL;\n\n\tif (kbuf)\n\t\tinfo = kbuf;\n\telse if (__copy_from_user(infobuf, ubuf, count))\n\t\treturn -EFAULT;\n\telse\n\t\tinfo = infobuf;\n\n\tset_tls_desc(target,\n\t\t     GDT_ENTRY_TLS_MIN + (pos / sizeof(struct user_desc)),\n\t\t     info, count / sizeof(struct user_desc));\n\n\treturn 0;\n}",
        "code_after_change": "int regset_tls_set(struct task_struct *target, const struct user_regset *regset,\n\t\t   unsigned int pos, unsigned int count,\n\t\t   const void *kbuf, const void __user *ubuf)\n{\n\tstruct user_desc infobuf[GDT_ENTRY_TLS_ENTRIES];\n\tconst struct user_desc *info;\n\tint i;\n\n\tif (pos >= GDT_ENTRY_TLS_ENTRIES * sizeof(struct user_desc) ||\n\t    (pos % sizeof(struct user_desc)) != 0 ||\n\t    (count % sizeof(struct user_desc)) != 0)\n\t\treturn -EINVAL;\n\n\tif (kbuf)\n\t\tinfo = kbuf;\n\telse if (__copy_from_user(infobuf, ubuf, count))\n\t\treturn -EFAULT;\n\telse\n\t\tinfo = infobuf;\n\n\tfor (i = 0; i < count / sizeof(struct user_desc); i++)\n\t\tif (!tls_desc_okay(info + i))\n\t\t\treturn -EINVAL;\n\n\tset_tls_desc(target,\n\t\t     GDT_ENTRY_TLS_MIN + (pos / sizeof(struct user_desc)),\n\t\t     info, count / sizeof(struct user_desc));\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,7 @@\n {\n \tstruct user_desc infobuf[GDT_ENTRY_TLS_ENTRIES];\n \tconst struct user_desc *info;\n+\tint i;\n \n \tif (pos >= GDT_ENTRY_TLS_ENTRIES * sizeof(struct user_desc) ||\n \t    (pos % sizeof(struct user_desc)) != 0 ||\n@@ -17,6 +18,10 @@\n \telse\n \t\tinfo = infobuf;\n \n+\tfor (i = 0; i < count / sizeof(struct user_desc); i++)\n+\t\tif (!tls_desc_okay(info + i))\n+\t\t\treturn -EINVAL;\n+\n \tset_tls_desc(target,\n \t\t     GDT_ENTRY_TLS_MIN + (pos / sizeof(struct user_desc)),\n \t\t     info, count / sizeof(struct user_desc));",
        "function_modified_lines": {
            "added": [
                "\tint i;",
                "\tfor (i = 0; i < count / sizeof(struct user_desc); i++)",
                "\t\tif (!tls_desc_okay(info + i))",
                "\t\t\treturn -EINVAL;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "arch/x86/kernel/tls.c in the Thread Local Storage (TLS) implementation in the Linux kernel through 3.18.1 allows local users to bypass the espfix protection mechanism, and consequently makes it easier for local users to bypass the ASLR protection mechanism, via a crafted application that makes a set_thread_area system call and later reads a 16-bit value.",
        "id": 603
    },
    {
        "cve_id": "CVE-2014-0181",
        "code_before_change": "static int handle_cmd(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct sk_buff *rep_buf;\n\tstruct nlmsghdr *rep_nlh;\n\tstruct nlmsghdr *req_nlh = info->nlhdr;\n\tstruct tipc_genlmsghdr *req_userhdr = info->userhdr;\n\tint hdr_space = nlmsg_total_size(GENL_HDRLEN + TIPC_GENL_HDRLEN);\n\tu16 cmd;\n\n\tif ((req_userhdr->cmd & 0xC000) && (!capable(CAP_NET_ADMIN)))\n\t\tcmd = TIPC_CMD_NOT_NET_ADMIN;\n\telse\n\t\tcmd = req_userhdr->cmd;\n\n\trep_buf = tipc_cfg_do_cmd(req_userhdr->dest, cmd,\n\t\t\tnlmsg_data(req_nlh) + GENL_HDRLEN + TIPC_GENL_HDRLEN,\n\t\t\tnlmsg_attrlen(req_nlh, GENL_HDRLEN + TIPC_GENL_HDRLEN),\n\t\t\thdr_space);\n\n\tif (rep_buf) {\n\t\tskb_push(rep_buf, hdr_space);\n\t\trep_nlh = nlmsg_hdr(rep_buf);\n\t\tmemcpy(rep_nlh, req_nlh, hdr_space);\n\t\trep_nlh->nlmsg_len = rep_buf->len;\n\t\tgenlmsg_unicast(&init_net, rep_buf, NETLINK_CB(skb).portid);\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "static int handle_cmd(struct sk_buff *skb, struct genl_info *info)\n{\n\tstruct sk_buff *rep_buf;\n\tstruct nlmsghdr *rep_nlh;\n\tstruct nlmsghdr *req_nlh = info->nlhdr;\n\tstruct tipc_genlmsghdr *req_userhdr = info->userhdr;\n\tint hdr_space = nlmsg_total_size(GENL_HDRLEN + TIPC_GENL_HDRLEN);\n\tu16 cmd;\n\n\tif ((req_userhdr->cmd & 0xC000) && (!netlink_capable(skb, CAP_NET_ADMIN)))\n\t\tcmd = TIPC_CMD_NOT_NET_ADMIN;\n\telse\n\t\tcmd = req_userhdr->cmd;\n\n\trep_buf = tipc_cfg_do_cmd(req_userhdr->dest, cmd,\n\t\t\tnlmsg_data(req_nlh) + GENL_HDRLEN + TIPC_GENL_HDRLEN,\n\t\t\tnlmsg_attrlen(req_nlh, GENL_HDRLEN + TIPC_GENL_HDRLEN),\n\t\t\thdr_space);\n\n\tif (rep_buf) {\n\t\tskb_push(rep_buf, hdr_space);\n\t\trep_nlh = nlmsg_hdr(rep_buf);\n\t\tmemcpy(rep_nlh, req_nlh, hdr_space);\n\t\trep_nlh->nlmsg_len = rep_buf->len;\n\t\tgenlmsg_unicast(&init_net, rep_buf, NETLINK_CB(skb).portid);\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,7 @@\n \tint hdr_space = nlmsg_total_size(GENL_HDRLEN + TIPC_GENL_HDRLEN);\n \tu16 cmd;\n \n-\tif ((req_userhdr->cmd & 0xC000) && (!capable(CAP_NET_ADMIN)))\n+\tif ((req_userhdr->cmd & 0xC000) && (!netlink_capable(skb, CAP_NET_ADMIN)))\n \t\tcmd = TIPC_CMD_NOT_NET_ADMIN;\n \telse\n \t\tcmd = req_userhdr->cmd;",
        "function_modified_lines": {
            "added": [
                "\tif ((req_userhdr->cmd & 0xC000) && (!netlink_capable(skb, CAP_NET_ADMIN)))"
            ],
            "deleted": [
                "\tif ((req_userhdr->cmd & 0xC000) && (!capable(CAP_NET_ADMIN)))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Netlink implementation in the Linux kernel through 3.14.1 does not provide a mechanism for authorizing socket operations based on the opener of a socket, which allows local users to bypass intended access restrictions and modify network configurations by using a Netlink socket for the (1) stdout or (2) stderr of a setuid program.",
        "id": 459
    },
    {
        "cve_id": "CVE-2016-3157",
        "code_before_change": "static void xen_set_iopl_mask(unsigned mask)\n{\n\tstruct physdev_set_iopl set_iopl;\n\n\t/* Force the change at ring 0. */\n\tset_iopl.iopl = (mask == 0) ? 1 : (mask >> 12) & 3;\n\tHYPERVISOR_physdev_op(PHYSDEVOP_set_iopl, &set_iopl);\n}",
        "code_after_change": "void xen_set_iopl_mask(unsigned mask)\n{\n\tstruct physdev_set_iopl set_iopl;\n\n\t/* Force the change at ring 0. */\n\tset_iopl.iopl = (mask == 0) ? 1 : (mask >> 12) & 3;\n\tHYPERVISOR_physdev_op(PHYSDEVOP_set_iopl, &set_iopl);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,4 +1,4 @@\n-static void xen_set_iopl_mask(unsigned mask)\n+void xen_set_iopl_mask(unsigned mask)\n {\n \tstruct physdev_set_iopl set_iopl;\n ",
        "function_modified_lines": {
            "added": [
                "void xen_set_iopl_mask(unsigned mask)"
            ],
            "deleted": [
                "static void xen_set_iopl_mask(unsigned mask)"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The __switch_to function in arch/x86/kernel/process_64.c in the Linux kernel does not properly context-switch IOPL on 64-bit PV Xen guests, which allows local guest OS users to gain privileges, cause a denial of service (guest OS crash), or obtain sensitive information by leveraging I/O port access.",
        "id": 986
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen = len + transhdrlen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (opt == NULL)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, hlimit, tclass, opt,\n\t\t\t      &fl6, (struct rt6_info *)dst,\n\t\t\t      msg->msg_flags, dontfrag);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
        "code_after_change": "static int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen = len + transhdrlen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, hlimit, tclass, opt,\n\t\t\t      &fl6, (struct rt6_info *)dst,\n\t\t\t      msg->msg_flags, dontfrag);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,7 @@\n \tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n \tstruct in6_addr *daddr, *final_p, final;\n \tstruct ipv6_pinfo *np = inet6_sk(sk);\n+\tstruct ipv6_txoptions *opt_to_free = NULL;\n \tstruct ipv6_txoptions *opt = NULL;\n \tstruct ip6_flowlabel *flowlabel = NULL;\n \tstruct dst_entry *dst = NULL;\n@@ -93,8 +94,10 @@\n \t\t\topt = NULL;\n \t}\n \n-\tif (opt == NULL)\n-\t\topt = np->opt;\n+\tif (!opt) {\n+\t\topt = txopt_get(np);\n+\t\topt_to_free = opt;\n+\t}\n \tif (flowlabel)\n \t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n \topt = ipv6_fixup_options(&opt_space, opt);\n@@ -149,6 +152,7 @@\n \tdst_release(dst);\n out:\n \tfl6_sock_release(flowlabel);\n+\ttxopt_put(opt_to_free);\n \n \treturn err < 0 ? err : len;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct ipv6_txoptions *opt_to_free = NULL;",
                "\tif (!opt) {",
                "\t\topt = txopt_get(np);",
                "\t\topt_to_free = opt;",
                "\t}",
                "\ttxopt_put(opt_to_free);"
            ],
            "deleted": [
                "\tif (opt == NULL)",
                "\t\topt = np->opt;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1009
    },
    {
        "cve_id": "CVE-2016-6786",
        "code_before_change": "int perf_event_task_disable(void)\n{\n\tstruct perf_event *event;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_for_each_entry(event, &current->perf_event_list, owner_entry)\n\t\tperf_event_for_each_child(event, perf_event_disable);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\treturn 0;\n}",
        "code_after_change": "int perf_event_task_disable(void)\n{\n\tstruct perf_event_context *ctx;\n\tstruct perf_event *event;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_for_each_entry(event, &current->perf_event_list, owner_entry) {\n\t\tctx = perf_event_ctx_lock(event);\n\t\tperf_event_for_each_child(event, _perf_event_disable);\n\t\tperf_event_ctx_unlock(event, ctx);\n\t}\n\tmutex_unlock(&current->perf_event_mutex);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,10 +1,14 @@\n int perf_event_task_disable(void)\n {\n+\tstruct perf_event_context *ctx;\n \tstruct perf_event *event;\n \n \tmutex_lock(&current->perf_event_mutex);\n-\tlist_for_each_entry(event, &current->perf_event_list, owner_entry)\n-\t\tperf_event_for_each_child(event, perf_event_disable);\n+\tlist_for_each_entry(event, &current->perf_event_list, owner_entry) {\n+\t\tctx = perf_event_ctx_lock(event);\n+\t\tperf_event_for_each_child(event, _perf_event_disable);\n+\t\tperf_event_ctx_unlock(event, ctx);\n+\t}\n \tmutex_unlock(&current->perf_event_mutex);\n \n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\tstruct perf_event_context *ctx;",
                "\tlist_for_each_entry(event, &current->perf_event_list, owner_entry) {",
                "\t\tctx = perf_event_ctx_lock(event);",
                "\t\tperf_event_for_each_child(event, _perf_event_disable);",
                "\t\tperf_event_ctx_unlock(event, ctx);",
                "\t}"
            ],
            "deleted": [
                "\tlist_for_each_entry(event, &current->perf_event_list, owner_entry)",
                "\t\tperf_event_for_each_child(event, perf_event_disable);"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "kernel/events/core.c in the performance subsystem in the Linux kernel before 4.0 mismanages locks during certain migrations, which allows local users to gain privileges via a crafted application, aka Android internal bug 30955111.",
        "id": 1084
    },
    {
        "cve_id": "CVE-2014-0181",
        "code_before_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
        "code_after_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -131,12 +131,12 @@\n \t\t\t\tmodified = 1;\n \t\t\t}\n \n-\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n+\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n \t\t}\n \n \t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n \t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n-\t\t\t\treturn rtnl_group_changelink(net,\n+\t\t\t\treturn rtnl_group_changelink(skb, net,\n \t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n \t\t\t\t\t\tifm, tb);\n \t\t\treturn -ENODEV;",
        "function_modified_lines": {
            "added": [
                "\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);",
                "\t\t\t\treturn rtnl_group_changelink(skb, net,"
            ],
            "deleted": [
                "\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);",
                "\t\t\t\treturn rtnl_group_changelink(net,"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Netlink implementation in the Linux kernel through 3.14.1 does not provide a mechanism for authorizing socket operations based on the opener of a socket, which allows local users to bypass intended access restrictions and modify network configurations by using a Netlink socket for the (1) stdout or (2) stderr of a setuid program.",
        "id": 439
    },
    {
        "cve_id": "CVE-2016-10200",
        "code_before_change": "static int l2tp_ip_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sockaddr_l2tpip *addr = (struct sockaddr_l2tpip *) uaddr;\n\tstruct net *net = sock_net(sk);\n\tint ret;\n\tint chk_addr_ret;\n\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(struct sockaddr_l2tpip))\n\t\treturn -EINVAL;\n\tif (addr->l2tp_family != AF_INET)\n\t\treturn -EINVAL;\n\n\tret = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip_lock);\n\tif (__l2tp_ip_bind_lookup(net, addr->l2tp_addr.s_addr,\n\t\t\t\t  sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\tlock_sock(sk);\n\tif (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))\n\t\tgoto out;\n\n\tchk_addr_ret = inet_addr_type(net, addr->l2tp_addr.s_addr);\n\tret = -EADDRNOTAVAIL;\n\tif (addr->l2tp_addr.s_addr && chk_addr_ret != RTN_LOCAL &&\n\t    chk_addr_ret != RTN_MULTICAST && chk_addr_ret != RTN_BROADCAST)\n\t\tgoto out;\n\n\tif (addr->l2tp_addr.s_addr)\n\t\tinet->inet_rcv_saddr = inet->inet_saddr = addr->l2tp_addr.s_addr;\n\tif (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST)\n\t\tinet->inet_saddr = 0;  /* Use device */\n\tsk_dst_reset(sk);\n\n\tl2tp_ip_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip_lock);\n\tsk_add_bind_node(sk, &l2tp_ip_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip_lock);\n\tret = 0;\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\nout:\n\trelease_sock(sk);\n\n\treturn ret;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\treturn ret;\n}",
        "code_after_change": "static int l2tp_ip_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sockaddr_l2tpip *addr = (struct sockaddr_l2tpip *) uaddr;\n\tstruct net *net = sock_net(sk);\n\tint ret;\n\tint chk_addr_ret;\n\n\tif (addr_len < sizeof(struct sockaddr_l2tpip))\n\t\treturn -EINVAL;\n\tif (addr->l2tp_family != AF_INET)\n\t\treturn -EINVAL;\n\n\tret = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip_lock);\n\tif (__l2tp_ip_bind_lookup(net, addr->l2tp_addr.s_addr,\n\t\t\t\t  sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\tlock_sock(sk);\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\tgoto out;\n\n\tif (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))\n\t\tgoto out;\n\n\tchk_addr_ret = inet_addr_type(net, addr->l2tp_addr.s_addr);\n\tret = -EADDRNOTAVAIL;\n\tif (addr->l2tp_addr.s_addr && chk_addr_ret != RTN_LOCAL &&\n\t    chk_addr_ret != RTN_MULTICAST && chk_addr_ret != RTN_BROADCAST)\n\t\tgoto out;\n\n\tif (addr->l2tp_addr.s_addr)\n\t\tinet->inet_rcv_saddr = inet->inet_saddr = addr->l2tp_addr.s_addr;\n\tif (chk_addr_ret == RTN_MULTICAST || chk_addr_ret == RTN_BROADCAST)\n\t\tinet->inet_saddr = 0;  /* Use device */\n\tsk_dst_reset(sk);\n\n\tl2tp_ip_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip_lock);\n\tsk_add_bind_node(sk, &l2tp_ip_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip_lock);\n\tret = 0;\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\nout:\n\trelease_sock(sk);\n\n\treturn ret;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip_lock);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,8 +6,6 @@\n \tint ret;\n \tint chk_addr_ret;\n \n-\tif (!sock_flag(sk, SOCK_ZAPPED))\n-\t\treturn -EINVAL;\n \tif (addr_len < sizeof(struct sockaddr_l2tpip))\n \t\treturn -EINVAL;\n \tif (addr->l2tp_family != AF_INET)\n@@ -22,6 +20,9 @@\n \tread_unlock_bh(&l2tp_ip_lock);\n \n \tlock_sock(sk);\n+\tif (!sock_flag(sk, SOCK_ZAPPED))\n+\t\tgoto out;\n+\n \tif (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))\n \t\tgoto out;\n ",
        "function_modified_lines": {
            "added": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\tgoto out;",
                ""
            ],
            "deleted": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\treturn -EINVAL;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the L2TPv3 IP Encapsulation feature in the Linux kernel before 4.8.14 allows local users to gain privileges or cause a denial of service (use-after-free) by making multiple bind system calls without properly ascertaining whether a socket has the SOCK_ZAPPED status, related to net/l2tp/l2tp_ip.c and net/l2tp/l2tp_ip6.c.",
        "id": 898
    },
    {
        "cve_id": "CVE-2016-4997",
        "code_before_change": "static int\ncheck_compat_entry_size_and_hooks(struct compat_ip6t_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ip6t_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ip6t_entry) - sizeof(struct compat_ip6t_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ipv6, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ip6t_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV6, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET6, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}",
        "code_after_change": "static int\ncheck_compat_entry_size_and_hooks(struct compat_ip6t_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ip6t_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->elems,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ip6t_entry) - sizeof(struct compat_ip6t_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ipv6, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ip6t_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV6, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET6, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -33,7 +33,7 @@\n \tif (!ip6_checkentry(&e->ipv6))\n \t\treturn -EINVAL;\n \n-\tret = xt_compat_check_entry_offsets(e,\n+\tret = xt_compat_check_entry_offsets(e, e->elems,\n \t\t\t\t\t    e->target_offset, e->next_offset);\n \tif (ret)\n \t\treturn ret;",
        "function_modified_lines": {
            "added": [
                "\tret = xt_compat_check_entry_offsets(e, e->elems,"
            ],
            "deleted": [
                "\tret = xt_compat_check_entry_offsets(e,"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The compat IPT_SO_SET_REPLACE and IP6T_SO_SET_REPLACE setsockopt implementations in the netfilter subsystem in the Linux kernel before 4.6.3 allow local users to gain privileges or cause a denial of service (memory corruption) by leveraging in-container root access to provide a crafted offset value that triggers an unintended decrement.",
        "id": 1044
    },
    {
        "cve_id": "CVE-2016-9644",
        "code_before_change": "dotraplinkage void\ndo_general_protection(struct pt_regs *regs, long error_code)\n{\n\tstruct task_struct *tsk;\n\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(), \"entry code didn't wake RCU\");\n\tconditional_sti(regs);\n\n\tif (v8086_mode(regs)) {\n\t\tlocal_irq_enable();\n\t\thandle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);\n\t\treturn;\n\t}\n\n\ttsk = current;\n\tif (!user_mode(regs)) {\n\t\tif (fixup_exception(regs))\n\t\t\treturn;\n\n\t\ttsk->thread.error_code = error_code;\n\t\ttsk->thread.trap_nr = X86_TRAP_GP;\n\t\tif (notify_die(DIE_GPF, \"general protection fault\", regs, error_code,\n\t\t\t       X86_TRAP_GP, SIGSEGV) != NOTIFY_STOP)\n\t\t\tdie(\"general protection fault\", regs, error_code);\n\t\treturn;\n\t}\n\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_nr = X86_TRAP_GP;\n\n\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t\tprintk_ratelimit()) {\n\t\tpr_info(\"%s[%d] general protection ip:%lx sp:%lx error:%lx\",\n\t\t\ttsk->comm, task_pid_nr(tsk),\n\t\t\tregs->ip, regs->sp, error_code);\n\t\tprint_vma_addr(\" in \", regs->ip);\n\t\tpr_cont(\"\\n\");\n\t}\n\n\tforce_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);\n}",
        "code_after_change": "dotraplinkage void\ndo_general_protection(struct pt_regs *regs, long error_code)\n{\n\tstruct task_struct *tsk;\n\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(), \"entry code didn't wake RCU\");\n\tconditional_sti(regs);\n\n\tif (v8086_mode(regs)) {\n\t\tlocal_irq_enable();\n\t\thandle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);\n\t\treturn;\n\t}\n\n\ttsk = current;\n\tif (!user_mode(regs)) {\n\t\tif (fixup_exception(regs, X86_TRAP_GP))\n\t\t\treturn;\n\n\t\ttsk->thread.error_code = error_code;\n\t\ttsk->thread.trap_nr = X86_TRAP_GP;\n\t\tif (notify_die(DIE_GPF, \"general protection fault\", regs, error_code,\n\t\t\t       X86_TRAP_GP, SIGSEGV) != NOTIFY_STOP)\n\t\t\tdie(\"general protection fault\", regs, error_code);\n\t\treturn;\n\t}\n\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_nr = X86_TRAP_GP;\n\n\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t\tprintk_ratelimit()) {\n\t\tpr_info(\"%s[%d] general protection ip:%lx sp:%lx error:%lx\",\n\t\t\ttsk->comm, task_pid_nr(tsk),\n\t\t\tregs->ip, regs->sp, error_code);\n\t\tprint_vma_addr(\" in \", regs->ip);\n\t\tpr_cont(\"\\n\");\n\t}\n\n\tforce_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);\n}",
        "patch": "--- code before\n+++ code after\n@@ -14,7 +14,7 @@\n \n \ttsk = current;\n \tif (!user_mode(regs)) {\n-\t\tif (fixup_exception(regs))\n+\t\tif (fixup_exception(regs, X86_TRAP_GP))\n \t\t\treturn;\n \n \t\ttsk->thread.error_code = error_code;",
        "function_modified_lines": {
            "added": [
                "\t\tif (fixup_exception(regs, X86_TRAP_GP))"
            ],
            "deleted": [
                "\t\tif (fixup_exception(regs))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The __get_user_asm_ex macro in arch/x86/include/asm/uaccess.h in the Linux kernel 4.4.22 through 4.4.28 contains extended asm statements that are incompatible with the exception table, which allows local users to obtain root access on non-SMEP platforms via a crafted application.  NOTE: this vulnerability exists because of incorrect backporting of the CVE-2016-9178 patch to older kernels.",
        "id": 1152
    },
    {
        "cve_id": "CVE-2016-8632",
        "code_before_change": "static int tipc_udp_enable(struct net *net, struct tipc_bearer *b,\n\t\t\t   struct nlattr *attrs[])\n{\n\tint err = -EINVAL;\n\tstruct udp_bearer *ub;\n\tstruct udp_media_addr remote = {0};\n\tstruct udp_media_addr local = {0};\n\tstruct udp_port_cfg udp_conf = {0};\n\tstruct udp_tunnel_sock_cfg tuncfg = {NULL};\n\tstruct nlattr *opts[TIPC_NLA_UDP_MAX + 1];\n\n\tub = kzalloc(sizeof(*ub), GFP_ATOMIC);\n\tif (!ub)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&ub->rcast.list);\n\n\tif (!attrs[TIPC_NLA_BEARER_UDP_OPTS])\n\t\tgoto err;\n\n\tif (nla_parse_nested(opts, TIPC_NLA_UDP_MAX,\n\t\t\t     attrs[TIPC_NLA_BEARER_UDP_OPTS],\n\t\t\t     tipc_nl_udp_policy))\n\t\tgoto err;\n\n\tif (!opts[TIPC_NLA_UDP_LOCAL] || !opts[TIPC_NLA_UDP_REMOTE]) {\n\t\tpr_err(\"Invalid UDP bearer configuration\");\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_LOCAL], &local,\n\t\t\t\t  &ub->ifindex);\n\tif (err)\n\t\tgoto err;\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_REMOTE], &remote, NULL);\n\tif (err)\n\t\tgoto err;\n\n\tb->bcast_addr.media_id = TIPC_MEDIA_TYPE_UDP;\n\tb->bcast_addr.broadcast = 1;\n\trcu_assign_pointer(b->media_ptr, ub);\n\trcu_assign_pointer(ub->bearer, b);\n\ttipc_udp_media_addr_set(&b->addr, &local);\n\tif (local.proto == htons(ETH_P_IP)) {\n\t\tstruct net_device *dev;\n\n\t\tdev = __ip_dev_find(net, local.ipv4.s_addr, false);\n\t\tif (!dev) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto err;\n\t\t}\n\t\tudp_conf.family = AF_INET;\n\t\tudp_conf.local_ip.s_addr = htonl(INADDR_ANY);\n\t\tudp_conf.use_udp_checksums = false;\n\t\tub->ifindex = dev->ifindex;\n\t\tb->mtu = dev->mtu - sizeof(struct iphdr)\n\t\t\t- sizeof(struct udphdr);\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else if (local.proto == htons(ETH_P_IPV6)) {\n\t\tudp_conf.family = AF_INET6;\n\t\tudp_conf.use_udp6_tx_checksums = true;\n\t\tudp_conf.use_udp6_rx_checksums = true;\n\t\tudp_conf.local_ip6 = in6addr_any;\n\t\tb->mtu = 1280;\n#endif\n\t} else {\n\t\terr = -EAFNOSUPPORT;\n\t\tgoto err;\n\t}\n\tudp_conf.local_udp_port = local.port;\n\terr = udp_sock_create(net, &udp_conf, &ub->ubsock);\n\tif (err)\n\t\tgoto err;\n\ttuncfg.sk_user_data = ub;\n\ttuncfg.encap_type = 1;\n\ttuncfg.encap_rcv = tipc_udp_recv;\n\ttuncfg.encap_destroy = NULL;\n\tsetup_udp_tunnel_sock(net, ub->ubsock, &tuncfg);\n\n\t/**\n\t * The bcast media address port is used for all peers and the ip\n\t * is used if it's a multicast address.\n\t */\n\tmemcpy(&b->bcast_addr.value, &remote, sizeof(remote));\n\tif (tipc_udp_is_mcast_addr(&remote))\n\t\terr = enable_mcast(ub, &remote);\n\telse\n\t\terr = tipc_udp_rcast_add(b, &remote);\n\tif (err)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\tif (ub->ubsock)\n\t\tudp_tunnel_sock_release(ub->ubsock);\n\tkfree(ub);\n\treturn err;\n}",
        "code_after_change": "static int tipc_udp_enable(struct net *net, struct tipc_bearer *b,\n\t\t\t   struct nlattr *attrs[])\n{\n\tint err = -EINVAL;\n\tstruct udp_bearer *ub;\n\tstruct udp_media_addr remote = {0};\n\tstruct udp_media_addr local = {0};\n\tstruct udp_port_cfg udp_conf = {0};\n\tstruct udp_tunnel_sock_cfg tuncfg = {NULL};\n\tstruct nlattr *opts[TIPC_NLA_UDP_MAX + 1];\n\n\tub = kzalloc(sizeof(*ub), GFP_ATOMIC);\n\tif (!ub)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&ub->rcast.list);\n\n\tif (!attrs[TIPC_NLA_BEARER_UDP_OPTS])\n\t\tgoto err;\n\n\tif (nla_parse_nested(opts, TIPC_NLA_UDP_MAX,\n\t\t\t     attrs[TIPC_NLA_BEARER_UDP_OPTS],\n\t\t\t     tipc_nl_udp_policy))\n\t\tgoto err;\n\n\tif (!opts[TIPC_NLA_UDP_LOCAL] || !opts[TIPC_NLA_UDP_REMOTE]) {\n\t\tpr_err(\"Invalid UDP bearer configuration\");\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_LOCAL], &local,\n\t\t\t\t  &ub->ifindex);\n\tif (err)\n\t\tgoto err;\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_REMOTE], &remote, NULL);\n\tif (err)\n\t\tgoto err;\n\n\tb->bcast_addr.media_id = TIPC_MEDIA_TYPE_UDP;\n\tb->bcast_addr.broadcast = 1;\n\trcu_assign_pointer(b->media_ptr, ub);\n\trcu_assign_pointer(ub->bearer, b);\n\ttipc_udp_media_addr_set(&b->addr, &local);\n\tif (local.proto == htons(ETH_P_IP)) {\n\t\tstruct net_device *dev;\n\n\t\tdev = __ip_dev_find(net, local.ipv4.s_addr, false);\n\t\tif (!dev) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto err;\n\t\t}\n\t\tudp_conf.family = AF_INET;\n\t\tudp_conf.local_ip.s_addr = htonl(INADDR_ANY);\n\t\tudp_conf.use_udp_checksums = false;\n\t\tub->ifindex = dev->ifindex;\n\t\tif (tipc_mtu_bad(dev, sizeof(struct iphdr) +\n\t\t\t\t      sizeof(struct udphdr))) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\t\tb->mtu = dev->mtu - sizeof(struct iphdr)\n\t\t\t- sizeof(struct udphdr);\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else if (local.proto == htons(ETH_P_IPV6)) {\n\t\tudp_conf.family = AF_INET6;\n\t\tudp_conf.use_udp6_tx_checksums = true;\n\t\tudp_conf.use_udp6_rx_checksums = true;\n\t\tudp_conf.local_ip6 = in6addr_any;\n\t\tb->mtu = 1280;\n#endif\n\t} else {\n\t\terr = -EAFNOSUPPORT;\n\t\tgoto err;\n\t}\n\tudp_conf.local_udp_port = local.port;\n\terr = udp_sock_create(net, &udp_conf, &ub->ubsock);\n\tif (err)\n\t\tgoto err;\n\ttuncfg.sk_user_data = ub;\n\ttuncfg.encap_type = 1;\n\ttuncfg.encap_rcv = tipc_udp_recv;\n\ttuncfg.encap_destroy = NULL;\n\tsetup_udp_tunnel_sock(net, ub->ubsock, &tuncfg);\n\n\t/**\n\t * The bcast media address port is used for all peers and the ip\n\t * is used if it's a multicast address.\n\t */\n\tmemcpy(&b->bcast_addr.value, &remote, sizeof(remote));\n\tif (tipc_udp_is_mcast_addr(&remote))\n\t\terr = enable_mcast(ub, &remote);\n\telse\n\t\terr = tipc_udp_rcast_add(b, &remote);\n\tif (err)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\tif (ub->ubsock)\n\t\tudp_tunnel_sock_release(ub->ubsock);\n\tkfree(ub);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -55,6 +55,11 @@\n \t\tudp_conf.local_ip.s_addr = htonl(INADDR_ANY);\n \t\tudp_conf.use_udp_checksums = false;\n \t\tub->ifindex = dev->ifindex;\n+\t\tif (tipc_mtu_bad(dev, sizeof(struct iphdr) +\n+\t\t\t\t      sizeof(struct udphdr))) {\n+\t\t\terr = -EINVAL;\n+\t\t\tgoto err;\n+\t\t}\n \t\tb->mtu = dev->mtu - sizeof(struct iphdr)\n \t\t\t- sizeof(struct udphdr);\n #if IS_ENABLED(CONFIG_IPV6)",
        "function_modified_lines": {
            "added": [
                "\t\tif (tipc_mtu_bad(dev, sizeof(struct iphdr) +",
                "\t\t\t\t      sizeof(struct udphdr))) {",
                "\t\t\terr = -EINVAL;",
                "\t\t\tgoto err;",
                "\t\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264",
            "CWE-119"
        ],
        "cve_description": "The tipc_msg_build function in net/tipc/msg.c in the Linux kernel through 4.8.11 does not validate the relationship between the minimum fragment length and the maximum packet size, which allows local users to gain privileges or cause a denial of service (heap-based buffer overflow) by leveraging the CAP_NET_ADMIN capability.",
        "id": 1122
    },
    {
        "cve_id": "CVE-2015-8955",
        "code_before_change": "static int\nvalidate_group(struct perf_event *event)\n{\n\tstruct perf_event *sibling, *leader = event->group_leader;\n\tstruct pmu_hw_events fake_pmu;\n\tDECLARE_BITMAP(fake_used_mask, ARMPMU_MAX_HWEVENTS);\n\n\t/*\n\t * Initialise the fake PMU. We only need to populate the\n\t * used_mask for the purposes of validation.\n\t */\n\tmemset(fake_used_mask, 0, sizeof(fake_used_mask));\n\tfake_pmu.used_mask = fake_used_mask;\n\n\tif (!validate_event(&fake_pmu, leader))\n\t\treturn -EINVAL;\n\n\tlist_for_each_entry(sibling, &leader->sibling_list, group_entry) {\n\t\tif (!validate_event(&fake_pmu, sibling))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!validate_event(&fake_pmu, event))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}",
        "code_after_change": "static int\nvalidate_group(struct perf_event *event)\n{\n\tstruct perf_event *sibling, *leader = event->group_leader;\n\tstruct pmu_hw_events fake_pmu;\n\tDECLARE_BITMAP(fake_used_mask, ARMPMU_MAX_HWEVENTS);\n\n\t/*\n\t * Initialise the fake PMU. We only need to populate the\n\t * used_mask for the purposes of validation.\n\t */\n\tmemset(fake_used_mask, 0, sizeof(fake_used_mask));\n\tfake_pmu.used_mask = fake_used_mask;\n\n\tif (!validate_event(event->pmu, &fake_pmu, leader))\n\t\treturn -EINVAL;\n\n\tlist_for_each_entry(sibling, &leader->sibling_list, group_entry) {\n\t\tif (!validate_event(event->pmu, &fake_pmu, sibling))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!validate_event(event->pmu, &fake_pmu, event))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,15 +12,15 @@\n \tmemset(fake_used_mask, 0, sizeof(fake_used_mask));\n \tfake_pmu.used_mask = fake_used_mask;\n \n-\tif (!validate_event(&fake_pmu, leader))\n+\tif (!validate_event(event->pmu, &fake_pmu, leader))\n \t\treturn -EINVAL;\n \n \tlist_for_each_entry(sibling, &leader->sibling_list, group_entry) {\n-\t\tif (!validate_event(&fake_pmu, sibling))\n+\t\tif (!validate_event(event->pmu, &fake_pmu, sibling))\n \t\t\treturn -EINVAL;\n \t}\n \n-\tif (!validate_event(&fake_pmu, event))\n+\tif (!validate_event(event->pmu, &fake_pmu, event))\n \t\treturn -EINVAL;\n \n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\tif (!validate_event(event->pmu, &fake_pmu, leader))",
                "\t\tif (!validate_event(event->pmu, &fake_pmu, sibling))",
                "\tif (!validate_event(event->pmu, &fake_pmu, event))"
            ],
            "deleted": [
                "\tif (!validate_event(&fake_pmu, leader))",
                "\t\tif (!validate_event(&fake_pmu, sibling))",
                "\tif (!validate_event(&fake_pmu, event))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "arch/arm64/kernel/perf_event.c in the Linux kernel before 4.1 on arm64 platforms allows local users to gain privileges or cause a denial of service (invalid pointer dereference) via vectors involving events that are mishandled during a span of multiple HW PMUs.",
        "id": 867
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 0x1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq = 0;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &ipv6_mapped;\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (!saddr) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp &&\n\t    ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr))\n\t\ttcp_fetch_timewait_stamp(sk, dst);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (np->opt)\n\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n\t\t\t\t\t  np->opt->opt_nflen);\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (!tp->write_seq && likely(!tp->repair))\n\t\ttp->write_seq = secure_tcpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t\t     sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t\t     inet->inet_sport,\n\t\t\t\t\t\t\t     inet->inet_dport);\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
        "code_after_change": "static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct ipv6_txoptions *opt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 0x1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq = 0;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &ipv6_mapped;\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (!saddr) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp &&\n\t    ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr))\n\t\ttcp_fetch_timewait_stamp(sk, dst);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen +\n\t\t\t\t\t opt->opt_nflen;\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (!tp->write_seq && likely(!tp->repair))\n\t\ttp->write_seq = secure_tcpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t\t     sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t\t     inet->inet_sport,\n\t\t\t\t\t\t\t     inet->inet_dport);\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \tstruct ipv6_pinfo *np = inet6_sk(sk);\n \tstruct tcp_sock *tp = tcp_sk(sk);\n \tstruct in6_addr *saddr = NULL, *final_p, final;\n+\tstruct ipv6_txoptions *opt;\n \tstruct flowi6 fl6;\n \tstruct dst_entry *dst;\n \tint addr_type;\n@@ -122,7 +123,8 @@\n \tfl6.fl6_dport = usin->sin6_port;\n \tfl6.fl6_sport = inet->inet_sport;\n \n-\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n+\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n+\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n \n \tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n \n@@ -150,9 +152,9 @@\n \t\ttcp_fetch_timewait_stamp(sk, dst);\n \n \ticsk->icsk_ext_hdr_len = 0;\n-\tif (np->opt)\n-\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n-\t\t\t\t\t  np->opt->opt_nflen);\n+\tif (opt)\n+\t\ticsk->icsk_ext_hdr_len = opt->opt_flen +\n+\t\t\t\t\t opt->opt_nflen;\n \n \ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct ipv6_txoptions *opt;",
                "\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));",
                "\tfinal_p = fl6_update_dst(&fl6, opt, &final);",
                "\tif (opt)",
                "\t\ticsk->icsk_ext_hdr_len = opt->opt_flen +",
                "\t\t\t\t\t opt->opt_nflen;"
            ],
            "deleted": [
                "\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);",
                "\tif (np->opt)",
                "\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +",
                "\t\t\t\t\t  np->opt->opt_nflen);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1007
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static struct dst_entry *inet6_csk_route_socket(struct sock *sk,\n\t\t\t\t\t\tstruct flowi6 *fl6)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = sk->sk_protocol;\n\tfl6->daddr = sk->sk_v6_daddr;\n\tfl6->saddr = np->saddr;\n\tfl6->flowlabel = np->flow_label;\n\tIP6_ECN_flow_xmit(sk, fl6->flowlabel);\n\tfl6->flowi6_oif = sk->sk_bound_dev_if;\n\tfl6->flowi6_mark = sk->sk_mark;\n\tfl6->fl6_sport = inet->inet_sport;\n\tfl6->fl6_dport = inet->inet_dport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(fl6));\n\n\tfinal_p = fl6_update_dst(fl6, np->opt, &final);\n\n\tdst = __inet6_csk_dst_check(sk, np->dst_cookie);\n\tif (!dst) {\n\t\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\n\t\tif (!IS_ERR(dst))\n\t\t\t__inet6_csk_dst_store(sk, dst, NULL, NULL);\n\t}\n\treturn dst;\n}",
        "code_after_change": "static struct dst_entry *inet6_csk_route_socket(struct sock *sk,\n\t\t\t\t\t\tstruct flowi6 *fl6)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = sk->sk_protocol;\n\tfl6->daddr = sk->sk_v6_daddr;\n\tfl6->saddr = np->saddr;\n\tfl6->flowlabel = np->flow_label;\n\tIP6_ECN_flow_xmit(sk, fl6->flowlabel);\n\tfl6->flowi6_oif = sk->sk_bound_dev_if;\n\tfl6->flowi6_mark = sk->sk_mark;\n\tfl6->fl6_sport = inet->inet_sport;\n\tfl6->fl6_dport = inet->inet_dport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(fl6));\n\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\n\tdst = __inet6_csk_dst_check(sk, np->dst_cookie);\n\tif (!dst) {\n\t\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\n\t\tif (!IS_ERR(dst))\n\t\t\t__inet6_csk_dst_store(sk, dst, NULL, NULL);\n\t}\n\treturn dst;\n}",
        "patch": "--- code before\n+++ code after\n@@ -18,7 +18,9 @@\n \tfl6->fl6_dport = inet->inet_dport;\n \tsecurity_sk_classify_flow(sk, flowi6_to_flowi(fl6));\n \n-\tfinal_p = fl6_update_dst(fl6, np->opt, &final);\n+\trcu_read_lock();\n+\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n+\trcu_read_unlock();\n \n \tdst = __inet6_csk_dst_check(sk, np->dst_cookie);\n \tif (!dst) {",
        "function_modified_lines": {
            "added": [
                "\trcu_read_lock();",
                "\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);",
                "\trcu_read_unlock();"
            ],
            "deleted": [
                "\tfinal_p = fl6_update_dst(fl6, np->opt, &final);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 997
    },
    {
        "cve_id": "CVE-2013-4299",
        "code_before_change": "static int read_exceptions(struct pstore *ps,\n\t\t\t   int (*callback)(void *callback_context, chunk_t old,\n\t\t\t\t\t   chunk_t new),\n\t\t\t   void *callback_context)\n{\n\tint r, full = 1;\n\n\t/*\n\t * Keeping reading chunks and inserting exceptions until\n\t * we find a partially full area.\n\t */\n\tfor (ps->current_area = 0; full; ps->current_area++) {\n\t\tr = area_io(ps, READ);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = insert_exceptions(ps, callback, callback_context, &full);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tps->current_area--;\n\n\treturn 0;\n}",
        "code_after_change": "static int read_exceptions(struct pstore *ps,\n\t\t\t   int (*callback)(void *callback_context, chunk_t old,\n\t\t\t\t\t   chunk_t new),\n\t\t\t   void *callback_context)\n{\n\tint r, full = 1;\n\n\t/*\n\t * Keeping reading chunks and inserting exceptions until\n\t * we find a partially full area.\n\t */\n\tfor (ps->current_area = 0; full; ps->current_area++) {\n\t\tr = area_io(ps, READ);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = insert_exceptions(ps, callback, callback_context, &full);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tps->current_area--;\n\n\tskip_metadata(ps);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,5 +21,7 @@\n \n \tps->current_area--;\n \n+\tskip_metadata(ps);\n+\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tskip_metadata(ps);",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264",
            "CWE-200"
        ],
        "cve_description": "Interpretation conflict in drivers/md/dm-snap-persistent.c in the Linux kernel through 3.11.6 allows remote authenticated users to obtain sensitive information or modify data via a crafted mapping to a snapshot block device.",
        "id": 292
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "void inet6_destroy_sock(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct ipv6_txoptions *opt;\n\n\t/* Release rx options */\n\n\tskb = xchg(&np->pktoptions, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\tskb = xchg(&np->rxpmtu, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\t/* Free flowlabels */\n\tfl6_free_socklist(sk);\n\n\t/* Free tx options */\n\n\topt = xchg(&np->opt, NULL);\n\tif (opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n}",
        "code_after_change": "void inet6_destroy_sock(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct ipv6_txoptions *opt;\n\n\t/* Release rx options */\n\n\tskb = xchg(&np->pktoptions, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\tskb = xchg(&np->rxpmtu, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\t/* Free flowlabels */\n\tfl6_free_socklist(sk);\n\n\t/* Free tx options */\n\n\topt = xchg((__force struct ipv6_txoptions **)&np->opt, NULL);\n\tif (opt) {\n\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\ttxopt_put(opt);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,7 +19,9 @@\n \n \t/* Free tx options */\n \n-\topt = xchg(&np->opt, NULL);\n-\tif (opt)\n-\t\tsock_kfree_s(sk, opt, opt->tot_len);\n+\topt = xchg((__force struct ipv6_txoptions **)&np->opt, NULL);\n+\tif (opt) {\n+\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n+\t\ttxopt_put(opt);\n+\t}\n }",
        "function_modified_lines": {
            "added": [
                "\topt = xchg((__force struct ipv6_txoptions **)&np->opt, NULL);",
                "\tif (opt) {",
                "\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);",
                "\t\ttxopt_put(opt);",
                "\t}"
            ],
            "deleted": [
                "\topt = xchg(&np->opt, NULL);",
                "\tif (opt)",
                "\t\tsock_kfree_s(sk, opt, opt->tot_len);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 992
    },
    {
        "cve_id": "CVE-2016-9120",
        "code_before_change": "static long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tion_free(client, handle);\n\t\tion_handle_put(handle);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}",
        "code_after_change": "static long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tion_free_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -43,11 +43,15 @@\n \t{\n \t\tstruct ion_handle *handle;\n \n-\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n-\t\tif (IS_ERR(handle))\n+\t\tmutex_lock(&client->lock);\n+\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n+\t\tif (IS_ERR(handle)) {\n+\t\t\tmutex_unlock(&client->lock);\n \t\t\treturn PTR_ERR(handle);\n-\t\tion_free(client, handle);\n-\t\tion_handle_put(handle);\n+\t\t}\n+\t\tion_free_nolock(client, handle);\n+\t\tion_handle_put_nolock(handle);\n+\t\tmutex_unlock(&client->lock);\n \t\tbreak;\n \t}\n \tcase ION_IOC_SHARE:",
        "function_modified_lines": {
            "added": [
                "\t\tmutex_lock(&client->lock);",
                "\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);",
                "\t\tif (IS_ERR(handle)) {",
                "\t\t\tmutex_unlock(&client->lock);",
                "\t\t}",
                "\t\tion_free_nolock(client, handle);",
                "\t\tion_handle_put_nolock(handle);",
                "\t\tmutex_unlock(&client->lock);"
            ],
            "deleted": [
                "\t\thandle = ion_handle_get_by_id(client, data.handle.handle);",
                "\t\tif (IS_ERR(handle))",
                "\t\tion_free(client, handle);",
                "\t\tion_handle_put(handle);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "Race condition in the ion_ioctl function in drivers/staging/android/ion/ion.c in the Linux kernel before 4.6 allows local users to gain privileges or cause a denial of service (use-after-free) by calling ION_IOC_FREE on two CPUs at the same time.",
        "id": 1140
    },
    {
        "cve_id": "CVE-2016-9644",
        "code_before_change": "int kprobe_fault_handler(struct pt_regs *regs, int trapnr)\n{\n\tstruct kprobe *cur = kprobe_running();\n\tstruct kprobe_ctlblk *kcb = get_kprobe_ctlblk();\n\n\tif (unlikely(regs->ip == (unsigned long)cur->ainsn.insn)) {\n\t\t/* This must happen on single-stepping */\n\t\tWARN_ON(kcb->kprobe_status != KPROBE_HIT_SS &&\n\t\t\tkcb->kprobe_status != KPROBE_REENTER);\n\t\t/*\n\t\t * We are here because the instruction being single\n\t\t * stepped caused a page fault. We reset the current\n\t\t * kprobe and the ip points back to the probe address\n\t\t * and allow the page fault handler to continue as a\n\t\t * normal page fault.\n\t\t */\n\t\tregs->ip = (unsigned long)cur->addr;\n\t\tregs->flags |= kcb->kprobe_old_flags;\n\t\tif (kcb->kprobe_status == KPROBE_REENTER)\n\t\t\trestore_previous_kprobe(kcb);\n\t\telse\n\t\t\treset_current_kprobe();\n\t\tpreempt_enable_no_resched();\n\t} else if (kcb->kprobe_status == KPROBE_HIT_ACTIVE ||\n\t\t   kcb->kprobe_status == KPROBE_HIT_SSDONE) {\n\t\t/*\n\t\t * We increment the nmissed count for accounting,\n\t\t * we can also use npre/npostfault count for accounting\n\t\t * these specific fault cases.\n\t\t */\n\t\tkprobes_inc_nmissed_count(cur);\n\n\t\t/*\n\t\t * We come here because instructions in the pre/post\n\t\t * handler caused the page_fault, this could happen\n\t\t * if handler tries to access user space by\n\t\t * copy_from_user(), get_user() etc. Let the\n\t\t * user-specified handler try to fix it first.\n\t\t */\n\t\tif (cur->fault_handler && cur->fault_handler(cur, regs, trapnr))\n\t\t\treturn 1;\n\n\t\t/*\n\t\t * In case the user-specified fault handler returned\n\t\t * zero, try to fix up.\n\t\t */\n\t\tif (fixup_exception(regs))\n\t\t\treturn 1;\n\n\t\t/*\n\t\t * fixup routine could not handle it,\n\t\t * Let do_page_fault() fix it.\n\t\t */\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "int kprobe_fault_handler(struct pt_regs *regs, int trapnr)\n{\n\tstruct kprobe *cur = kprobe_running();\n\tstruct kprobe_ctlblk *kcb = get_kprobe_ctlblk();\n\n\tif (unlikely(regs->ip == (unsigned long)cur->ainsn.insn)) {\n\t\t/* This must happen on single-stepping */\n\t\tWARN_ON(kcb->kprobe_status != KPROBE_HIT_SS &&\n\t\t\tkcb->kprobe_status != KPROBE_REENTER);\n\t\t/*\n\t\t * We are here because the instruction being single\n\t\t * stepped caused a page fault. We reset the current\n\t\t * kprobe and the ip points back to the probe address\n\t\t * and allow the page fault handler to continue as a\n\t\t * normal page fault.\n\t\t */\n\t\tregs->ip = (unsigned long)cur->addr;\n\t\tregs->flags |= kcb->kprobe_old_flags;\n\t\tif (kcb->kprobe_status == KPROBE_REENTER)\n\t\t\trestore_previous_kprobe(kcb);\n\t\telse\n\t\t\treset_current_kprobe();\n\t\tpreempt_enable_no_resched();\n\t} else if (kcb->kprobe_status == KPROBE_HIT_ACTIVE ||\n\t\t   kcb->kprobe_status == KPROBE_HIT_SSDONE) {\n\t\t/*\n\t\t * We increment the nmissed count for accounting,\n\t\t * we can also use npre/npostfault count for accounting\n\t\t * these specific fault cases.\n\t\t */\n\t\tkprobes_inc_nmissed_count(cur);\n\n\t\t/*\n\t\t * We come here because instructions in the pre/post\n\t\t * handler caused the page_fault, this could happen\n\t\t * if handler tries to access user space by\n\t\t * copy_from_user(), get_user() etc. Let the\n\t\t * user-specified handler try to fix it first.\n\t\t */\n\t\tif (cur->fault_handler && cur->fault_handler(cur, regs, trapnr))\n\t\t\treturn 1;\n\n\t\t/*\n\t\t * In case the user-specified fault handler returned\n\t\t * zero, try to fix up.\n\t\t */\n\t\tif (fixup_exception(regs, trapnr))\n\t\t\treturn 1;\n\n\t\t/*\n\t\t * fixup routine could not handle it,\n\t\t * Let do_page_fault() fix it.\n\t\t */\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -44,7 +44,7 @@\n \t\t * In case the user-specified fault handler returned\n \t\t * zero, try to fix up.\n \t\t */\n-\t\tif (fixup_exception(regs))\n+\t\tif (fixup_exception(regs, trapnr))\n \t\t\treturn 1;\n \n \t\t/*",
        "function_modified_lines": {
            "added": [
                "\t\tif (fixup_exception(regs, trapnr))"
            ],
            "deleted": [
                "\t\tif (fixup_exception(regs))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The __get_user_asm_ex macro in arch/x86/include/asm/uaccess.h in the Linux kernel 4.4.22 through 4.4.28 contains extended asm statements that are incompatible with the exception table, which allows local users to obtain root access on non-SMEP platforms via a crafted application.  NOTE: this vulnerability exists because of incorrect backporting of the CVE-2016-9178 patch to older kernels.",
        "id": 1151
    },
    {
        "cve_id": "CVE-2014-0181",
        "code_before_change": "static int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}",
        "code_after_change": "static int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,7 +13,7 @@\n \tu32 qid;\n \tint err;\n \n-\tif ((n->nlmsg_type != RTM_GETTCLASS) && !capable(CAP_NET_ADMIN))\n+\tif ((n->nlmsg_type != RTM_GETTCLASS) && !netlink_capable(skb, CAP_NET_ADMIN))\n \t\treturn -EPERM;\n \n \terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);",
        "function_modified_lines": {
            "added": [
                "\tif ((n->nlmsg_type != RTM_GETTCLASS) && !netlink_capable(skb, CAP_NET_ADMIN))"
            ],
            "deleted": [
                "\tif ((n->nlmsg_type != RTM_GETTCLASS) && !capable(CAP_NET_ADMIN))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Netlink implementation in the Linux kernel through 3.14.1 does not provide a mechanism for authorizing socket operations based on the opener of a socket, which allows local users to bypass intended access restrictions and modify network configurations by using a Netlink socket for the (1) stdout or (2) stderr of a setuid program.",
        "id": 458
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\topt = flowlabel ? flowlabel->opt : np->opt;\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}",
        "code_after_change": "static int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\trcu_read_lock();\n\topt = flowlabel ? flowlabel->opt : rcu_dereference(np->opt);\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -125,8 +125,10 @@\n \n \tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n \n-\topt = flowlabel ? flowlabel->opt : np->opt;\n+\trcu_read_lock();\n+\topt = flowlabel ? flowlabel->opt : rcu_dereference(np->opt);\n \tfinal_p = fl6_update_dst(&fl6, opt, &final);\n+\trcu_read_unlock();\n \n \tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n \terr = 0;",
        "function_modified_lines": {
            "added": [
                "\trcu_read_lock();",
                "\topt = flowlabel ? flowlabel->opt : rcu_dereference(np->opt);",
                "\trcu_read_unlock();"
            ],
            "deleted": [
                "\topt = flowlabel ? flowlabel->opt : np->opt;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 994
    },
    {
        "cve_id": "CVE-2016-4997",
        "code_before_change": "static int\ncheck_compat_entry_size_and_hooks(struct compat_ipt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ipt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ipt_entry) - sizeof(struct compat_ipt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ip, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ipt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV4, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}",
        "code_after_change": "static int\ncheck_compat_entry_size_and_hooks(struct compat_ipt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ipt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->elems,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ipt_entry) - sizeof(struct compat_ipt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ip, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ipt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV4, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -33,7 +33,7 @@\n \tif (!ip_checkentry(&e->ip))\n \t\treturn -EINVAL;\n \n-\tret = xt_compat_check_entry_offsets(e,\n+\tret = xt_compat_check_entry_offsets(e, e->elems,\n \t\t\t\t\t    e->target_offset, e->next_offset);\n \tif (ret)\n \t\treturn ret;",
        "function_modified_lines": {
            "added": [
                "\tret = xt_compat_check_entry_offsets(e, e->elems,"
            ],
            "deleted": [
                "\tret = xt_compat_check_entry_offsets(e,"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The compat IPT_SO_SET_REPLACE and IP6T_SO_SET_REPLACE setsockopt implementations in the netfilter subsystem in the Linux kernel before 4.6.3 allow local users to gain privileges or cause a denial of service (memory corruption) by leveraging in-container root access to provide a crafted offset value that triggers an unintended decrement.",
        "id": 1042
    },
    {
        "cve_id": "CVE-2014-4014",
        "code_before_change": "void setattr_copy(struct inode *inode, const struct iattr *attr)\n{\n\tunsigned int ia_valid = attr->ia_valid;\n\n\tif (ia_valid & ATTR_UID)\n\t\tinode->i_uid = attr->ia_uid;\n\tif (ia_valid & ATTR_GID)\n\t\tinode->i_gid = attr->ia_gid;\n\tif (ia_valid & ATTR_ATIME)\n\t\tinode->i_atime = timespec_trunc(attr->ia_atime,\n\t\t\t\t\t\tinode->i_sb->s_time_gran);\n\tif (ia_valid & ATTR_MTIME)\n\t\tinode->i_mtime = timespec_trunc(attr->ia_mtime,\n\t\t\t\t\t\tinode->i_sb->s_time_gran);\n\tif (ia_valid & ATTR_CTIME)\n\t\tinode->i_ctime = timespec_trunc(attr->ia_ctime,\n\t\t\t\t\t\tinode->i_sb->s_time_gran);\n\tif (ia_valid & ATTR_MODE) {\n\t\tumode_t mode = attr->ia_mode;\n\n\t\tif (!in_group_p(inode->i_gid) &&\n\t\t    !inode_capable(inode, CAP_FSETID))\n\t\t\tmode &= ~S_ISGID;\n\t\tinode->i_mode = mode;\n\t}\n}",
        "code_after_change": "void setattr_copy(struct inode *inode, const struct iattr *attr)\n{\n\tunsigned int ia_valid = attr->ia_valid;\n\n\tif (ia_valid & ATTR_UID)\n\t\tinode->i_uid = attr->ia_uid;\n\tif (ia_valid & ATTR_GID)\n\t\tinode->i_gid = attr->ia_gid;\n\tif (ia_valid & ATTR_ATIME)\n\t\tinode->i_atime = timespec_trunc(attr->ia_atime,\n\t\t\t\t\t\tinode->i_sb->s_time_gran);\n\tif (ia_valid & ATTR_MTIME)\n\t\tinode->i_mtime = timespec_trunc(attr->ia_mtime,\n\t\t\t\t\t\tinode->i_sb->s_time_gran);\n\tif (ia_valid & ATTR_CTIME)\n\t\tinode->i_ctime = timespec_trunc(attr->ia_ctime,\n\t\t\t\t\t\tinode->i_sb->s_time_gran);\n\tif (ia_valid & ATTR_MODE) {\n\t\tumode_t mode = attr->ia_mode;\n\n\t\tif (!in_group_p(inode->i_gid) &&\n\t\t    !capable_wrt_inode_uidgid(inode, CAP_FSETID))\n\t\t\tmode &= ~S_ISGID;\n\t\tinode->i_mode = mode;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,7 +19,7 @@\n \t\tumode_t mode = attr->ia_mode;\n \n \t\tif (!in_group_p(inode->i_gid) &&\n-\t\t    !inode_capable(inode, CAP_FSETID))\n+\t\t    !capable_wrt_inode_uidgid(inode, CAP_FSETID))\n \t\t\tmode &= ~S_ISGID;\n \t\tinode->i_mode = mode;\n \t}",
        "function_modified_lines": {
            "added": [
                "\t\t    !capable_wrt_inode_uidgid(inode, CAP_FSETID))"
            ],
            "deleted": [
                "\t\t    !inode_capable(inode, CAP_FSETID))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The capabilities implementation in the Linux kernel before 3.14.8 does not properly consider that namespaces are inapplicable to inodes, which allows local users to bypass intended chmod restrictions by first creating a user namespace, as demonstrated by setting the setgid bit on a file with group ownership of root.",
        "id": 550
    },
    {
        "cve_id": "CVE-2014-9870",
        "code_before_change": "long arch_ptrace(struct task_struct *child, long request,\n\t\t unsigned long addr, unsigned long data)\n{\n\tint ret;\n\tunsigned long __user *datap = (unsigned long __user *) data;\n\n\tswitch (request) {\n\t\tcase PTRACE_PEEKUSR:\n\t\t\tret = ptrace_read_user(child, addr, datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_POKEUSR:\n\t\t\tret = ptrace_write_user(child, addr, data);\n\t\t\tbreak;\n\n\t\tcase PTRACE_GETREGS:\n\t\t\tret = copy_regset_to_user(child,\n\t\t\t\t\t\t  &user_arm_view, REGSET_GPR,\n\t\t\t\t\t\t  0, sizeof(struct pt_regs),\n\t\t\t\t\t\t  datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETREGS:\n\t\t\tret = copy_regset_from_user(child,\n\t\t\t\t\t\t    &user_arm_view, REGSET_GPR,\n\t\t\t\t\t\t    0, sizeof(struct pt_regs),\n\t\t\t\t\t\t    datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_GETFPREGS:\n\t\t\tret = copy_regset_to_user(child,\n\t\t\t\t\t\t  &user_arm_view, REGSET_FPR,\n\t\t\t\t\t\t  0, sizeof(union fp_state),\n\t\t\t\t\t\t  datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETFPREGS:\n\t\t\tret = copy_regset_from_user(child,\n\t\t\t\t\t\t    &user_arm_view, REGSET_FPR,\n\t\t\t\t\t\t    0, sizeof(union fp_state),\n\t\t\t\t\t\t    datap);\n\t\t\tbreak;\n\n#ifdef CONFIG_IWMMXT\n\t\tcase PTRACE_GETWMMXREGS:\n\t\t\tret = ptrace_getwmmxregs(child, datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETWMMXREGS:\n\t\t\tret = ptrace_setwmmxregs(child, datap);\n\t\t\tbreak;\n#endif\n\n\t\tcase PTRACE_GET_THREAD_AREA:\n\t\t\tret = put_user(task_thread_info(child)->tp_value,\n\t\t\t\t       datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SET_SYSCALL:\n\t\t\ttask_thread_info(child)->syscall = data;\n\t\t\tret = 0;\n\t\t\tbreak;\n\n#ifdef CONFIG_CRUNCH\n\t\tcase PTRACE_GETCRUNCHREGS:\n\t\t\tret = ptrace_getcrunchregs(child, datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETCRUNCHREGS:\n\t\t\tret = ptrace_setcrunchregs(child, datap);\n\t\t\tbreak;\n#endif\n\n#ifdef CONFIG_VFP\n\t\tcase PTRACE_GETVFPREGS:\n\t\t\tret = copy_regset_to_user(child,\n\t\t\t\t\t\t  &user_arm_view, REGSET_VFP,\n\t\t\t\t\t\t  0, ARM_VFPREGS_SIZE,\n\t\t\t\t\t\t  datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETVFPREGS:\n\t\t\tret = copy_regset_from_user(child,\n\t\t\t\t\t\t    &user_arm_view, REGSET_VFP,\n\t\t\t\t\t\t    0, ARM_VFPREGS_SIZE,\n\t\t\t\t\t\t    datap);\n\t\t\tbreak;\n#endif\n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n\t\tcase PTRACE_GETHBPREGS:\n\t\t\tif (ptrace_get_breakpoints(child) < 0)\n\t\t\t\treturn -ESRCH;\n\n\t\t\tret = ptrace_gethbpregs(child, addr,\n\t\t\t\t\t\t(unsigned long __user *)data);\n\t\t\tptrace_put_breakpoints(child);\n\t\t\tbreak;\n\t\tcase PTRACE_SETHBPREGS:\n\t\t\tif (ptrace_get_breakpoints(child) < 0)\n\t\t\t\treturn -ESRCH;\n\n\t\t\tret = ptrace_sethbpregs(child, addr,\n\t\t\t\t\t\t(unsigned long __user *)data);\n\t\t\tptrace_put_breakpoints(child);\n\t\t\tbreak;\n#endif\n\n\t\tdefault:\n\t\t\tret = ptrace_request(child, request, addr, data);\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}",
        "code_after_change": "long arch_ptrace(struct task_struct *child, long request,\n\t\t unsigned long addr, unsigned long data)\n{\n\tint ret;\n\tunsigned long __user *datap = (unsigned long __user *) data;\n\n\tswitch (request) {\n\t\tcase PTRACE_PEEKUSR:\n\t\t\tret = ptrace_read_user(child, addr, datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_POKEUSR:\n\t\t\tret = ptrace_write_user(child, addr, data);\n\t\t\tbreak;\n\n\t\tcase PTRACE_GETREGS:\n\t\t\tret = copy_regset_to_user(child,\n\t\t\t\t\t\t  &user_arm_view, REGSET_GPR,\n\t\t\t\t\t\t  0, sizeof(struct pt_regs),\n\t\t\t\t\t\t  datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETREGS:\n\t\t\tret = copy_regset_from_user(child,\n\t\t\t\t\t\t    &user_arm_view, REGSET_GPR,\n\t\t\t\t\t\t    0, sizeof(struct pt_regs),\n\t\t\t\t\t\t    datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_GETFPREGS:\n\t\t\tret = copy_regset_to_user(child,\n\t\t\t\t\t\t  &user_arm_view, REGSET_FPR,\n\t\t\t\t\t\t  0, sizeof(union fp_state),\n\t\t\t\t\t\t  datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETFPREGS:\n\t\t\tret = copy_regset_from_user(child,\n\t\t\t\t\t\t    &user_arm_view, REGSET_FPR,\n\t\t\t\t\t\t    0, sizeof(union fp_state),\n\t\t\t\t\t\t    datap);\n\t\t\tbreak;\n\n#ifdef CONFIG_IWMMXT\n\t\tcase PTRACE_GETWMMXREGS:\n\t\t\tret = ptrace_getwmmxregs(child, datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETWMMXREGS:\n\t\t\tret = ptrace_setwmmxregs(child, datap);\n\t\t\tbreak;\n#endif\n\n\t\tcase PTRACE_GET_THREAD_AREA:\n\t\t\tret = put_user(task_thread_info(child)->tp_value[0],\n\t\t\t\t       datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SET_SYSCALL:\n\t\t\ttask_thread_info(child)->syscall = data;\n\t\t\tret = 0;\n\t\t\tbreak;\n\n#ifdef CONFIG_CRUNCH\n\t\tcase PTRACE_GETCRUNCHREGS:\n\t\t\tret = ptrace_getcrunchregs(child, datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETCRUNCHREGS:\n\t\t\tret = ptrace_setcrunchregs(child, datap);\n\t\t\tbreak;\n#endif\n\n#ifdef CONFIG_VFP\n\t\tcase PTRACE_GETVFPREGS:\n\t\t\tret = copy_regset_to_user(child,\n\t\t\t\t\t\t  &user_arm_view, REGSET_VFP,\n\t\t\t\t\t\t  0, ARM_VFPREGS_SIZE,\n\t\t\t\t\t\t  datap);\n\t\t\tbreak;\n\n\t\tcase PTRACE_SETVFPREGS:\n\t\t\tret = copy_regset_from_user(child,\n\t\t\t\t\t\t    &user_arm_view, REGSET_VFP,\n\t\t\t\t\t\t    0, ARM_VFPREGS_SIZE,\n\t\t\t\t\t\t    datap);\n\t\t\tbreak;\n#endif\n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n\t\tcase PTRACE_GETHBPREGS:\n\t\t\tif (ptrace_get_breakpoints(child) < 0)\n\t\t\t\treturn -ESRCH;\n\n\t\t\tret = ptrace_gethbpregs(child, addr,\n\t\t\t\t\t\t(unsigned long __user *)data);\n\t\t\tptrace_put_breakpoints(child);\n\t\t\tbreak;\n\t\tcase PTRACE_SETHBPREGS:\n\t\t\tif (ptrace_get_breakpoints(child) < 0)\n\t\t\t\treturn -ESRCH;\n\n\t\t\tret = ptrace_sethbpregs(child, addr,\n\t\t\t\t\t\t(unsigned long __user *)data);\n\t\t\tptrace_put_breakpoints(child);\n\t\t\tbreak;\n#endif\n\n\t\tdefault:\n\t\t\tret = ptrace_request(child, request, addr, data);\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -52,7 +52,7 @@\n #endif\n \n \t\tcase PTRACE_GET_THREAD_AREA:\n-\t\t\tret = put_user(task_thread_info(child)->tp_value,\n+\t\t\tret = put_user(task_thread_info(child)->tp_value[0],\n \t\t\t\t       datap);\n \t\t\tbreak;\n ",
        "function_modified_lines": {
            "added": [
                "\t\t\tret = put_user(task_thread_info(child)->tp_value[0],"
            ],
            "deleted": [
                "\t\t\tret = put_user(task_thread_info(child)->tp_value,"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Linux kernel before 3.11 on ARM platforms, as used in Android before 2016-08-05 on Nexus 5 and 7 (2013) devices, does not properly consider user-space access to the TPIDRURW register, which allows local users to gain privileges via a crafted application, aka Android internal bug 28749743 and Qualcomm internal bug CR561044.",
        "id": 703
    },
    {
        "cve_id": "CVE-2016-10044",
        "code_before_change": "static struct dentry *aio_mount(struct file_system_type *fs_type,\n\t\t\t\tint flags, const char *dev_name, void *data)\n{\n\tstatic const struct dentry_operations ops = {\n\t\t.d_dname\t= simple_dname,\n\t};\n\treturn mount_pseudo(fs_type, \"aio:\", NULL, &ops, AIO_RING_MAGIC);\n}",
        "code_after_change": "static struct dentry *aio_mount(struct file_system_type *fs_type,\n\t\t\t\tint flags, const char *dev_name, void *data)\n{\n\tstatic const struct dentry_operations ops = {\n\t\t.d_dname\t= simple_dname,\n\t};\n\tstruct dentry *root = mount_pseudo(fs_type, \"aio:\", NULL, &ops,\n\t\t\t\t\t   AIO_RING_MAGIC);\n\n\tif (!IS_ERR(root))\n\t\troot->d_sb->s_iflags |= SB_I_NOEXEC;\n\treturn root;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,5 +4,10 @@\n \tstatic const struct dentry_operations ops = {\n \t\t.d_dname\t= simple_dname,\n \t};\n-\treturn mount_pseudo(fs_type, \"aio:\", NULL, &ops, AIO_RING_MAGIC);\n+\tstruct dentry *root = mount_pseudo(fs_type, \"aio:\", NULL, &ops,\n+\t\t\t\t\t   AIO_RING_MAGIC);\n+\n+\tif (!IS_ERR(root))\n+\t\troot->d_sb->s_iflags |= SB_I_NOEXEC;\n+\treturn root;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct dentry *root = mount_pseudo(fs_type, \"aio:\", NULL, &ops,",
                "\t\t\t\t\t   AIO_RING_MAGIC);",
                "",
                "\tif (!IS_ERR(root))",
                "\t\troot->d_sb->s_iflags |= SB_I_NOEXEC;",
                "\treturn root;"
            ],
            "deleted": [
                "\treturn mount_pseudo(fs_type, \"aio:\", NULL, &ops, AIO_RING_MAGIC);"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The aio_mount function in fs/aio.c in the Linux kernel before 4.7.7 does not properly restrict execute access, which makes it easier for local users to bypass intended SELinux W^X policy restrictions, and consequently gain privileges, via an io_setup system call.",
        "id": 892
    },
    {
        "cve_id": "CVE-2016-10200",
        "code_before_change": "static int l2tp_ip6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_l2tpip6 *addr = (struct sockaddr_l2tpip6 *) uaddr;\n\tstruct net *net = sock_net(sk);\n\t__be32 v4addr = 0;\n\tint addr_type;\n\tint err;\n\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\treturn -EINVAL;\n\tif (addr->l2tp_family != AF_INET6)\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(*addr))\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&addr->l2tp_addr);\n\n\t/* l2tp_ip6 sockets are IPv6 only */\n\tif (addr_type == IPV6_ADDR_MAPPED)\n\t\treturn -EADDRNOTAVAIL;\n\n\t/* L2TP is point-point, not multicast */\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -EADDRNOTAVAIL;\n\n\terr = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip6_lock);\n\tif (__l2tp_ip6_bind_lookup(net, &addr->l2tp_addr,\n\t\t\t\t   sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\tread_unlock_bh(&l2tp_ip6_lock);\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_CLOSE)\n\t\tgoto out_unlock;\n\n\t/* Check if the address belongs to the host. */\n\trcu_read_lock();\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t    addr->l2tp_scope_id) {\n\t\t\t\t/* Override any existing binding, if another\n\t\t\t\t * one is supplied by user.\n\t\t\t\t */\n\t\t\t\tsk->sk_bound_dev_if = addr->l2tp_scope_id;\n\t\t\t}\n\n\t\t\t/* Binding to link-local address requires an\n\t\t\t   interface */\n\t\t\tif (!sk->sk_bound_dev_if)\n\t\t\t\tgoto out_unlock_rcu;\n\n\t\t\terr = -ENODEV;\n\t\t\tdev = dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\t\tif (!dev)\n\t\t\t\tgoto out_unlock_rcu;\n\t\t}\n\n\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t */\n\t\tv4addr = LOOPBACK4_IPV6;\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!ipv6_chk_addr(sock_net(sk), &addr->l2tp_addr, dev, 0))\n\t\t\tgoto out_unlock_rcu;\n\t}\n\trcu_read_unlock();\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = v4addr;\n\tsk->sk_v6_rcv_saddr = addr->l2tp_addr;\n\tnp->saddr = addr->l2tp_addr;\n\n\tl2tp_ip6_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip6_lock);\n\tsk_add_bind_node(sk, &l2tp_ip6_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip6_lock);\n\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\trelease_sock(sk);\n\treturn 0;\n\nout_unlock_rcu:\n\trcu_read_unlock();\nout_unlock:\n\trelease_sock(sk);\n\treturn err;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip6_lock);\n\treturn err;\n}",
        "code_after_change": "static int l2tp_ip6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_l2tpip6 *addr = (struct sockaddr_l2tpip6 *) uaddr;\n\tstruct net *net = sock_net(sk);\n\t__be32 v4addr = 0;\n\tint addr_type;\n\tint err;\n\n\tif (addr->l2tp_family != AF_INET6)\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(*addr))\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&addr->l2tp_addr);\n\n\t/* l2tp_ip6 sockets are IPv6 only */\n\tif (addr_type == IPV6_ADDR_MAPPED)\n\t\treturn -EADDRNOTAVAIL;\n\n\t/* L2TP is point-point, not multicast */\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -EADDRNOTAVAIL;\n\n\terr = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip6_lock);\n\tif (__l2tp_ip6_bind_lookup(net, &addr->l2tp_addr,\n\t\t\t\t   sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\tread_unlock_bh(&l2tp_ip6_lock);\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\tgoto out_unlock;\n\n\tif (sk->sk_state != TCP_CLOSE)\n\t\tgoto out_unlock;\n\n\t/* Check if the address belongs to the host. */\n\trcu_read_lock();\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t    addr->l2tp_scope_id) {\n\t\t\t\t/* Override any existing binding, if another\n\t\t\t\t * one is supplied by user.\n\t\t\t\t */\n\t\t\t\tsk->sk_bound_dev_if = addr->l2tp_scope_id;\n\t\t\t}\n\n\t\t\t/* Binding to link-local address requires an\n\t\t\t   interface */\n\t\t\tif (!sk->sk_bound_dev_if)\n\t\t\t\tgoto out_unlock_rcu;\n\n\t\t\terr = -ENODEV;\n\t\t\tdev = dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\t\tif (!dev)\n\t\t\t\tgoto out_unlock_rcu;\n\t\t}\n\n\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t */\n\t\tv4addr = LOOPBACK4_IPV6;\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!ipv6_chk_addr(sock_net(sk), &addr->l2tp_addr, dev, 0))\n\t\t\tgoto out_unlock_rcu;\n\t}\n\trcu_read_unlock();\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = v4addr;\n\tsk->sk_v6_rcv_saddr = addr->l2tp_addr;\n\tnp->saddr = addr->l2tp_addr;\n\n\tl2tp_ip6_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip6_lock);\n\tsk_add_bind_node(sk, &l2tp_ip6_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip6_lock);\n\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\trelease_sock(sk);\n\treturn 0;\n\nout_unlock_rcu:\n\trcu_read_unlock();\nout_unlock:\n\trelease_sock(sk);\n\treturn err;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip6_lock);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,8 +8,6 @@\n \tint addr_type;\n \tint err;\n \n-\tif (!sock_flag(sk, SOCK_ZAPPED))\n-\t\treturn -EINVAL;\n \tif (addr->l2tp_family != AF_INET6)\n \t\treturn -EINVAL;\n \tif (addr_len < sizeof(*addr))\n@@ -35,6 +33,9 @@\n \tlock_sock(sk);\n \n \terr = -EINVAL;\n+\tif (!sock_flag(sk, SOCK_ZAPPED))\n+\t\tgoto out_unlock;\n+\n \tif (sk->sk_state != TCP_CLOSE)\n \t\tgoto out_unlock;\n ",
        "function_modified_lines": {
            "added": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\tgoto out_unlock;",
                ""
            ],
            "deleted": [
                "\tif (!sock_flag(sk, SOCK_ZAPPED))",
                "\t\treturn -EINVAL;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-362",
            "CWE-416"
        ],
        "cve_description": "Race condition in the L2TPv3 IP Encapsulation feature in the Linux kernel before 4.8.14 allows local users to gain privileges or cause a denial of service (use-after-free) by making multiple bind system calls without properly ascertaining whether a socket has the SOCK_ZAPPED status, related to net/l2tp/l2tp_ip.c and net/l2tp/l2tp_ip6.c.",
        "id": 899
    },
    {
        "cve_id": "CVE-2013-6431",
        "code_before_change": "int fib6_add(struct fib6_node *root, struct rt6_info *rt, struct nl_info *info)\n{\n\tstruct fib6_node *fn, *pn = NULL;\n\tint err = -ENOMEM;\n\tint allow_create = 1;\n\tint replace_required = 0;\n\n\tif (info->nlh) {\n\t\tif (!(info->nlh->nlmsg_flags & NLM_F_CREATE))\n\t\t\tallow_create = 0;\n\t\tif (info->nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\treplace_required = 1;\n\t}\n\tif (!allow_create && !replace_required)\n\t\tpr_warn(\"RTM_NEWROUTE with no NLM_F_CREATE or NLM_F_REPLACE\\n\");\n\n\tfn = fib6_add_1(root, &rt->rt6i_dst.addr, rt->rt6i_dst.plen,\n\t\t\toffsetof(struct rt6_info, rt6i_dst), allow_create,\n\t\t\treplace_required);\n\n\tif (IS_ERR(fn)) {\n\t\terr = PTR_ERR(fn);\n\t\tgoto out;\n\t}\n\n\tpn = fn;\n\n#ifdef CONFIG_IPV6_SUBTREES\n\tif (rt->rt6i_src.plen) {\n\t\tstruct fib6_node *sn;\n\n\t\tif (!fn->subtree) {\n\t\t\tstruct fib6_node *sfn;\n\n\t\t\t/*\n\t\t\t * Create subtree.\n\t\t\t *\n\t\t\t *\t\tfn[main tree]\n\t\t\t *\t\t|\n\t\t\t *\t\tsfn[subtree root]\n\t\t\t *\t\t   \\\n\t\t\t *\t\t    sn[new leaf node]\n\t\t\t */\n\n\t\t\t/* Create subtree root node */\n\t\t\tsfn = node_alloc();\n\t\t\tif (!sfn)\n\t\t\t\tgoto st_failure;\n\n\t\t\tsfn->leaf = info->nl_net->ipv6.ip6_null_entry;\n\t\t\tatomic_inc(&info->nl_net->ipv6.ip6_null_entry->rt6i_ref);\n\t\t\tsfn->fn_flags = RTN_ROOT;\n\t\t\tsfn->fn_sernum = fib6_new_sernum();\n\n\t\t\t/* Now add the first leaf node to new subtree */\n\n\t\t\tsn = fib6_add_1(sfn, &rt->rt6i_src.addr,\n\t\t\t\t\trt->rt6i_src.plen,\n\t\t\t\t\toffsetof(struct rt6_info, rt6i_src),\n\t\t\t\t\tallow_create, replace_required);\n\n\t\t\tif (IS_ERR(sn)) {\n\t\t\t\t/* If it is failed, discard just allocated\n\t\t\t\t   root, and then (in st_failure) stale node\n\t\t\t\t   in main tree.\n\t\t\t\t */\n\t\t\t\tnode_free(sfn);\n\t\t\t\terr = PTR_ERR(sn);\n\t\t\t\tgoto st_failure;\n\t\t\t}\n\n\t\t\t/* Now link new subtree to main tree */\n\t\t\tsfn->parent = fn;\n\t\t\tfn->subtree = sfn;\n\t\t} else {\n\t\t\tsn = fib6_add_1(fn->subtree, &rt->rt6i_src.addr,\n\t\t\t\t\trt->rt6i_src.plen,\n\t\t\t\t\toffsetof(struct rt6_info, rt6i_src),\n\t\t\t\t\tallow_create, replace_required);\n\n\t\t\tif (IS_ERR(sn)) {\n\t\t\t\terr = PTR_ERR(sn);\n\t\t\t\tgoto st_failure;\n\t\t\t}\n\t\t}\n\n\t\tif (!fn->leaf) {\n\t\t\tfn->leaf = rt;\n\t\t\tatomic_inc(&rt->rt6i_ref);\n\t\t}\n\t\tfn = sn;\n\t}\n#endif\n\n\terr = fib6_add_rt2node(fn, rt, info);\n\tif (!err) {\n\t\tfib6_start_gc(info->nl_net, rt);\n\t\tif (!(rt->rt6i_flags & RTF_CACHE))\n\t\t\tfib6_prune_clones(info->nl_net, pn, rt);\n\t}\n\nout:\n\tif (err) {\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t/*\n\t\t * If fib6_add_1 has cleared the old leaf pointer in the\n\t\t * super-tree leaf node we have to find a new one for it.\n\t\t */\n\t\tif (pn != fn && pn->leaf == rt) {\n\t\t\tpn->leaf = NULL;\n\t\t\tatomic_dec(&rt->rt6i_ref);\n\t\t}\n\t\tif (pn != fn && !pn->leaf && !(pn->fn_flags & RTN_RTINFO)) {\n\t\t\tpn->leaf = fib6_find_prefix(info->nl_net, pn);\n#if RT6_DEBUG >= 2\n\t\t\tif (!pn->leaf) {\n\t\t\t\tWARN_ON(pn->leaf == NULL);\n\t\t\t\tpn->leaf = info->nl_net->ipv6.ip6_null_entry;\n\t\t\t}\n#endif\n\t\t\tatomic_inc(&pn->leaf->rt6i_ref);\n\t\t}\n#endif\n\t\tdst_free(&rt->dst);\n\t}\n\treturn err;\n\n#ifdef CONFIG_IPV6_SUBTREES\n\t/* Subtree creation failed, probably main tree node\n\t   is orphan. If it is, shoot it.\n\t */\nst_failure:\n\tif (fn && !(fn->fn_flags & (RTN_RTINFO|RTN_ROOT)))\n\t\tfib6_repair_tree(info->nl_net, fn);\n\tdst_free(&rt->dst);\n\treturn err;\n#endif\n}",
        "code_after_change": "int fib6_add(struct fib6_node *root, struct rt6_info *rt, struct nl_info *info)\n{\n\tstruct fib6_node *fn, *pn = NULL;\n\tint err = -ENOMEM;\n\tint allow_create = 1;\n\tint replace_required = 0;\n\n\tif (info->nlh) {\n\t\tif (!(info->nlh->nlmsg_flags & NLM_F_CREATE))\n\t\t\tallow_create = 0;\n\t\tif (info->nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\treplace_required = 1;\n\t}\n\tif (!allow_create && !replace_required)\n\t\tpr_warn(\"RTM_NEWROUTE with no NLM_F_CREATE or NLM_F_REPLACE\\n\");\n\n\tfn = fib6_add_1(root, &rt->rt6i_dst.addr, rt->rt6i_dst.plen,\n\t\t\toffsetof(struct rt6_info, rt6i_dst), allow_create,\n\t\t\treplace_required);\n\tif (IS_ERR(fn)) {\n\t\terr = PTR_ERR(fn);\n\t\tfn = NULL;\n\t\tgoto out;\n\t}\n\n\tpn = fn;\n\n#ifdef CONFIG_IPV6_SUBTREES\n\tif (rt->rt6i_src.plen) {\n\t\tstruct fib6_node *sn;\n\n\t\tif (!fn->subtree) {\n\t\t\tstruct fib6_node *sfn;\n\n\t\t\t/*\n\t\t\t * Create subtree.\n\t\t\t *\n\t\t\t *\t\tfn[main tree]\n\t\t\t *\t\t|\n\t\t\t *\t\tsfn[subtree root]\n\t\t\t *\t\t   \\\n\t\t\t *\t\t    sn[new leaf node]\n\t\t\t */\n\n\t\t\t/* Create subtree root node */\n\t\t\tsfn = node_alloc();\n\t\t\tif (!sfn)\n\t\t\t\tgoto st_failure;\n\n\t\t\tsfn->leaf = info->nl_net->ipv6.ip6_null_entry;\n\t\t\tatomic_inc(&info->nl_net->ipv6.ip6_null_entry->rt6i_ref);\n\t\t\tsfn->fn_flags = RTN_ROOT;\n\t\t\tsfn->fn_sernum = fib6_new_sernum();\n\n\t\t\t/* Now add the first leaf node to new subtree */\n\n\t\t\tsn = fib6_add_1(sfn, &rt->rt6i_src.addr,\n\t\t\t\t\trt->rt6i_src.plen,\n\t\t\t\t\toffsetof(struct rt6_info, rt6i_src),\n\t\t\t\t\tallow_create, replace_required);\n\n\t\t\tif (IS_ERR(sn)) {\n\t\t\t\t/* If it is failed, discard just allocated\n\t\t\t\t   root, and then (in st_failure) stale node\n\t\t\t\t   in main tree.\n\t\t\t\t */\n\t\t\t\tnode_free(sfn);\n\t\t\t\terr = PTR_ERR(sn);\n\t\t\t\tgoto st_failure;\n\t\t\t}\n\n\t\t\t/* Now link new subtree to main tree */\n\t\t\tsfn->parent = fn;\n\t\t\tfn->subtree = sfn;\n\t\t} else {\n\t\t\tsn = fib6_add_1(fn->subtree, &rt->rt6i_src.addr,\n\t\t\t\t\trt->rt6i_src.plen,\n\t\t\t\t\toffsetof(struct rt6_info, rt6i_src),\n\t\t\t\t\tallow_create, replace_required);\n\n\t\t\tif (IS_ERR(sn)) {\n\t\t\t\terr = PTR_ERR(sn);\n\t\t\t\tgoto st_failure;\n\t\t\t}\n\t\t}\n\n\t\tif (!fn->leaf) {\n\t\t\tfn->leaf = rt;\n\t\t\tatomic_inc(&rt->rt6i_ref);\n\t\t}\n\t\tfn = sn;\n\t}\n#endif\n\n\terr = fib6_add_rt2node(fn, rt, info);\n\tif (!err) {\n\t\tfib6_start_gc(info->nl_net, rt);\n\t\tif (!(rt->rt6i_flags & RTF_CACHE))\n\t\t\tfib6_prune_clones(info->nl_net, pn, rt);\n\t}\n\nout:\n\tif (err) {\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t/*\n\t\t * If fib6_add_1 has cleared the old leaf pointer in the\n\t\t * super-tree leaf node we have to find a new one for it.\n\t\t */\n\t\tif (pn != fn && pn->leaf == rt) {\n\t\t\tpn->leaf = NULL;\n\t\t\tatomic_dec(&rt->rt6i_ref);\n\t\t}\n\t\tif (pn != fn && !pn->leaf && !(pn->fn_flags & RTN_RTINFO)) {\n\t\t\tpn->leaf = fib6_find_prefix(info->nl_net, pn);\n#if RT6_DEBUG >= 2\n\t\t\tif (!pn->leaf) {\n\t\t\t\tWARN_ON(pn->leaf == NULL);\n\t\t\t\tpn->leaf = info->nl_net->ipv6.ip6_null_entry;\n\t\t\t}\n#endif\n\t\t\tatomic_inc(&pn->leaf->rt6i_ref);\n\t\t}\n#endif\n\t\tdst_free(&rt->dst);\n\t}\n\treturn err;\n\n#ifdef CONFIG_IPV6_SUBTREES\n\t/* Subtree creation failed, probably main tree node\n\t   is orphan. If it is, shoot it.\n\t */\nst_failure:\n\tif (fn && !(fn->fn_flags & (RTN_RTINFO|RTN_ROOT)))\n\t\tfib6_repair_tree(info->nl_net, fn);\n\tdst_free(&rt->dst);\n\treturn err;\n#endif\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,9 +17,9 @@\n \tfn = fib6_add_1(root, &rt->rt6i_dst.addr, rt->rt6i_dst.plen,\n \t\t\toffsetof(struct rt6_info, rt6i_dst), allow_create,\n \t\t\treplace_required);\n-\n \tif (IS_ERR(fn)) {\n \t\terr = PTR_ERR(fn);\n+\t\tfn = NULL;\n \t\tgoto out;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\tfn = NULL;"
            ],
            "deleted": [
                ""
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The fib6_add function in net/ipv6/ip6_fib.c in the Linux kernel before 3.11.5 does not properly implement error-code encoding, which allows local users to cause a denial of service (NULL pointer dereference and system crash) by leveraging the CAP_NET_ADMIN capability for an IPv6 SIOCADDRT ioctl call.",
        "id": 352
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "int inet6_csk_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl_unused)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint res;\n\n\tdst = inet6_csk_route_socket(sk, &fl6);\n\tif (IS_ERR(dst)) {\n\t\tsk->sk_err_soft = -PTR_ERR(dst);\n\t\tsk->sk_route_caps = 0;\n\t\tkfree_skb(skb);\n\t\treturn PTR_ERR(dst);\n\t}\n\n\trcu_read_lock();\n\tskb_dst_set_noref(skb, dst);\n\n\t/* Restore final destination back after routing done */\n\tfl6.daddr = sk->sk_v6_daddr;\n\n\tres = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);\n\trcu_read_unlock();\n\treturn res;\n}",
        "code_after_change": "int inet6_csk_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl_unused)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint res;\n\n\tdst = inet6_csk_route_socket(sk, &fl6);\n\tif (IS_ERR(dst)) {\n\t\tsk->sk_err_soft = -PTR_ERR(dst);\n\t\tsk->sk_route_caps = 0;\n\t\tkfree_skb(skb);\n\t\treturn PTR_ERR(dst);\n\t}\n\n\trcu_read_lock();\n\tskb_dst_set_noref(skb, dst);\n\n\t/* Restore final destination back after routing done */\n\tfl6.daddr = sk->sk_v6_daddr;\n\n\tres = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),\n\t\t       np->tclass);\n\trcu_read_unlock();\n\treturn res;\n}",
        "patch": "--- code before\n+++ code after\n@@ -19,7 +19,8 @@\n \t/* Restore final destination back after routing done */\n \tfl6.daddr = sk->sk_v6_daddr;\n \n-\tres = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);\n+\tres = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),\n+\t\t       np->tclass);\n \trcu_read_unlock();\n \treturn res;\n }",
        "function_modified_lines": {
            "added": [
                "\tres = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),",
                "\t\t       np->tclass);"
            ],
            "deleted": [
                "\tres = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 999
    },
    {
        "cve_id": "CVE-2014-0181",
        "code_before_change": "static int xfrm_user_rcv_msg(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *attrs[XFRMA_MAX+1];\n\tconst struct xfrm_link *link;\n\tint type, err;\n\n\ttype = nlh->nlmsg_type;\n\tif (type > XFRM_MSG_MAX)\n\t\treturn -EINVAL;\n\n\ttype -= XFRM_MSG_BASE;\n\tlink = &xfrm_dispatch[type];\n\n\t/* All operations require privileges, even GET */\n\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((type == (XFRM_MSG_GETSA - XFRM_MSG_BASE) ||\n\t     type == (XFRM_MSG_GETPOLICY - XFRM_MSG_BASE)) &&\n\t    (nlh->nlmsg_flags & NLM_F_DUMP)) {\n\t\tif (link->dump == NULL)\n\t\t\treturn -EINVAL;\n\n\t\t{\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.dump = link->dump,\n\t\t\t\t.done = link->done,\n\t\t\t};\n\t\t\treturn netlink_dump_start(net->xfrm.nlsk, skb, nlh, &c);\n\t\t}\n\t}\n\n\terr = nlmsg_parse(nlh, xfrm_msg_min[type], attrs, XFRMA_MAX,\n\t\t\t  xfrma_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (link->doit == NULL)\n\t\treturn -EINVAL;\n\n\treturn link->doit(skb, nlh, attrs);\n}",
        "code_after_change": "static int xfrm_user_rcv_msg(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *attrs[XFRMA_MAX+1];\n\tconst struct xfrm_link *link;\n\tint type, err;\n\n\ttype = nlh->nlmsg_type;\n\tif (type > XFRM_MSG_MAX)\n\t\treturn -EINVAL;\n\n\ttype -= XFRM_MSG_BASE;\n\tlink = &xfrm_dispatch[type];\n\n\t/* All operations require privileges, even GET */\n\tif (!netlink_net_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((type == (XFRM_MSG_GETSA - XFRM_MSG_BASE) ||\n\t     type == (XFRM_MSG_GETPOLICY - XFRM_MSG_BASE)) &&\n\t    (nlh->nlmsg_flags & NLM_F_DUMP)) {\n\t\tif (link->dump == NULL)\n\t\t\treturn -EINVAL;\n\n\t\t{\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.dump = link->dump,\n\t\t\t\t.done = link->done,\n\t\t\t};\n\t\t\treturn netlink_dump_start(net->xfrm.nlsk, skb, nlh, &c);\n\t\t}\n\t}\n\n\terr = nlmsg_parse(nlh, xfrm_msg_min[type], attrs, XFRMA_MAX,\n\t\t\t  xfrma_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (link->doit == NULL)\n\t\treturn -EINVAL;\n\n\treturn link->doit(skb, nlh, attrs);\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,7 +13,7 @@\n \tlink = &xfrm_dispatch[type];\n \n \t/* All operations require privileges, even GET */\n-\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n+\tif (!netlink_net_capable(skb, CAP_NET_ADMIN))\n \t\treturn -EPERM;\n \n \tif ((type == (XFRM_MSG_GETSA - XFRM_MSG_BASE) ||",
        "function_modified_lines": {
            "added": [
                "\tif (!netlink_net_capable(skb, CAP_NET_ADMIN))"
            ],
            "deleted": [
                "\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Netlink implementation in the Linux kernel through 3.14.1 does not provide a mechanism for authorizing socket operations based on the opener of a socket, which allows local users to bypass intended access restrictions and modify network configurations by using a Netlink socket for the (1) stdout or (2) stderr of a setuid program.",
        "id": 460
    },
    {
        "cve_id": "CVE-2016-6786",
        "code_before_change": "int perf_event_task_enable(void)\n{\n\tstruct perf_event *event;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_for_each_entry(event, &current->perf_event_list, owner_entry)\n\t\tperf_event_for_each_child(event, perf_event_enable);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\treturn 0;\n}",
        "code_after_change": "int perf_event_task_enable(void)\n{\n\tstruct perf_event_context *ctx;\n\tstruct perf_event *event;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_for_each_entry(event, &current->perf_event_list, owner_entry) {\n\t\tctx = perf_event_ctx_lock(event);\n\t\tperf_event_for_each_child(event, _perf_event_enable);\n\t\tperf_event_ctx_unlock(event, ctx);\n\t}\n\tmutex_unlock(&current->perf_event_mutex);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,10 +1,14 @@\n int perf_event_task_enable(void)\n {\n+\tstruct perf_event_context *ctx;\n \tstruct perf_event *event;\n \n \tmutex_lock(&current->perf_event_mutex);\n-\tlist_for_each_entry(event, &current->perf_event_list, owner_entry)\n-\t\tperf_event_for_each_child(event, perf_event_enable);\n+\tlist_for_each_entry(event, &current->perf_event_list, owner_entry) {\n+\t\tctx = perf_event_ctx_lock(event);\n+\t\tperf_event_for_each_child(event, _perf_event_enable);\n+\t\tperf_event_ctx_unlock(event, ctx);\n+\t}\n \tmutex_unlock(&current->perf_event_mutex);\n \n \treturn 0;",
        "function_modified_lines": {
            "added": [
                "\tstruct perf_event_context *ctx;",
                "\tlist_for_each_entry(event, &current->perf_event_list, owner_entry) {",
                "\t\tctx = perf_event_ctx_lock(event);",
                "\t\tperf_event_for_each_child(event, _perf_event_enable);",
                "\t\tperf_event_ctx_unlock(event, ctx);",
                "\t}"
            ],
            "deleted": [
                "\tlist_for_each_entry(event, &current->perf_event_list, owner_entry)",
                "\t\tperf_event_for_each_child(event, perf_event_enable);"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "kernel/events/core.c in the performance subsystem in the Linux kernel before 4.0 mismanages locks during certain migrations, which allows local users to gain privileges via a crafted application, aka Android internal bug 30955111.",
        "id": 1080
    },
    {
        "cve_id": "CVE-2016-9644",
        "code_before_change": "static nokprobe_inline int\ndo_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,\n\t\t  struct pt_regs *regs,\tlong error_code)\n{\n\tif (v8086_mode(regs)) {\n\t\t/*\n\t\t * Traps 0, 1, 3, 4, and 5 should be forwarded to vm86.\n\t\t * On nmi (interrupt 2), do_trap should not be called.\n\t\t */\n\t\tif (trapnr < X86_TRAP_UD) {\n\t\t\tif (!handle_vm86_trap((struct kernel_vm86_regs *) regs,\n\t\t\t\t\t\terror_code, trapnr))\n\t\t\t\treturn 0;\n\t\t}\n\t\treturn -1;\n\t}\n\n\tif (!user_mode(regs)) {\n\t\tif (!fixup_exception(regs)) {\n\t\t\ttsk->thread.error_code = error_code;\n\t\t\ttsk->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn 0;\n\t}\n\n\treturn -1;\n}",
        "code_after_change": "static nokprobe_inline int\ndo_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,\n\t\t  struct pt_regs *regs,\tlong error_code)\n{\n\tif (v8086_mode(regs)) {\n\t\t/*\n\t\t * Traps 0, 1, 3, 4, and 5 should be forwarded to vm86.\n\t\t * On nmi (interrupt 2), do_trap should not be called.\n\t\t */\n\t\tif (trapnr < X86_TRAP_UD) {\n\t\t\tif (!handle_vm86_trap((struct kernel_vm86_regs *) regs,\n\t\t\t\t\t\terror_code, trapnr))\n\t\t\t\treturn 0;\n\t\t}\n\t\treturn -1;\n\t}\n\n\tif (!user_mode(regs)) {\n\t\tif (!fixup_exception(regs, trapnr)) {\n\t\t\ttsk->thread.error_code = error_code;\n\t\t\ttsk->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn 0;\n\t}\n\n\treturn -1;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,7 +16,7 @@\n \t}\n \n \tif (!user_mode(regs)) {\n-\t\tif (!fixup_exception(regs)) {\n+\t\tif (!fixup_exception(regs, trapnr)) {\n \t\t\ttsk->thread.error_code = error_code;\n \t\t\ttsk->thread.trap_nr = trapnr;\n \t\t\tdie(str, regs, error_code);",
        "function_modified_lines": {
            "added": [
                "\t\tif (!fixup_exception(regs, trapnr)) {"
            ],
            "deleted": [
                "\t\tif (!fixup_exception(regs)) {"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The __get_user_asm_ex macro in arch/x86/include/asm/uaccess.h in the Linux kernel 4.4.22 through 4.4.28 contains extended asm statements that are incompatible with the exception table, which allows local users to obtain root access on non-SMEP platforms via a crafted application.  NOTE: this vulnerability exists because of incorrect backporting of the CVE-2016-9178 patch to older kernels.",
        "id": 1153
    },
    {
        "cve_id": "CVE-2014-9888",
        "code_before_change": "static void *arm_coherent_dma_alloc(struct device *dev, size_t size,\n\tdma_addr_t *handle, gfp_t gfp, struct dma_attrs *attrs)\n{\n\tpgprot_t prot = __get_dma_pgprot(attrs, pgprot_kernel);\n\tvoid *memory;\n\n\tif (dma_alloc_from_coherent(dev, size, handle, &memory))\n\t\treturn memory;\n\n\treturn __dma_alloc(dev, size, handle, gfp, prot, true,\n\t\t\t   __builtin_return_address(0));\n}",
        "code_after_change": "static void *arm_coherent_dma_alloc(struct device *dev, size_t size,\n\tdma_addr_t *handle, gfp_t gfp, struct dma_attrs *attrs)\n{\n\tpgprot_t prot = __get_dma_pgprot(attrs, PAGE_KERNEL);\n\tvoid *memory;\n\n\tif (dma_alloc_from_coherent(dev, size, handle, &memory))\n\t\treturn memory;\n\n\treturn __dma_alloc(dev, size, handle, gfp, prot, true,\n\t\t\t   __builtin_return_address(0));\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,7 @@\n static void *arm_coherent_dma_alloc(struct device *dev, size_t size,\n \tdma_addr_t *handle, gfp_t gfp, struct dma_attrs *attrs)\n {\n-\tpgprot_t prot = __get_dma_pgprot(attrs, pgprot_kernel);\n+\tpgprot_t prot = __get_dma_pgprot(attrs, PAGE_KERNEL);\n \tvoid *memory;\n \n \tif (dma_alloc_from_coherent(dev, size, handle, &memory))",
        "function_modified_lines": {
            "added": [
                "\tpgprot_t prot = __get_dma_pgprot(attrs, PAGE_KERNEL);"
            ],
            "deleted": [
                "\tpgprot_t prot = __get_dma_pgprot(attrs, pgprot_kernel);"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "arch/arm/mm/dma-mapping.c in the Linux kernel before 3.13 on ARM platforms, as used in Android before 2016-08-05 on Nexus 5 and 7 (2013) devices, does not prevent executable DMA mappings, which might allow local users to gain privileges via a crafted application, aka Android internal bug 28803642 and Qualcomm internal bug CR642735.",
        "id": 706
    },
    {
        "cve_id": "CVE-2016-10318",
        "code_before_change": "int fscrypt_process_policy(struct inode *inode,\n\t\t\t\tconst struct fscrypt_policy *policy)\n{\n\tif (policy->version != 0)\n\t\treturn -EINVAL;\n\n\tif (!inode_has_encryption_context(inode)) {\n\t\tif (!inode->i_sb->s_cop->empty_dir)\n\t\t\treturn -EOPNOTSUPP;\n\t\tif (!inode->i_sb->s_cop->empty_dir(inode))\n\t\t\treturn -ENOTEMPTY;\n\t\treturn create_encryption_context_from_policy(inode, policy);\n\t}\n\n\tif (is_encryption_context_consistent_with_policy(inode, policy))\n\t\treturn 0;\n\n\tprintk(KERN_WARNING \"%s: Policy inconsistent with encryption context\\n\",\n\t       __func__);\n\treturn -EINVAL;\n}",
        "code_after_change": "int fscrypt_process_policy(struct inode *inode,\n\t\t\t\tconst struct fscrypt_policy *policy)\n{\n\tif (!inode_owner_or_capable(inode))\n\t\treturn -EACCES;\n\n\tif (policy->version != 0)\n\t\treturn -EINVAL;\n\n\tif (!inode_has_encryption_context(inode)) {\n\t\tif (!inode->i_sb->s_cop->empty_dir)\n\t\t\treturn -EOPNOTSUPP;\n\t\tif (!inode->i_sb->s_cop->empty_dir(inode))\n\t\t\treturn -ENOTEMPTY;\n\t\treturn create_encryption_context_from_policy(inode, policy);\n\t}\n\n\tif (is_encryption_context_consistent_with_policy(inode, policy))\n\t\treturn 0;\n\n\tprintk(KERN_WARNING \"%s: Policy inconsistent with encryption context\\n\",\n\t       __func__);\n\treturn -EINVAL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,6 +1,9 @@\n int fscrypt_process_policy(struct inode *inode,\n \t\t\t\tconst struct fscrypt_policy *policy)\n {\n+\tif (!inode_owner_or_capable(inode))\n+\t\treturn -EACCES;\n+\n \tif (policy->version != 0)\n \t\treturn -EINVAL;\n ",
        "function_modified_lines": {
            "added": [
                "\tif (!inode_owner_or_capable(inode))",
                "\t\treturn -EACCES;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "A missing authorization check in the fscrypt_process_policy function in fs/crypto/policy.c in the ext4 and f2fs filesystem encryption support in the Linux kernel before 4.7.4 allows a user to assign an encryption policy to a directory owned by a different user, potentially creating a denial of service.",
        "id": 903
    },
    {
        "cve_id": "CVE-2015-2686",
        "code_before_change": "\nSYSCALL_DEFINE6(sendto, int, fd, void __user *, buff, size_t, len,\n\t\tunsigned int, flags, struct sockaddr __user *, addr,\n\t\tint, addr_len)\n{\n\tstruct socket *sock;\n\tstruct sockaddr_storage address;\n\tint err;\n\tstruct msghdr msg;\n\tstruct iovec iov;\n\tint fput_needed;\n\n\tif (len > INT_MAX)\n\t\tlen = INT_MAX;\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\tiov.iov_base = buff;\n\tiov.iov_len = len;\n\tmsg.msg_name = NULL;\n\tiov_iter_init(&msg.msg_iter, WRITE, &iov, 1, len);\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\tmsg.msg_namelen = 0;\n\tif (addr) {\n\t\terr = move_addr_to_kernel(addr, addr_len, &address);\n\t\tif (err < 0)\n\t\t\tgoto out_put;\n\t\tmsg.msg_name = (struct sockaddr *)&address;\n\t\tmsg.msg_namelen = addr_len;\n\t}\n\tif (sock->file->f_flags & O_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\tmsg.msg_flags = flags;\n\terr = sock_sendmsg(sock, &msg, len);\n\nout_put:\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}",
        "code_after_change": "\nSYSCALL_DEFINE6(sendto, int, fd, void __user *, buff, size_t, len,\n\t\tunsigned int, flags, struct sockaddr __user *, addr,\n\t\tint, addr_len)\n{\n\tstruct socket *sock;\n\tstruct sockaddr_storage address;\n\tint err;\n\tstruct msghdr msg;\n\tstruct iovec iov;\n\tint fput_needed;\n\n\tif (len > INT_MAX)\n\t\tlen = INT_MAX;\n\tif (unlikely(!access_ok(VERIFY_READ, buff, len)))\n\t\treturn -EFAULT;\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\tiov.iov_base = buff;\n\tiov.iov_len = len;\n\tmsg.msg_name = NULL;\n\tiov_iter_init(&msg.msg_iter, WRITE, &iov, 1, len);\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\tmsg.msg_namelen = 0;\n\tif (addr) {\n\t\terr = move_addr_to_kernel(addr, addr_len, &address);\n\t\tif (err < 0)\n\t\t\tgoto out_put;\n\t\tmsg.msg_name = (struct sockaddr *)&address;\n\t\tmsg.msg_namelen = addr_len;\n\t}\n\tif (sock->file->f_flags & O_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\tmsg.msg_flags = flags;\n\terr = sock_sendmsg(sock, &msg, len);\n\nout_put:\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,6 +12,8 @@\n \n \tif (len > INT_MAX)\n \t\tlen = INT_MAX;\n+\tif (unlikely(!access_ok(VERIFY_READ, buff, len)))\n+\t\treturn -EFAULT;\n \tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n \tif (!sock)\n \t\tgoto out;",
        "function_modified_lines": {
            "added": [
                "\tif (unlikely(!access_ok(VERIFY_READ, buff, len)))",
                "\t\treturn -EFAULT;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "net/socket.c in the Linux kernel 3.19 before 3.19.3 does not validate certain range data for (1) sendto and (2) recvfrom system calls, which allows local users to gain privileges by leveraging a subsystem that uses the copy_from_iter function in the iov_iter interface, as demonstrated by the Bluetooth subsystem.",
        "id": 746
    },
    {
        "cve_id": "CVE-2016-4440",
        "code_before_change": "static void vmx_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tvmcs_write32(PIN_BASED_VM_EXEC_CONTROL, vmx_pin_based_exec_ctrl(vmx));\n}",
        "code_after_change": "static void vmx_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tvmcs_write32(PIN_BASED_VM_EXEC_CONTROL, vmx_pin_based_exec_ctrl(vmx));\n\tif (cpu_has_secondary_exec_ctrls()) {\n\t\tif (kvm_vcpu_apicv_active(vcpu))\n\t\t\tvmcs_set_bits(SECONDARY_VM_EXEC_CONTROL,\n\t\t\t\t      SECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\t\t\t      SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);\n\t\telse\n\t\t\tvmcs_clear_bits(SECONDARY_VM_EXEC_CONTROL,\n\t\t\t\t\tSECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\t\t\t\tSECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);\n\t}\n\n\tif (cpu_has_vmx_msr_bitmap())\n\t\tvmx_set_msr_bitmap(vcpu);\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,4 +3,17 @@\n \tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n \n \tvmcs_write32(PIN_BASED_VM_EXEC_CONTROL, vmx_pin_based_exec_ctrl(vmx));\n+\tif (cpu_has_secondary_exec_ctrls()) {\n+\t\tif (kvm_vcpu_apicv_active(vcpu))\n+\t\t\tvmcs_set_bits(SECONDARY_VM_EXEC_CONTROL,\n+\t\t\t\t      SECONDARY_EXEC_APIC_REGISTER_VIRT |\n+\t\t\t\t      SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);\n+\t\telse\n+\t\t\tvmcs_clear_bits(SECONDARY_VM_EXEC_CONTROL,\n+\t\t\t\t\tSECONDARY_EXEC_APIC_REGISTER_VIRT |\n+\t\t\t\t\tSECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);\n+\t}\n+\n+\tif (cpu_has_vmx_msr_bitmap())\n+\t\tvmx_set_msr_bitmap(vcpu);\n }",
        "function_modified_lines": {
            "added": [
                "\tif (cpu_has_secondary_exec_ctrls()) {",
                "\t\tif (kvm_vcpu_apicv_active(vcpu))",
                "\t\t\tvmcs_set_bits(SECONDARY_VM_EXEC_CONTROL,",
                "\t\t\t\t      SECONDARY_EXEC_APIC_REGISTER_VIRT |",
                "\t\t\t\t      SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);",
                "\t\telse",
                "\t\t\tvmcs_clear_bits(SECONDARY_VM_EXEC_CONTROL,",
                "\t\t\t\t\tSECONDARY_EXEC_APIC_REGISTER_VIRT |",
                "\t\t\t\t\tSECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);",
                "\t}",
                "",
                "\tif (cpu_has_vmx_msr_bitmap())",
                "\t\tvmx_set_msr_bitmap(vcpu);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "arch/x86/kvm/vmx.c in the Linux kernel through 4.6.3 mishandles the APICv on/off state, which allows guest OS users to obtain direct APIC MSR access on the host OS, and consequently cause a denial of service (host OS crash) or possibly execute arbitrary code on the host OS, via x2APIC mode.",
        "id": 1014
    },
    {
        "cve_id": "CVE-2016-9120",
        "code_before_change": "static int ion_handle_put(struct ion_handle *handle)\n{\n\tstruct ion_client *client = handle->client;\n\tint ret;\n\n\tmutex_lock(&client->lock);\n\tret = kref_put(&handle->ref, ion_handle_destroy);\n\tmutex_unlock(&client->lock);\n\n\treturn ret;\n}",
        "code_after_change": "int ion_handle_put(struct ion_handle *handle)\n{\n\tstruct ion_client *client = handle->client;\n\tint ret;\n\n\tmutex_lock(&client->lock);\n\tret = ion_handle_put_nolock(handle);\n\tmutex_unlock(&client->lock);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,10 +1,10 @@\n-static int ion_handle_put(struct ion_handle *handle)\n+int ion_handle_put(struct ion_handle *handle)\n {\n \tstruct ion_client *client = handle->client;\n \tint ret;\n \n \tmutex_lock(&client->lock);\n-\tret = kref_put(&handle->ref, ion_handle_destroy);\n+\tret = ion_handle_put_nolock(handle);\n \tmutex_unlock(&client->lock);\n \n \treturn ret;",
        "function_modified_lines": {
            "added": [
                "int ion_handle_put(struct ion_handle *handle)",
                "\tret = ion_handle_put_nolock(handle);"
            ],
            "deleted": [
                "static int ion_handle_put(struct ion_handle *handle)",
                "\tret = kref_put(&handle->ref, ion_handle_destroy);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "Race condition in the ion_ioctl function in drivers/staging/android/ion/ion.c in the Linux kernel before 4.6 allows local users to gain privileges or cause a denial of service (use-after-free) by calling ION_IOC_FREE on two CPUs at the same time.",
        "id": 1141
    },
    {
        "cve_id": "CVE-2015-2686",
        "code_before_change": "\nSYSCALL_DEFINE6(recvfrom, int, fd, void __user *, ubuf, size_t, size,\n\t\tunsigned int, flags, struct sockaddr __user *, addr,\n\t\tint __user *, addr_len)\n{\n\tstruct socket *sock;\n\tstruct iovec iov;\n\tstruct msghdr msg;\n\tstruct sockaddr_storage address;\n\tint err, err2;\n\tint fput_needed;\n\n\tif (size > INT_MAX)\n\t\tsize = INT_MAX;\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\tiov.iov_len = size;\n\tiov.iov_base = ubuf;\n\tiov_iter_init(&msg.msg_iter, READ, &iov, 1, size);\n\t/* Save some cycles and don't copy the address if not needed */\n\tmsg.msg_name = addr ? (struct sockaddr *)&address : NULL;\n\t/* We assume all kernel code knows the size of sockaddr_storage */\n\tmsg.msg_namelen = 0;\n\tif (sock->file->f_flags & O_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\terr = sock_recvmsg(sock, &msg, size, flags);\n\n\tif (err >= 0 && addr != NULL) {\n\t\terr2 = move_addr_to_user(&address,\n\t\t\t\t\t msg.msg_namelen, addr, addr_len);\n\t\tif (err2 < 0)\n\t\t\terr = err2;\n\t}\n\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}",
        "code_after_change": "\nSYSCALL_DEFINE6(recvfrom, int, fd, void __user *, ubuf, size_t, size,\n\t\tunsigned int, flags, struct sockaddr __user *, addr,\n\t\tint __user *, addr_len)\n{\n\tstruct socket *sock;\n\tstruct iovec iov;\n\tstruct msghdr msg;\n\tstruct sockaddr_storage address;\n\tint err, err2;\n\tint fput_needed;\n\n\tif (size > INT_MAX)\n\t\tsize = INT_MAX;\n\tif (unlikely(!access_ok(VERIFY_WRITE, ubuf, size)))\n\t\treturn -EFAULT;\n\tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n\tif (!sock)\n\t\tgoto out;\n\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\tiov.iov_len = size;\n\tiov.iov_base = ubuf;\n\tiov_iter_init(&msg.msg_iter, READ, &iov, 1, size);\n\t/* Save some cycles and don't copy the address if not needed */\n\tmsg.msg_name = addr ? (struct sockaddr *)&address : NULL;\n\t/* We assume all kernel code knows the size of sockaddr_storage */\n\tmsg.msg_namelen = 0;\n\tif (sock->file->f_flags & O_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\terr = sock_recvmsg(sock, &msg, size, flags);\n\n\tif (err >= 0 && addr != NULL) {\n\t\terr2 = move_addr_to_user(&address,\n\t\t\t\t\t msg.msg_namelen, addr, addr_len);\n\t\tif (err2 < 0)\n\t\t\terr = err2;\n\t}\n\n\tfput_light(sock->file, fput_needed);\nout:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,6 +12,8 @@\n \n \tif (size > INT_MAX)\n \t\tsize = INT_MAX;\n+\tif (unlikely(!access_ok(VERIFY_WRITE, ubuf, size)))\n+\t\treturn -EFAULT;\n \tsock = sockfd_lookup_light(fd, &err, &fput_needed);\n \tif (!sock)\n \t\tgoto out;",
        "function_modified_lines": {
            "added": [
                "\tif (unlikely(!access_ok(VERIFY_WRITE, ubuf, size)))",
                "\t\treturn -EFAULT;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "net/socket.c in the Linux kernel 3.19 before 3.19.3 does not validate certain range data for (1) sendto and (2) recvfrom system calls, which allows local users to gain privileges by leveraging a subsystem that uses the copy_from_iter function in the iov_iter interface, as demonstrated by the Bluetooth subsystem.",
        "id": 745
    },
    {
        "cve_id": "CVE-2013-1858",
        "code_before_change": "static struct task_struct *copy_process(unsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace)\n{\n\tint retval;\n\tstruct task_struct *p;\n\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid namespace\n\t * don't allow the creation of threads.\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_NEWPID)) &&\n\t    (task_active_pid_ns(current) != current->nsproxy->pid_ns))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tretval = security_task_create(clone_flags);\n\tif (retval)\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current);\n\tif (!p)\n\t\tgoto fork_out;\n\n\tftrace_graph_init_task(p);\n\tget_seccomp_filter(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&\n\t\t    p->real_cred->user != INIT_USER)\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tif (!try_module_get(task_thread_info(p)->exec_domain->module))\n\t\tgoto bad_fork_cleanup_count;\n\n\tp->did_exec = 0;\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tcopy_flags(clone_flags, p);\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n\tp->utimescaled = p->stimescaled = 0;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\tp->prev_cputime.utime = p->prev_cputime.stime = 0;\n#endif\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqlock_init(&p->vtime_seqlock);\n\tp->vtime_snap = 0;\n\tp->vtime_snap_whence = VTIME_SLEEPING;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tdo_posix_clock_monotonic_gettime(&p->start_time);\n\tp->real_start_time = p->start_time;\n\tmonotonic_to_bootbased(&p->real_start_time);\n\tp->io_context = NULL;\n\tp->audit_context = NULL;\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_begin(current);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_cgroup;\n\t}\n\tmpol_fix_fork_child_flag(p);\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_MEMCG\n\tp->memcg_batch.do_batch = 0;\n\tp->memcg_batch.memcg = NULL;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tsched_fork(p);\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\t/* copy all the process information */\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, stack_start, stack_size, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tif (pid != &init_struct_pid) {\n\t\tretval = -ENOMEM;\n\t\tpid = alloc_pid(p->nsproxy->pid_ns);\n\t\tif (!pid)\n\t\t\tgoto bad_fork_cleanup_io;\n\t}\n\n\tp->pid = pid_nr(pid);\n\tp->tgid = p->pid;\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->tgid = current->tgid;\n\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\tuprobe_copy_process(p);\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tp->sas_ss_sp = p->sas_ss_size = 0;\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->exit_signal = -1;\n\telse if (clone_flags & CLONE_PARENT)\n\t\tp->exit_signal = current->group_leader->exit_signal;\n\telse\n\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\n\tp->pdeath_signal = 0;\n\tp->exit_state = 0;\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\t/*\n\t * Ok, make it visible to the rest of the system.\n\t * We dont wake it up yet.\n\t */\n\tp->group_leader = p;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\t/* Need tasklist lock for parent etc handling! */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Process group and session signals need to be delivered to just the\n\t * parent before the fork or both the parent and the child after the\n\t * fork. Restart if a signal comes in before we add the new process to\n\t * it's process group.\n\t * A fatal signal pending means that current will exit, so the new\n\t * thread can't slip out of an OOM kill (or normal SIGKILL).\n\t*/\n\trecalc_sigpending();\n\tif (signal_pending(current)) {\n\t\tspin_unlock(&current->sighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tretval = -ERESTARTNOINTR;\n\t\tgoto bad_fork_free_pid;\n\t}\n\n\tif (clone_flags & CLONE_THREAD) {\n\t\tcurrent->signal->nr_threads++;\n\t\tatomic_inc(&current->signal->live);\n\t\tatomic_inc(&current->signal->sigcnt);\n\t\tp->group_leader = current->group_leader;\n\t\tlist_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);\n\t}\n\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tif (thread_group_leader(p)) {\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\n\t\t\tp->signal->leader_pid = pid;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\tattach_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tattach_pid(p, PIDTYPE_SID, task_session(current));\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID, pid);\n\t\tnr_threads++;\n\t}\n\n\ttotal_forks++;\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\n\treturn p;\n\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_policy:\n\tperf_event_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_cgroup:\n#endif\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tcgroup_exit(p, 0);\n\tdelayacct_tsk_free(p);\n\tmodule_put(task_thread_info(p)->exec_domain->module);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tfree_task(p);\nfork_out:\n\treturn ERR_PTR(retval);\n}",
        "code_after_change": "static struct task_struct *copy_process(unsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace)\n{\n\tint retval;\n\tstruct task_struct *p;\n\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid namespace\n\t * don't allow the creation of threads.\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_NEWPID)) &&\n\t    (task_active_pid_ns(current) != current->nsproxy->pid_ns))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tretval = security_task_create(clone_flags);\n\tif (retval)\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current);\n\tif (!p)\n\t\tgoto fork_out;\n\n\tftrace_graph_init_task(p);\n\tget_seccomp_filter(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&\n\t\t    p->real_cred->user != INIT_USER)\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tif (!try_module_get(task_thread_info(p)->exec_domain->module))\n\t\tgoto bad_fork_cleanup_count;\n\n\tp->did_exec = 0;\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tcopy_flags(clone_flags, p);\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n\tp->utimescaled = p->stimescaled = 0;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\tp->prev_cputime.utime = p->prev_cputime.stime = 0;\n#endif\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqlock_init(&p->vtime_seqlock);\n\tp->vtime_snap = 0;\n\tp->vtime_snap_whence = VTIME_SLEEPING;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tdo_posix_clock_monotonic_gettime(&p->start_time);\n\tp->real_start_time = p->start_time;\n\tmonotonic_to_bootbased(&p->real_start_time);\n\tp->io_context = NULL;\n\tp->audit_context = NULL;\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_begin(current);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_cgroup;\n\t}\n\tmpol_fix_fork_child_flag(p);\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_MEMCG\n\tp->memcg_batch.do_batch = 0;\n\tp->memcg_batch.memcg = NULL;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tsched_fork(p);\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\t/* copy all the process information */\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, stack_start, stack_size, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tif (pid != &init_struct_pid) {\n\t\tretval = -ENOMEM;\n\t\tpid = alloc_pid(p->nsproxy->pid_ns);\n\t\tif (!pid)\n\t\t\tgoto bad_fork_cleanup_io;\n\t}\n\n\tp->pid = pid_nr(pid);\n\tp->tgid = p->pid;\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->tgid = current->tgid;\n\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\tuprobe_copy_process(p);\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tp->sas_ss_sp = p->sas_ss_size = 0;\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->exit_signal = -1;\n\telse if (clone_flags & CLONE_PARENT)\n\t\tp->exit_signal = current->group_leader->exit_signal;\n\telse\n\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\n\tp->pdeath_signal = 0;\n\tp->exit_state = 0;\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\t/*\n\t * Ok, make it visible to the rest of the system.\n\t * We dont wake it up yet.\n\t */\n\tp->group_leader = p;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\t/* Need tasklist lock for parent etc handling! */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Process group and session signals need to be delivered to just the\n\t * parent before the fork or both the parent and the child after the\n\t * fork. Restart if a signal comes in before we add the new process to\n\t * it's process group.\n\t * A fatal signal pending means that current will exit, so the new\n\t * thread can't slip out of an OOM kill (or normal SIGKILL).\n\t*/\n\trecalc_sigpending();\n\tif (signal_pending(current)) {\n\t\tspin_unlock(&current->sighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tretval = -ERESTARTNOINTR;\n\t\tgoto bad_fork_free_pid;\n\t}\n\n\tif (clone_flags & CLONE_THREAD) {\n\t\tcurrent->signal->nr_threads++;\n\t\tatomic_inc(&current->signal->live);\n\t\tatomic_inc(&current->signal->sigcnt);\n\t\tp->group_leader = current->group_leader;\n\t\tlist_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);\n\t}\n\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tif (thread_group_leader(p)) {\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\n\t\t\tp->signal->leader_pid = pid;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\tattach_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tattach_pid(p, PIDTYPE_SID, task_session(current));\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID, pid);\n\t\tnr_threads++;\n\t}\n\n\ttotal_forks++;\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\n\treturn p;\n\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_policy:\n\tperf_event_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_cgroup:\n#endif\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tcgroup_exit(p, 0);\n\tdelayacct_tsk_free(p);\n\tmodule_put(task_thread_info(p)->exec_domain->module);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tfree_task(p);\nfork_out:\n\treturn ERR_PTR(retval);\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,6 +9,9 @@\n \tstruct task_struct *p;\n \n \tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n+\t\treturn ERR_PTR(-EINVAL);\n+\n+\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\n \t\treturn ERR_PTR(-EINVAL);\n \n \t/*",
        "function_modified_lines": {
            "added": [
                "\t\treturn ERR_PTR(-EINVAL);",
                "",
                "\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The clone system-call implementation in the Linux kernel before 3.8.3 does not properly handle a combination of the CLONE_NEWUSER and CLONE_FS flags, which allows local users to gain privileges by calling chroot and leveraging the sharing of the / directory between a parent process and a child process.",
        "id": 198
    },
    {
        "cve_id": "CVE-2016-9644",
        "code_before_change": "static noinline void\nno_context(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address, int signal, int si_code)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\tint sig;\n\n\t/* Are we prepared to handle this kernel fault? */\n\tif (fixup_exception(regs)) {\n\t\t/*\n\t\t * Any interrupt that takes a fault gets the fixup. This makes\n\t\t * the below recursive fault logic only apply to a faults from\n\t\t * task context.\n\t\t */\n\t\tif (in_interrupt())\n\t\t\treturn;\n\n\t\t/*\n\t\t * Per the above we're !in_interrupt(), aka. task context.\n\t\t *\n\t\t * In this case we need to make sure we're not recursively\n\t\t * faulting through the emulate_vsyscall() logic.\n\t\t */\n\t\tif (current_thread_info()->sig_on_uaccess_error && signal) {\n\t\t\ttsk->thread.trap_nr = X86_TRAP_PF;\n\t\t\ttsk->thread.error_code = error_code | PF_USER;\n\t\t\ttsk->thread.cr2 = address;\n\n\t\t\t/* XXX: hwpoison faults will set the wrong code. */\n\t\t\tforce_sig_info_fault(signal, si_code, address, tsk, 0);\n\t\t}\n\n\t\t/*\n\t\t * Barring that, we can do the fixup and be happy.\n\t\t */\n\t\treturn;\n\t}\n\n\t/*\n\t * 32-bit:\n\t *\n\t *   Valid to do another page fault here, because if this fault\n\t *   had been triggered by is_prefetch fixup_exception would have\n\t *   handled it.\n\t *\n\t * 64-bit:\n\t *\n\t *   Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice:\n\t */\n\tflags = oops_begin();\n\n\tshow_fault_oops(regs, error_code, address);\n\n\tif (task_stack_end_corrupted(tsk))\n\t\tprintk(KERN_EMERG \"Thread overran stack, or stack corrupted\\n\");\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_nr\t= X86_TRAP_PF;\n\ttsk->thread.error_code\t= error_code;\n\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_DEFAULT \"CR2: %016lx\\n\", address);\n\n\toops_end(flags, regs, sig);\n}",
        "code_after_change": "static noinline void\nno_context(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address, int signal, int si_code)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\tint sig;\n\n\t/* Are we prepared to handle this kernel fault? */\n\tif (fixup_exception(regs, X86_TRAP_PF)) {\n\t\t/*\n\t\t * Any interrupt that takes a fault gets the fixup. This makes\n\t\t * the below recursive fault logic only apply to a faults from\n\t\t * task context.\n\t\t */\n\t\tif (in_interrupt())\n\t\t\treturn;\n\n\t\t/*\n\t\t * Per the above we're !in_interrupt(), aka. task context.\n\t\t *\n\t\t * In this case we need to make sure we're not recursively\n\t\t * faulting through the emulate_vsyscall() logic.\n\t\t */\n\t\tif (current_thread_info()->sig_on_uaccess_error && signal) {\n\t\t\ttsk->thread.trap_nr = X86_TRAP_PF;\n\t\t\ttsk->thread.error_code = error_code | PF_USER;\n\t\t\ttsk->thread.cr2 = address;\n\n\t\t\t/* XXX: hwpoison faults will set the wrong code. */\n\t\t\tforce_sig_info_fault(signal, si_code, address, tsk, 0);\n\t\t}\n\n\t\t/*\n\t\t * Barring that, we can do the fixup and be happy.\n\t\t */\n\t\treturn;\n\t}\n\n\t/*\n\t * 32-bit:\n\t *\n\t *   Valid to do another page fault here, because if this fault\n\t *   had been triggered by is_prefetch fixup_exception would have\n\t *   handled it.\n\t *\n\t * 64-bit:\n\t *\n\t *   Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice:\n\t */\n\tflags = oops_begin();\n\n\tshow_fault_oops(regs, error_code, address);\n\n\tif (task_stack_end_corrupted(tsk))\n\t\tprintk(KERN_EMERG \"Thread overran stack, or stack corrupted\\n\");\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_nr\t= X86_TRAP_PF;\n\ttsk->thread.error_code\t= error_code;\n\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_DEFAULT \"CR2: %016lx\\n\", address);\n\n\toops_end(flags, regs, sig);\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,7 +7,7 @@\n \tint sig;\n \n \t/* Are we prepared to handle this kernel fault? */\n-\tif (fixup_exception(regs)) {\n+\tif (fixup_exception(regs, X86_TRAP_PF)) {\n \t\t/*\n \t\t * Any interrupt that takes a fault gets the fixup. This makes\n \t\t * the below recursive fault logic only apply to a faults from",
        "function_modified_lines": {
            "added": [
                "\tif (fixup_exception(regs, X86_TRAP_PF)) {"
            ],
            "deleted": [
                "\tif (fixup_exception(regs)) {"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The __get_user_asm_ex macro in arch/x86/include/asm/uaccess.h in the Linux kernel 4.4.22 through 4.4.28 contains extended asm statements that are incompatible with the exception table, which allows local users to obtain root access on non-SMEP platforms via a crafted application.  NOTE: this vulnerability exists because of incorrect backporting of the CVE-2016-9178 patch to older kernels.",
        "id": 1157
    },
    {
        "cve_id": "CVE-2016-6786",
        "code_before_change": "static void perf_event_for_each_child(struct perf_event *event,\n\t\t\t\t\tvoid (*func)(struct perf_event *))\n{\n\tstruct perf_event *child;\n\n\tWARN_ON_ONCE(event->ctx->parent_ctx);\n\tmutex_lock(&event->child_mutex);\n\tfunc(event);\n\tlist_for_each_entry(child, &event->child_list, child_list)\n\t\tfunc(child);\n\tmutex_unlock(&event->child_mutex);\n}",
        "code_after_change": "static void perf_event_for_each_child(struct perf_event *event,\n\t\t\t\t\tvoid (*func)(struct perf_event *))\n{\n\tstruct perf_event *child;\n\n\tWARN_ON_ONCE(event->ctx->parent_ctx);\n\n\tmutex_lock(&event->child_mutex);\n\tfunc(event);\n\tlist_for_each_entry(child, &event->child_list, child_list)\n\t\tfunc(child);\n\tmutex_unlock(&event->child_mutex);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,7 @@\n \tstruct perf_event *child;\n \n \tWARN_ON_ONCE(event->ctx->parent_ctx);\n+\n \tmutex_lock(&event->child_mutex);\n \tfunc(event);\n \tlist_for_each_entry(child, &event->child_list, child_list)",
        "function_modified_lines": {
            "added": [
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "kernel/events/core.c in the performance subsystem in the Linux kernel before 4.0 mismanages locks during certain migrations, which allows local users to gain privileges via a crafted application, aka Android internal bug 30955111.",
        "id": 1088
    },
    {
        "cve_id": "CVE-2012-2319",
        "code_before_change": "int hfsplus_rename_cat(u32 cnid,\n\t\t       struct inode *src_dir, struct qstr *src_name,\n\t\t       struct inode *dst_dir, struct qstr *dst_name)\n{\n\tstruct super_block *sb = src_dir->i_sb;\n\tstruct hfs_find_data src_fd, dst_fd;\n\thfsplus_cat_entry entry;\n\tint entry_size, type;\n\tint err;\n\n\tdprint(DBG_CAT_MOD, \"rename_cat: %u - %lu,%s - %lu,%s\\n\",\n\t\tcnid, src_dir->i_ino, src_name->name,\n\t\tdst_dir->i_ino, dst_name->name);\n\terr = hfs_find_init(HFSPLUS_SB(sb)->cat_tree, &src_fd);\n\tif (err)\n\t\treturn err;\n\tdst_fd = src_fd;\n\n\t/* find the old dir entry and read the data */\n\thfsplus_cat_build_key(sb, src_fd.search_key, src_dir->i_ino, src_name);\n\terr = hfs_brec_find(&src_fd);\n\tif (err)\n\t\tgoto out;\n\n\thfs_bnode_read(src_fd.bnode, &entry, src_fd.entryoffset,\n\t\t\t\tsrc_fd.entrylength);\n\n\t/* create new dir entry with the data from the old entry */\n\thfsplus_cat_build_key(sb, dst_fd.search_key, dst_dir->i_ino, dst_name);\n\terr = hfs_brec_find(&dst_fd);\n\tif (err != -ENOENT) {\n\t\tif (!err)\n\t\t\terr = -EEXIST;\n\t\tgoto out;\n\t}\n\n\terr = hfs_brec_insert(&dst_fd, &entry, src_fd.entrylength);\n\tif (err)\n\t\tgoto out;\n\tdst_dir->i_size++;\n\tdst_dir->i_mtime = dst_dir->i_ctime = CURRENT_TIME_SEC;\n\n\t/* finally remove the old entry */\n\thfsplus_cat_build_key(sb, src_fd.search_key, src_dir->i_ino, src_name);\n\terr = hfs_brec_find(&src_fd);\n\tif (err)\n\t\tgoto out;\n\terr = hfs_brec_remove(&src_fd);\n\tif (err)\n\t\tgoto out;\n\tsrc_dir->i_size--;\n\tsrc_dir->i_mtime = src_dir->i_ctime = CURRENT_TIME_SEC;\n\n\t/* remove old thread entry */\n\thfsplus_cat_build_key(sb, src_fd.search_key, cnid, NULL);\n\terr = hfs_brec_find(&src_fd);\n\tif (err)\n\t\tgoto out;\n\ttype = hfs_bnode_read_u16(src_fd.bnode, src_fd.entryoffset);\n\terr = hfs_brec_remove(&src_fd);\n\tif (err)\n\t\tgoto out;\n\n\t/* create new thread entry */\n\thfsplus_cat_build_key(sb, dst_fd.search_key, cnid, NULL);\n\tentry_size = hfsplus_fill_cat_thread(sb, &entry, type,\n\t\tdst_dir->i_ino, dst_name);\n\terr = hfs_brec_find(&dst_fd);\n\tif (err != -ENOENT) {\n\t\tif (!err)\n\t\t\terr = -EEXIST;\n\t\tgoto out;\n\t}\n\terr = hfs_brec_insert(&dst_fd, &entry, entry_size);\n\n\thfsplus_mark_inode_dirty(dst_dir, HFSPLUS_I_CAT_DIRTY);\n\thfsplus_mark_inode_dirty(src_dir, HFSPLUS_I_CAT_DIRTY);\nout:\n\thfs_bnode_put(dst_fd.bnode);\n\thfs_find_exit(&src_fd);\n\treturn err;\n}",
        "code_after_change": "int hfsplus_rename_cat(u32 cnid,\n\t\t       struct inode *src_dir, struct qstr *src_name,\n\t\t       struct inode *dst_dir, struct qstr *dst_name)\n{\n\tstruct super_block *sb = src_dir->i_sb;\n\tstruct hfs_find_data src_fd, dst_fd;\n\thfsplus_cat_entry entry;\n\tint entry_size, type;\n\tint err;\n\n\tdprint(DBG_CAT_MOD, \"rename_cat: %u - %lu,%s - %lu,%s\\n\",\n\t\tcnid, src_dir->i_ino, src_name->name,\n\t\tdst_dir->i_ino, dst_name->name);\n\terr = hfs_find_init(HFSPLUS_SB(sb)->cat_tree, &src_fd);\n\tif (err)\n\t\treturn err;\n\tdst_fd = src_fd;\n\n\t/* find the old dir entry and read the data */\n\thfsplus_cat_build_key(sb, src_fd.search_key, src_dir->i_ino, src_name);\n\terr = hfs_brec_find(&src_fd);\n\tif (err)\n\t\tgoto out;\n\tif (src_fd.entrylength > sizeof(entry) || src_fd.entrylength < 0) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\thfs_bnode_read(src_fd.bnode, &entry, src_fd.entryoffset,\n\t\t\t\tsrc_fd.entrylength);\n\n\t/* create new dir entry with the data from the old entry */\n\thfsplus_cat_build_key(sb, dst_fd.search_key, dst_dir->i_ino, dst_name);\n\terr = hfs_brec_find(&dst_fd);\n\tif (err != -ENOENT) {\n\t\tif (!err)\n\t\t\terr = -EEXIST;\n\t\tgoto out;\n\t}\n\n\terr = hfs_brec_insert(&dst_fd, &entry, src_fd.entrylength);\n\tif (err)\n\t\tgoto out;\n\tdst_dir->i_size++;\n\tdst_dir->i_mtime = dst_dir->i_ctime = CURRENT_TIME_SEC;\n\n\t/* finally remove the old entry */\n\thfsplus_cat_build_key(sb, src_fd.search_key, src_dir->i_ino, src_name);\n\terr = hfs_brec_find(&src_fd);\n\tif (err)\n\t\tgoto out;\n\terr = hfs_brec_remove(&src_fd);\n\tif (err)\n\t\tgoto out;\n\tsrc_dir->i_size--;\n\tsrc_dir->i_mtime = src_dir->i_ctime = CURRENT_TIME_SEC;\n\n\t/* remove old thread entry */\n\thfsplus_cat_build_key(sb, src_fd.search_key, cnid, NULL);\n\terr = hfs_brec_find(&src_fd);\n\tif (err)\n\t\tgoto out;\n\ttype = hfs_bnode_read_u16(src_fd.bnode, src_fd.entryoffset);\n\terr = hfs_brec_remove(&src_fd);\n\tif (err)\n\t\tgoto out;\n\n\t/* create new thread entry */\n\thfsplus_cat_build_key(sb, dst_fd.search_key, cnid, NULL);\n\tentry_size = hfsplus_fill_cat_thread(sb, &entry, type,\n\t\tdst_dir->i_ino, dst_name);\n\terr = hfs_brec_find(&dst_fd);\n\tif (err != -ENOENT) {\n\t\tif (!err)\n\t\t\terr = -EEXIST;\n\t\tgoto out;\n\t}\n\terr = hfs_brec_insert(&dst_fd, &entry, entry_size);\n\n\thfsplus_mark_inode_dirty(dst_dir, HFSPLUS_I_CAT_DIRTY);\n\thfsplus_mark_inode_dirty(src_dir, HFSPLUS_I_CAT_DIRTY);\nout:\n\thfs_bnode_put(dst_fd.bnode);\n\thfs_find_exit(&src_fd);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,6 +21,10 @@\n \terr = hfs_brec_find(&src_fd);\n \tif (err)\n \t\tgoto out;\n+\tif (src_fd.entrylength > sizeof(entry) || src_fd.entrylength < 0) {\n+\t\terr = -EIO;\n+\t\tgoto out;\n+\t}\n \n \thfs_bnode_read(src_fd.bnode, &entry, src_fd.entryoffset,\n \t\t\t\tsrc_fd.entrylength);",
        "function_modified_lines": {
            "added": [
                "\tif (src_fd.entrylength > sizeof(entry) || src_fd.entrylength < 0) {",
                "\t\terr = -EIO;",
                "\t\tgoto out;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "Multiple buffer overflows in the hfsplus filesystem implementation in the Linux kernel before 3.3.5 allow local users to gain privileges via a crafted HFS plus filesystem, a related issue to CVE-2009-4020.",
        "id": 44
    },
    {
        "cve_id": "CVE-2012-2313",
        "code_before_change": "static int\nrio_ioctl (struct net_device *dev, struct ifreq *rq, int cmd)\n{\n\tint phy_addr;\n\tstruct netdev_private *np = netdev_priv(dev);\n\tstruct mii_data *miidata = (struct mii_data *) &rq->ifr_ifru;\n\n\tstruct netdev_desc *desc;\n\tint i;\n\n\tphy_addr = np->phy_addr;\n\tswitch (cmd) {\n\tcase SIOCDEVPRIVATE:\n\t\tbreak;\n\n\tcase SIOCDEVPRIVATE + 1:\n\t\tmiidata->out_value = mii_read (dev, phy_addr, miidata->reg_num);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 2:\n\t\tmii_write (dev, phy_addr, miidata->reg_num, miidata->in_value);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 3:\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 4:\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 5:\n\t\tnetif_stop_queue (dev);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 6:\n\t\tnetif_wake_queue (dev);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 7:\n\t\tprintk\n\t\t    (\"tx_full=%x cur_tx=%lx old_tx=%lx cur_rx=%lx old_rx=%lx\\n\",\n\t\t     netif_queue_stopped(dev), np->cur_tx, np->old_tx, np->cur_rx,\n\t\t     np->old_rx);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 8:\n\t\tprintk(\"TX ring:\\n\");\n\t\tfor (i = 0; i < TX_RING_SIZE; i++) {\n\t\t\tdesc = &np->tx_ring[i];\n\t\t\tprintk\n\t\t\t    (\"%02x:cur:%08x next:%08x status:%08x frag1:%08x frag0:%08x\",\n\t\t\t     i,\n\t\t\t     (u32) (np->tx_ring_dma + i * sizeof (*desc)),\n\t\t\t     (u32)le64_to_cpu(desc->next_desc),\n\t\t\t     (u32)le64_to_cpu(desc->status),\n\t\t\t     (u32)(le64_to_cpu(desc->fraginfo) >> 32),\n\t\t\t     (u32)le64_to_cpu(desc->fraginfo));\n\t\t\tprintk (\"\\n\");\n\t\t}\n\t\tprintk (\"\\n\");\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int\nrio_ioctl (struct net_device *dev, struct ifreq *rq, int cmd)\n{\n\tint phy_addr;\n\tstruct netdev_private *np = netdev_priv(dev);\n\tstruct mii_ioctl_data *miidata = if_mii(rq);\n\n\tphy_addr = np->phy_addr;\n\tswitch (cmd) {\n\tcase SIOCGMIIPHY:\n\t\tmiidata->phy_id = phy_addr;\n\t\tbreak;\n\tcase SIOCGMIIREG:\n\t\tmiidata->val_out = mii_read (dev, phy_addr, miidata->reg_num);\n\t\tbreak;\n\tcase SIOCSMIIREG:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\tmii_write (dev, phy_addr, miidata->reg_num, miidata->val_in);\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,55 +3,21 @@\n {\n \tint phy_addr;\n \tstruct netdev_private *np = netdev_priv(dev);\n-\tstruct mii_data *miidata = (struct mii_data *) &rq->ifr_ifru;\n-\n-\tstruct netdev_desc *desc;\n-\tint i;\n+\tstruct mii_ioctl_data *miidata = if_mii(rq);\n \n \tphy_addr = np->phy_addr;\n \tswitch (cmd) {\n-\tcase SIOCDEVPRIVATE:\n+\tcase SIOCGMIIPHY:\n+\t\tmiidata->phy_id = phy_addr;\n \t\tbreak;\n-\n-\tcase SIOCDEVPRIVATE + 1:\n-\t\tmiidata->out_value = mii_read (dev, phy_addr, miidata->reg_num);\n+\tcase SIOCGMIIREG:\n+\t\tmiidata->val_out = mii_read (dev, phy_addr, miidata->reg_num);\n \t\tbreak;\n-\tcase SIOCDEVPRIVATE + 2:\n-\t\tmii_write (dev, phy_addr, miidata->reg_num, miidata->in_value);\n+\tcase SIOCSMIIREG:\n+\t\tif (!capable(CAP_NET_ADMIN))\n+\t\t\treturn -EPERM;\n+\t\tmii_write (dev, phy_addr, miidata->reg_num, miidata->val_in);\n \t\tbreak;\n-\tcase SIOCDEVPRIVATE + 3:\n-\t\tbreak;\n-\tcase SIOCDEVPRIVATE + 4:\n-\t\tbreak;\n-\tcase SIOCDEVPRIVATE + 5:\n-\t\tnetif_stop_queue (dev);\n-\t\tbreak;\n-\tcase SIOCDEVPRIVATE + 6:\n-\t\tnetif_wake_queue (dev);\n-\t\tbreak;\n-\tcase SIOCDEVPRIVATE + 7:\n-\t\tprintk\n-\t\t    (\"tx_full=%x cur_tx=%lx old_tx=%lx cur_rx=%lx old_rx=%lx\\n\",\n-\t\t     netif_queue_stopped(dev), np->cur_tx, np->old_tx, np->cur_rx,\n-\t\t     np->old_rx);\n-\t\tbreak;\n-\tcase SIOCDEVPRIVATE + 8:\n-\t\tprintk(\"TX ring:\\n\");\n-\t\tfor (i = 0; i < TX_RING_SIZE; i++) {\n-\t\t\tdesc = &np->tx_ring[i];\n-\t\t\tprintk\n-\t\t\t    (\"%02x:cur:%08x next:%08x status:%08x frag1:%08x frag0:%08x\",\n-\t\t\t     i,\n-\t\t\t     (u32) (np->tx_ring_dma + i * sizeof (*desc)),\n-\t\t\t     (u32)le64_to_cpu(desc->next_desc),\n-\t\t\t     (u32)le64_to_cpu(desc->status),\n-\t\t\t     (u32)(le64_to_cpu(desc->fraginfo) >> 32),\n-\t\t\t     (u32)le64_to_cpu(desc->fraginfo));\n-\t\t\tprintk (\"\\n\");\n-\t\t}\n-\t\tprintk (\"\\n\");\n-\t\tbreak;\n-\n \tdefault:\n \t\treturn -EOPNOTSUPP;\n \t}",
        "function_modified_lines": {
            "added": [
                "\tstruct mii_ioctl_data *miidata = if_mii(rq);",
                "\tcase SIOCGMIIPHY:",
                "\t\tmiidata->phy_id = phy_addr;",
                "\tcase SIOCGMIIREG:",
                "\t\tmiidata->val_out = mii_read (dev, phy_addr, miidata->reg_num);",
                "\tcase SIOCSMIIREG:",
                "\t\tif (!capable(CAP_NET_ADMIN))",
                "\t\t\treturn -EPERM;",
                "\t\tmii_write (dev, phy_addr, miidata->reg_num, miidata->val_in);"
            ],
            "deleted": [
                "\tstruct mii_data *miidata = (struct mii_data *) &rq->ifr_ifru;",
                "",
                "\tstruct netdev_desc *desc;",
                "\tint i;",
                "\tcase SIOCDEVPRIVATE:",
                "",
                "\tcase SIOCDEVPRIVATE + 1:",
                "\t\tmiidata->out_value = mii_read (dev, phy_addr, miidata->reg_num);",
                "\tcase SIOCDEVPRIVATE + 2:",
                "\t\tmii_write (dev, phy_addr, miidata->reg_num, miidata->in_value);",
                "\tcase SIOCDEVPRIVATE + 3:",
                "\t\tbreak;",
                "\tcase SIOCDEVPRIVATE + 4:",
                "\t\tbreak;",
                "\tcase SIOCDEVPRIVATE + 5:",
                "\t\tnetif_stop_queue (dev);",
                "\t\tbreak;",
                "\tcase SIOCDEVPRIVATE + 6:",
                "\t\tnetif_wake_queue (dev);",
                "\t\tbreak;",
                "\tcase SIOCDEVPRIVATE + 7:",
                "\t\tprintk",
                "\t\t    (\"tx_full=%x cur_tx=%lx old_tx=%lx cur_rx=%lx old_rx=%lx\\n\",",
                "\t\t     netif_queue_stopped(dev), np->cur_tx, np->old_tx, np->cur_rx,",
                "\t\t     np->old_rx);",
                "\t\tbreak;",
                "\tcase SIOCDEVPRIVATE + 8:",
                "\t\tprintk(\"TX ring:\\n\");",
                "\t\tfor (i = 0; i < TX_RING_SIZE; i++) {",
                "\t\t\tdesc = &np->tx_ring[i];",
                "\t\t\tprintk",
                "\t\t\t    (\"%02x:cur:%08x next:%08x status:%08x frag1:%08x frag0:%08x\",",
                "\t\t\t     i,",
                "\t\t\t     (u32) (np->tx_ring_dma + i * sizeof (*desc)),",
                "\t\t\t     (u32)le64_to_cpu(desc->next_desc),",
                "\t\t\t     (u32)le64_to_cpu(desc->status),",
                "\t\t\t     (u32)(le64_to_cpu(desc->fraginfo) >> 32),",
                "\t\t\t     (u32)le64_to_cpu(desc->fraginfo));",
                "\t\t\tprintk (\"\\n\");",
                "\t\t}",
                "\t\tprintk (\"\\n\");",
                "\t\tbreak;",
                ""
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The rio_ioctl function in drivers/net/ethernet/dlink/dl2k.c in the Linux kernel before 3.3.7 does not restrict access to the SIOCSMIIREG command, which allows local users to write data to an Ethernet adapter via an ioctl call.",
        "id": 43
    },
    {
        "cve_id": "CVE-2013-4300",
        "code_before_change": "static __inline__ int scm_check_creds(struct ucred *creds)\n{\n\tconst struct cred *cred = current_cred();\n\tkuid_t uid = make_kuid(cred->user_ns, creds->uid);\n\tkgid_t gid = make_kgid(cred->user_ns, creds->gid);\n\n\tif (!uid_valid(uid) || !gid_valid(gid))\n\t\treturn -EINVAL;\n\n\tif ((creds->pid == task_tgid_vnr(current) ||\n\t     ns_capable(current->nsproxy->pid_ns->user_ns, CAP_SYS_ADMIN)) &&\n\t    ((uid_eq(uid, cred->uid)   || uid_eq(uid, cred->euid) ||\n\t      uid_eq(uid, cred->suid)) || nsown_capable(CAP_SETUID)) &&\n\t    ((gid_eq(gid, cred->gid)   || gid_eq(gid, cred->egid) ||\n\t      gid_eq(gid, cred->sgid)) || nsown_capable(CAP_SETGID))) {\n\t       return 0;\n\t}\n\treturn -EPERM;\n}",
        "code_after_change": "static __inline__ int scm_check_creds(struct ucred *creds)\n{\n\tconst struct cred *cred = current_cred();\n\tkuid_t uid = make_kuid(cred->user_ns, creds->uid);\n\tkgid_t gid = make_kgid(cred->user_ns, creds->gid);\n\n\tif (!uid_valid(uid) || !gid_valid(gid))\n\t\treturn -EINVAL;\n\n\tif ((creds->pid == task_tgid_vnr(current) ||\n\t     ns_capable(task_active_pid_ns(current)->user_ns, CAP_SYS_ADMIN)) &&\n\t    ((uid_eq(uid, cred->uid)   || uid_eq(uid, cred->euid) ||\n\t      uid_eq(uid, cred->suid)) || nsown_capable(CAP_SETUID)) &&\n\t    ((gid_eq(gid, cred->gid)   || gid_eq(gid, cred->egid) ||\n\t      gid_eq(gid, cred->sgid)) || nsown_capable(CAP_SETGID))) {\n\t       return 0;\n\t}\n\treturn -EPERM;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,7 +8,7 @@\n \t\treturn -EINVAL;\n \n \tif ((creds->pid == task_tgid_vnr(current) ||\n-\t     ns_capable(current->nsproxy->pid_ns->user_ns, CAP_SYS_ADMIN)) &&\n+\t     ns_capable(task_active_pid_ns(current)->user_ns, CAP_SYS_ADMIN)) &&\n \t    ((uid_eq(uid, cred->uid)   || uid_eq(uid, cred->euid) ||\n \t      uid_eq(uid, cred->suid)) || nsown_capable(CAP_SETUID)) &&\n \t    ((gid_eq(gid, cred->gid)   || gid_eq(gid, cred->egid) ||",
        "function_modified_lines": {
            "added": [
                "\t     ns_capable(task_active_pid_ns(current)->user_ns, CAP_SYS_ADMIN)) &&"
            ],
            "deleted": [
                "\t     ns_capable(current->nsproxy->pid_ns->user_ns, CAP_SYS_ADMIN)) &&"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The scm_check_creds function in net/core/scm.c in the Linux kernel before 3.11 performs a capability check in an incorrect namespace, which allows local users to gain privileges via PID spoofing.",
        "id": 294
    },
    {
        "cve_id": "CVE-2014-8133",
        "code_before_change": "int do_set_thread_area(struct task_struct *p, int idx,\n\t\t       struct user_desc __user *u_info,\n\t\t       int can_allocate)\n{\n\tstruct user_desc info;\n\n\tif (copy_from_user(&info, u_info, sizeof(info)))\n\t\treturn -EFAULT;\n\n\tif (idx == -1)\n\t\tidx = info.entry_number;\n\n\t/*\n\t * index -1 means the kernel should try to find and\n\t * allocate an empty descriptor:\n\t */\n\tif (idx == -1 && can_allocate) {\n\t\tidx = get_free_idx();\n\t\tif (idx < 0)\n\t\t\treturn idx;\n\t\tif (put_user(idx, &u_info->entry_number))\n\t\t\treturn -EFAULT;\n\t}\n\n\tif (idx < GDT_ENTRY_TLS_MIN || idx > GDT_ENTRY_TLS_MAX)\n\t\treturn -EINVAL;\n\n\tset_tls_desc(p, idx, &info, 1);\n\n\treturn 0;\n}",
        "code_after_change": "int do_set_thread_area(struct task_struct *p, int idx,\n\t\t       struct user_desc __user *u_info,\n\t\t       int can_allocate)\n{\n\tstruct user_desc info;\n\n\tif (copy_from_user(&info, u_info, sizeof(info)))\n\t\treturn -EFAULT;\n\n\tif (!tls_desc_okay(&info))\n\t\treturn -EINVAL;\n\n\tif (idx == -1)\n\t\tidx = info.entry_number;\n\n\t/*\n\t * index -1 means the kernel should try to find and\n\t * allocate an empty descriptor:\n\t */\n\tif (idx == -1 && can_allocate) {\n\t\tidx = get_free_idx();\n\t\tif (idx < 0)\n\t\t\treturn idx;\n\t\tif (put_user(idx, &u_info->entry_number))\n\t\t\treturn -EFAULT;\n\t}\n\n\tif (idx < GDT_ENTRY_TLS_MIN || idx > GDT_ENTRY_TLS_MAX)\n\t\treturn -EINVAL;\n\n\tset_tls_desc(p, idx, &info, 1);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,6 +6,9 @@\n \n \tif (copy_from_user(&info, u_info, sizeof(info)))\n \t\treturn -EFAULT;\n+\n+\tif (!tls_desc_okay(&info))\n+\t\treturn -EINVAL;\n \n \tif (idx == -1)\n \t\tidx = info.entry_number;",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (!tls_desc_okay(&info))",
                "\t\treturn -EINVAL;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "arch/x86/kernel/tls.c in the Thread Local Storage (TLS) implementation in the Linux kernel through 3.18.1 allows local users to bypass the espfix protection mechanism, and consequently makes it easier for local users to bypass the ASLR protection mechanism, via a crafted application that makes a set_thread_area system call and later reads a 16-bit value.",
        "id": 602
    },
    {
        "cve_id": "CVE-2014-4014",
        "code_before_change": "STATIC int\nxfs_ioctl_setattr(\n\txfs_inode_t\t\t*ip,\n\tstruct fsxattr\t\t*fa,\n\tint\t\t\tmask)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_trans\t*tp;\n\tunsigned int\t\tlock_flags = 0;\n\tstruct xfs_dquot\t*udqp = NULL;\n\tstruct xfs_dquot\t*pdqp = NULL;\n\tstruct xfs_dquot\t*olddquot = NULL;\n\tint\t\t\tcode;\n\n\ttrace_xfs_ioctl_setattr(ip);\n\n\tif (mp->m_flags & XFS_MOUNT_RDONLY)\n\t\treturn XFS_ERROR(EROFS);\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn XFS_ERROR(EIO);\n\n\t/*\n\t * Disallow 32bit project ids when projid32bit feature is not enabled.\n\t */\n\tif ((mask & FSX_PROJID) && (fa->fsx_projid > (__uint16_t)-1) &&\n\t\t\t!xfs_sb_version_hasprojid32bit(&ip->i_mount->m_sb))\n\t\treturn XFS_ERROR(EINVAL);\n\n\t/*\n\t * If disk quotas is on, we make sure that the dquots do exist on disk,\n\t * before we start any other transactions. Trying to do this later\n\t * is messy. We don't care to take a readlock to look at the ids\n\t * in inode here, because we can't hold it across the trans_reserve.\n\t * If the IDs do change before we take the ilock, we're covered\n\t * because the i_*dquot fields will get updated anyway.\n\t */\n\tif (XFS_IS_QUOTA_ON(mp) && (mask & FSX_PROJID)) {\n\t\tcode = xfs_qm_vop_dqalloc(ip, ip->i_d.di_uid,\n\t\t\t\t\t ip->i_d.di_gid, fa->fsx_projid,\n\t\t\t\t\t XFS_QMOPT_PQUOTA, &udqp, NULL, &pdqp);\n\t\tif (code)\n\t\t\treturn code;\n\t}\n\n\t/*\n\t * For the other attributes, we acquire the inode lock and\n\t * first do an error checking pass.\n\t */\n\ttp = xfs_trans_alloc(mp, XFS_TRANS_SETATTR_NOT_SIZE);\n\tcode = xfs_trans_reserve(tp, &M_RES(mp)->tr_ichange, 0, 0);\n\tif (code)\n\t\tgoto error_return;\n\n\tlock_flags = XFS_ILOCK_EXCL;\n\txfs_ilock(ip, lock_flags);\n\n\t/*\n\t * CAP_FOWNER overrides the following restrictions:\n\t *\n\t * The user ID of the calling process must be equal\n\t * to the file owner ID, except in cases where the\n\t * CAP_FSETID capability is applicable.\n\t */\n\tif (!inode_owner_or_capable(VFS_I(ip))) {\n\t\tcode = XFS_ERROR(EPERM);\n\t\tgoto error_return;\n\t}\n\n\t/*\n\t * Do a quota reservation only if projid is actually going to change.\n\t * Only allow changing of projid from init_user_ns since it is a\n\t * non user namespace aware identifier.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\tif (current_user_ns() != &init_user_ns) {\n\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\tgoto error_return;\n\t\t}\n\n\t\tif (XFS_IS_QUOTA_RUNNING(mp) &&\n\t\t    XFS_IS_PQUOTA_ON(mp) &&\n\t\t    xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tASSERT(tp);\n\t\t\tcode = xfs_qm_vop_chown_reserve(tp, ip, udqp, NULL,\n\t\t\t\t\t\tpdqp, capable(CAP_FOWNER) ?\n\t\t\t\t\t\tXFS_QMOPT_FORCE_RES : 0);\n\t\t\tif (code)\t/* out of quota */\n\t\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\tif (mask & FSX_EXTSIZE) {\n\t\t/*\n\t\t * Can't change extent size if any extents are allocated.\n\t\t */\n\t\tif (ip->i_d.di_nextents &&\n\t\t    ((ip->i_d.di_extsize << mp->m_sb.sb_blocklog) !=\n\t\t     fa->fsx_extsize)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * Extent size must be a multiple of the appropriate block\n\t\t * size, if set at all. It must also be smaller than the\n\t\t * maximum extent size supported by the filesystem.\n\t\t *\n\t\t * Also, for non-realtime files, limit the extent size hint to\n\t\t * half the size of the AGs in the filesystem so alignment\n\t\t * doesn't result in extents larger than an AG.\n\t\t */\n\t\tif (fa->fsx_extsize != 0) {\n\t\t\txfs_extlen_t    size;\n\t\t\txfs_fsblock_t   extsize_fsb;\n\n\t\t\textsize_fsb = XFS_B_TO_FSB(mp, fa->fsx_extsize);\n\t\t\tif (extsize_fsb > MAXEXTLEN) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\n\t\t\tif (XFS_IS_REALTIME_INODE(ip) ||\n\t\t\t    ((mask & FSX_XFLAGS) &&\n\t\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME))) {\n\t\t\t\tsize = mp->m_sb.sb_rextsize <<\n\t\t\t\t       mp->m_sb.sb_blocklog;\n\t\t\t} else {\n\t\t\t\tsize = mp->m_sb.sb_blocksize;\n\t\t\t\tif (extsize_fsb > mp->m_sb.sb_agblocks / 2) {\n\t\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\t\tgoto error_return;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (fa->fsx_extsize % size) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\t}\n\n\n\tif (mask & FSX_XFLAGS) {\n\t\t/*\n\t\t * Can't change realtime flag if any extents are allocated.\n\t\t */\n\t\tif ((ip->i_d.di_nextents || ip->i_delayed_blks) &&\n\t\t    (XFS_IS_REALTIME_INODE(ip)) !=\n\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * If realtime flag is set then must have realtime data.\n\t\t */\n\t\tif ((fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tif ((mp->m_sb.sb_rblocks == 0) ||\n\t\t\t    (mp->m_sb.sb_rextsize == 0) ||\n\t\t\t    (ip->i_d.di_extsize % mp->m_sb.sb_rextsize)) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Can't modify an immutable/append-only file unless\n\t\t * we have appropriate permission.\n\t\t */\n\t\tif ((ip->i_d.di_flags &\n\t\t\t\t(XFS_DIFLAG_IMMUTABLE|XFS_DIFLAG_APPEND) ||\n\t\t     (fa->fsx_xflags &\n\t\t\t\t(XFS_XFLAG_IMMUTABLE | XFS_XFLAG_APPEND))) &&\n\t\t    !capable(CAP_LINUX_IMMUTABLE)) {\n\t\t\tcode = XFS_ERROR(EPERM);\n\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\txfs_trans_ijoin(tp, ip, 0);\n\n\t/*\n\t * Change file ownership.  Must be the owner or privileged.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\t/*\n\t\t * CAP_FSETID overrides the following restrictions:\n\t\t *\n\t\t * The set-user-ID and set-group-ID bits of a file will be\n\t\t * cleared upon successful return from chown()\n\t\t */\n\t\tif ((ip->i_d.di_mode & (S_ISUID|S_ISGID)) &&\n\t\t    !inode_capable(VFS_I(ip), CAP_FSETID))\n\t\t\tip->i_d.di_mode &= ~(S_ISUID|S_ISGID);\n\n\t\t/*\n\t\t * Change the ownerships and register quota modifications\n\t\t * in the transaction.\n\t\t */\n\t\tif (xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tif (XFS_IS_QUOTA_RUNNING(mp) && XFS_IS_PQUOTA_ON(mp)) {\n\t\t\t\tolddquot = xfs_qm_vop_chown(tp, ip,\n\t\t\t\t\t\t\t&ip->i_pdquot, pdqp);\n\t\t\t}\n\t\t\txfs_set_projid(ip, fa->fsx_projid);\n\n\t\t\t/*\n\t\t\t * We may have to rev the inode as well as\n\t\t\t * the superblock version number since projids didn't\n\t\t\t * exist before DINODE_VERSION_2 and SB_VERSION_NLINK.\n\t\t\t */\n\t\t\tif (ip->i_d.di_version == 1)\n\t\t\t\txfs_bump_ino_vers2(tp, ip);\n\t\t}\n\n\t}\n\n\tif (mask & FSX_EXTSIZE)\n\t\tip->i_d.di_extsize = fa->fsx_extsize >> mp->m_sb.sb_blocklog;\n\tif (mask & FSX_XFLAGS) {\n\t\txfs_set_diflags(ip, fa->fsx_xflags);\n\t\txfs_diflags_to_linux(ip);\n\t}\n\n\txfs_trans_ichgtime(tp, ip, XFS_ICHGTIME_CHG);\n\txfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);\n\n\tXFS_STATS_INC(xs_ig_attrchg);\n\n\t/*\n\t * If this is a synchronous mount, make sure that the\n\t * transaction goes to disk before returning to the user.\n\t * This is slightly sub-optimal in that truncates require\n\t * two sync transactions instead of one for wsync filesystems.\n\t * One for the truncate and one for the timestamps since we\n\t * don't want to change the timestamps unless we're sure the\n\t * truncate worked.  Truncates are less than 1% of the laddis\n\t * mix so this probably isn't worth the trouble to optimize.\n\t */\n\tif (mp->m_flags & XFS_MOUNT_WSYNC)\n\t\txfs_trans_set_sync(tp);\n\tcode = xfs_trans_commit(tp, 0);\n\txfs_iunlock(ip, lock_flags);\n\n\t/*\n\t * Release any dquot(s) the inode had kept before chown.\n\t */\n\txfs_qm_dqrele(olddquot);\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\n\treturn code;\n\n error_return:\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\txfs_trans_cancel(tp, 0);\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\n\treturn code;\n}",
        "code_after_change": "STATIC int\nxfs_ioctl_setattr(\n\txfs_inode_t\t\t*ip,\n\tstruct fsxattr\t\t*fa,\n\tint\t\t\tmask)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_trans\t*tp;\n\tunsigned int\t\tlock_flags = 0;\n\tstruct xfs_dquot\t*udqp = NULL;\n\tstruct xfs_dquot\t*pdqp = NULL;\n\tstruct xfs_dquot\t*olddquot = NULL;\n\tint\t\t\tcode;\n\n\ttrace_xfs_ioctl_setattr(ip);\n\n\tif (mp->m_flags & XFS_MOUNT_RDONLY)\n\t\treturn XFS_ERROR(EROFS);\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn XFS_ERROR(EIO);\n\n\t/*\n\t * Disallow 32bit project ids when projid32bit feature is not enabled.\n\t */\n\tif ((mask & FSX_PROJID) && (fa->fsx_projid > (__uint16_t)-1) &&\n\t\t\t!xfs_sb_version_hasprojid32bit(&ip->i_mount->m_sb))\n\t\treturn XFS_ERROR(EINVAL);\n\n\t/*\n\t * If disk quotas is on, we make sure that the dquots do exist on disk,\n\t * before we start any other transactions. Trying to do this later\n\t * is messy. We don't care to take a readlock to look at the ids\n\t * in inode here, because we can't hold it across the trans_reserve.\n\t * If the IDs do change before we take the ilock, we're covered\n\t * because the i_*dquot fields will get updated anyway.\n\t */\n\tif (XFS_IS_QUOTA_ON(mp) && (mask & FSX_PROJID)) {\n\t\tcode = xfs_qm_vop_dqalloc(ip, ip->i_d.di_uid,\n\t\t\t\t\t ip->i_d.di_gid, fa->fsx_projid,\n\t\t\t\t\t XFS_QMOPT_PQUOTA, &udqp, NULL, &pdqp);\n\t\tif (code)\n\t\t\treturn code;\n\t}\n\n\t/*\n\t * For the other attributes, we acquire the inode lock and\n\t * first do an error checking pass.\n\t */\n\ttp = xfs_trans_alloc(mp, XFS_TRANS_SETATTR_NOT_SIZE);\n\tcode = xfs_trans_reserve(tp, &M_RES(mp)->tr_ichange, 0, 0);\n\tif (code)\n\t\tgoto error_return;\n\n\tlock_flags = XFS_ILOCK_EXCL;\n\txfs_ilock(ip, lock_flags);\n\n\t/*\n\t * CAP_FOWNER overrides the following restrictions:\n\t *\n\t * The user ID of the calling process must be equal\n\t * to the file owner ID, except in cases where the\n\t * CAP_FSETID capability is applicable.\n\t */\n\tif (!inode_owner_or_capable(VFS_I(ip))) {\n\t\tcode = XFS_ERROR(EPERM);\n\t\tgoto error_return;\n\t}\n\n\t/*\n\t * Do a quota reservation only if projid is actually going to change.\n\t * Only allow changing of projid from init_user_ns since it is a\n\t * non user namespace aware identifier.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\tif (current_user_ns() != &init_user_ns) {\n\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\tgoto error_return;\n\t\t}\n\n\t\tif (XFS_IS_QUOTA_RUNNING(mp) &&\n\t\t    XFS_IS_PQUOTA_ON(mp) &&\n\t\t    xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tASSERT(tp);\n\t\t\tcode = xfs_qm_vop_chown_reserve(tp, ip, udqp, NULL,\n\t\t\t\t\t\tpdqp, capable(CAP_FOWNER) ?\n\t\t\t\t\t\tXFS_QMOPT_FORCE_RES : 0);\n\t\t\tif (code)\t/* out of quota */\n\t\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\tif (mask & FSX_EXTSIZE) {\n\t\t/*\n\t\t * Can't change extent size if any extents are allocated.\n\t\t */\n\t\tif (ip->i_d.di_nextents &&\n\t\t    ((ip->i_d.di_extsize << mp->m_sb.sb_blocklog) !=\n\t\t     fa->fsx_extsize)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * Extent size must be a multiple of the appropriate block\n\t\t * size, if set at all. It must also be smaller than the\n\t\t * maximum extent size supported by the filesystem.\n\t\t *\n\t\t * Also, for non-realtime files, limit the extent size hint to\n\t\t * half the size of the AGs in the filesystem so alignment\n\t\t * doesn't result in extents larger than an AG.\n\t\t */\n\t\tif (fa->fsx_extsize != 0) {\n\t\t\txfs_extlen_t    size;\n\t\t\txfs_fsblock_t   extsize_fsb;\n\n\t\t\textsize_fsb = XFS_B_TO_FSB(mp, fa->fsx_extsize);\n\t\t\tif (extsize_fsb > MAXEXTLEN) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\n\t\t\tif (XFS_IS_REALTIME_INODE(ip) ||\n\t\t\t    ((mask & FSX_XFLAGS) &&\n\t\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME))) {\n\t\t\t\tsize = mp->m_sb.sb_rextsize <<\n\t\t\t\t       mp->m_sb.sb_blocklog;\n\t\t\t} else {\n\t\t\t\tsize = mp->m_sb.sb_blocksize;\n\t\t\t\tif (extsize_fsb > mp->m_sb.sb_agblocks / 2) {\n\t\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\t\tgoto error_return;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (fa->fsx_extsize % size) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\t}\n\n\n\tif (mask & FSX_XFLAGS) {\n\t\t/*\n\t\t * Can't change realtime flag if any extents are allocated.\n\t\t */\n\t\tif ((ip->i_d.di_nextents || ip->i_delayed_blks) &&\n\t\t    (XFS_IS_REALTIME_INODE(ip)) !=\n\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * If realtime flag is set then must have realtime data.\n\t\t */\n\t\tif ((fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tif ((mp->m_sb.sb_rblocks == 0) ||\n\t\t\t    (mp->m_sb.sb_rextsize == 0) ||\n\t\t\t    (ip->i_d.di_extsize % mp->m_sb.sb_rextsize)) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Can't modify an immutable/append-only file unless\n\t\t * we have appropriate permission.\n\t\t */\n\t\tif ((ip->i_d.di_flags &\n\t\t\t\t(XFS_DIFLAG_IMMUTABLE|XFS_DIFLAG_APPEND) ||\n\t\t     (fa->fsx_xflags &\n\t\t\t\t(XFS_XFLAG_IMMUTABLE | XFS_XFLAG_APPEND))) &&\n\t\t    !capable(CAP_LINUX_IMMUTABLE)) {\n\t\t\tcode = XFS_ERROR(EPERM);\n\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\txfs_trans_ijoin(tp, ip, 0);\n\n\t/*\n\t * Change file ownership.  Must be the owner or privileged.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\t/*\n\t\t * CAP_FSETID overrides the following restrictions:\n\t\t *\n\t\t * The set-user-ID and set-group-ID bits of a file will be\n\t\t * cleared upon successful return from chown()\n\t\t */\n\t\tif ((ip->i_d.di_mode & (S_ISUID|S_ISGID)) &&\n\t\t    !capable_wrt_inode_uidgid(VFS_I(ip), CAP_FSETID))\n\t\t\tip->i_d.di_mode &= ~(S_ISUID|S_ISGID);\n\n\t\t/*\n\t\t * Change the ownerships and register quota modifications\n\t\t * in the transaction.\n\t\t */\n\t\tif (xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tif (XFS_IS_QUOTA_RUNNING(mp) && XFS_IS_PQUOTA_ON(mp)) {\n\t\t\t\tolddquot = xfs_qm_vop_chown(tp, ip,\n\t\t\t\t\t\t\t&ip->i_pdquot, pdqp);\n\t\t\t}\n\t\t\txfs_set_projid(ip, fa->fsx_projid);\n\n\t\t\t/*\n\t\t\t * We may have to rev the inode as well as\n\t\t\t * the superblock version number since projids didn't\n\t\t\t * exist before DINODE_VERSION_2 and SB_VERSION_NLINK.\n\t\t\t */\n\t\t\tif (ip->i_d.di_version == 1)\n\t\t\t\txfs_bump_ino_vers2(tp, ip);\n\t\t}\n\n\t}\n\n\tif (mask & FSX_EXTSIZE)\n\t\tip->i_d.di_extsize = fa->fsx_extsize >> mp->m_sb.sb_blocklog;\n\tif (mask & FSX_XFLAGS) {\n\t\txfs_set_diflags(ip, fa->fsx_xflags);\n\t\txfs_diflags_to_linux(ip);\n\t}\n\n\txfs_trans_ichgtime(tp, ip, XFS_ICHGTIME_CHG);\n\txfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);\n\n\tXFS_STATS_INC(xs_ig_attrchg);\n\n\t/*\n\t * If this is a synchronous mount, make sure that the\n\t * transaction goes to disk before returning to the user.\n\t * This is slightly sub-optimal in that truncates require\n\t * two sync transactions instead of one for wsync filesystems.\n\t * One for the truncate and one for the timestamps since we\n\t * don't want to change the timestamps unless we're sure the\n\t * truncate worked.  Truncates are less than 1% of the laddis\n\t * mix so this probably isn't worth the trouble to optimize.\n\t */\n\tif (mp->m_flags & XFS_MOUNT_WSYNC)\n\t\txfs_trans_set_sync(tp);\n\tcode = xfs_trans_commit(tp, 0);\n\txfs_iunlock(ip, lock_flags);\n\n\t/*\n\t * Release any dquot(s) the inode had kept before chown.\n\t */\n\txfs_qm_dqrele(olddquot);\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\n\treturn code;\n\n error_return:\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\txfs_trans_cancel(tp, 0);\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\n\treturn code;\n}",
        "patch": "--- code before\n+++ code after\n@@ -190,7 +190,7 @@\n \t\t * cleared upon successful return from chown()\n \t\t */\n \t\tif ((ip->i_d.di_mode & (S_ISUID|S_ISGID)) &&\n-\t\t    !inode_capable(VFS_I(ip), CAP_FSETID))\n+\t\t    !capable_wrt_inode_uidgid(VFS_I(ip), CAP_FSETID))\n \t\t\tip->i_d.di_mode &= ~(S_ISUID|S_ISGID);\n \n \t\t/*",
        "function_modified_lines": {
            "added": [
                "\t\t    !capable_wrt_inode_uidgid(VFS_I(ip), CAP_FSETID))"
            ],
            "deleted": [
                "\t\t    !inode_capable(VFS_I(ip), CAP_FSETID))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The capabilities implementation in the Linux kernel before 3.14.8 does not properly consider that namespaces are inapplicable to inodes, which allows local users to bypass intended chmod restrictions by first creating a user namespace, as demonstrated by setting the setgid bit on a file with group ownership of root.",
        "id": 555
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (np->opt != NULL)\n\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n\t\t\t\t\t  np->opt->opt_nflen);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
        "code_after_change": "static int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct ipv6_txoptions *opt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \tstruct ipv6_pinfo *np = inet6_sk(sk);\n \tstruct dccp_sock *dp = dccp_sk(sk);\n \tstruct in6_addr *saddr = NULL, *final_p, final;\n+\tstruct ipv6_txoptions *opt;\n \tstruct flowi6 fl6;\n \tstruct dst_entry *dst;\n \tint addr_type;\n@@ -106,7 +107,8 @@\n \tfl6.fl6_sport = inet->inet_sport;\n \tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n \n-\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n+\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n+\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n \n \tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n \tif (IS_ERR(dst)) {\n@@ -126,9 +128,8 @@\n \t__ip6_dst_store(sk, dst, NULL, NULL);\n \n \ticsk->icsk_ext_hdr_len = 0;\n-\tif (np->opt != NULL)\n-\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n-\t\t\t\t\t  np->opt->opt_nflen);\n+\tif (opt)\n+\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n \n \tinet->inet_dport = usin->sin6_port;\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct ipv6_txoptions *opt;",
                "\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));",
                "\tfinal_p = fl6_update_dst(&fl6, opt, &final);",
                "\tif (opt)",
                "\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;"
            ],
            "deleted": [
                "\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);",
                "\tif (np->opt != NULL)",
                "\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +",
                "\t\t\t\t\t  np->opt->opt_nflen);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 990
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "struct ipv6_txoptions *\nipv6_renew_options(struct sock *sk, struct ipv6_txoptions *opt,\n\t\t   int newtype,\n\t\t   struct ipv6_opt_hdr __user *newopt, int newoptlen)\n{\n\tint tot_len = 0;\n\tchar *p;\n\tstruct ipv6_txoptions *opt2;\n\tint err;\n\n\tif (opt) {\n\t\tif (newtype != IPV6_HOPOPTS && opt->hopopt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->hopopt));\n\t\tif (newtype != IPV6_RTHDRDSTOPTS && opt->dst0opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst0opt));\n\t\tif (newtype != IPV6_RTHDR && opt->srcrt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->srcrt));\n\t\tif (newtype != IPV6_DSTOPTS && opt->dst1opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst1opt));\n\t}\n\n\tif (newopt && newoptlen)\n\t\ttot_len += CMSG_ALIGN(newoptlen);\n\n\tif (!tot_len)\n\t\treturn NULL;\n\n\ttot_len += sizeof(*opt2);\n\topt2 = sock_kmalloc(sk, tot_len, GFP_ATOMIC);\n\tif (!opt2)\n\t\treturn ERR_PTR(-ENOBUFS);\n\n\tmemset(opt2, 0, tot_len);\n\n\topt2->tot_len = tot_len;\n\tp = (char *)(opt2 + 1);\n\n\terr = ipv6_renew_option(opt ? opt->hopopt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_HOPOPTS,\n\t\t\t\t&opt2->hopopt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst0opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDRDSTOPTS,\n\t\t\t\t&opt2->dst0opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->srcrt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDR,\n\t\t\t\t(struct ipv6_opt_hdr **)&opt2->srcrt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst1opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_DSTOPTS,\n\t\t\t\t&opt2->dst1opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\topt2->opt_nflen = (opt2->hopopt ? ipv6_optlen(opt2->hopopt) : 0) +\n\t\t\t  (opt2->dst0opt ? ipv6_optlen(opt2->dst0opt) : 0) +\n\t\t\t  (opt2->srcrt ? ipv6_optlen(opt2->srcrt) : 0);\n\topt2->opt_flen = (opt2->dst1opt ? ipv6_optlen(opt2->dst1opt) : 0);\n\n\treturn opt2;\nout:\n\tsock_kfree_s(sk, opt2, opt2->tot_len);\n\treturn ERR_PTR(err);\n}",
        "code_after_change": "struct ipv6_txoptions *\nipv6_renew_options(struct sock *sk, struct ipv6_txoptions *opt,\n\t\t   int newtype,\n\t\t   struct ipv6_opt_hdr __user *newopt, int newoptlen)\n{\n\tint tot_len = 0;\n\tchar *p;\n\tstruct ipv6_txoptions *opt2;\n\tint err;\n\n\tif (opt) {\n\t\tif (newtype != IPV6_HOPOPTS && opt->hopopt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->hopopt));\n\t\tif (newtype != IPV6_RTHDRDSTOPTS && opt->dst0opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst0opt));\n\t\tif (newtype != IPV6_RTHDR && opt->srcrt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->srcrt));\n\t\tif (newtype != IPV6_DSTOPTS && opt->dst1opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst1opt));\n\t}\n\n\tif (newopt && newoptlen)\n\t\ttot_len += CMSG_ALIGN(newoptlen);\n\n\tif (!tot_len)\n\t\treturn NULL;\n\n\ttot_len += sizeof(*opt2);\n\topt2 = sock_kmalloc(sk, tot_len, GFP_ATOMIC);\n\tif (!opt2)\n\t\treturn ERR_PTR(-ENOBUFS);\n\n\tmemset(opt2, 0, tot_len);\n\tatomic_set(&opt2->refcnt, 1);\n\topt2->tot_len = tot_len;\n\tp = (char *)(opt2 + 1);\n\n\terr = ipv6_renew_option(opt ? opt->hopopt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_HOPOPTS,\n\t\t\t\t&opt2->hopopt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst0opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDRDSTOPTS,\n\t\t\t\t&opt2->dst0opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->srcrt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDR,\n\t\t\t\t(struct ipv6_opt_hdr **)&opt2->srcrt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst1opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_DSTOPTS,\n\t\t\t\t&opt2->dst1opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\topt2->opt_nflen = (opt2->hopopt ? ipv6_optlen(opt2->hopopt) : 0) +\n\t\t\t  (opt2->dst0opt ? ipv6_optlen(opt2->dst0opt) : 0) +\n\t\t\t  (opt2->srcrt ? ipv6_optlen(opt2->srcrt) : 0);\n\topt2->opt_flen = (opt2->dst1opt ? ipv6_optlen(opt2->dst1opt) : 0);\n\n\treturn opt2;\nout:\n\tsock_kfree_s(sk, opt2, opt2->tot_len);\n\treturn ERR_PTR(err);\n}",
        "patch": "--- code before\n+++ code after\n@@ -31,7 +31,7 @@\n \t\treturn ERR_PTR(-ENOBUFS);\n \n \tmemset(opt2, 0, tot_len);\n-\n+\tatomic_set(&opt2->refcnt, 1);\n \topt2->tot_len = tot_len;\n \tp = (char *)(opt2 + 1);\n ",
        "function_modified_lines": {
            "added": [
                "\tatomic_set(&opt2->refcnt, 1);"
            ],
            "deleted": [
                ""
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 996
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static struct sock *tcp_v6_syn_recv_sock(const struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t struct request_sock *req,\n\t\t\t\t\t struct dst_entry *dst,\n\t\t\t\t\t struct request_sock *req_unhash,\n\t\t\t\t\t bool *own_req)\n{\n\tstruct inet_request_sock *ireq;\n\tstruct ipv6_pinfo *newnp;\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp6_sock *newtcp6sk;\n\tstruct inet_sock *newinet;\n\tstruct tcp_sock *newtp;\n\tstruct sock *newsk;\n#ifdef CONFIG_TCP_MD5SIG\n\tstruct tcp_md5sig_key *key;\n#endif\n\tstruct flowi6 fl6;\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\n\t\tnewsk = tcp_v4_syn_recv_sock(sk, skb, req, dst,\n\t\t\t\t\t     req_unhash, own_req);\n\n\t\tif (!newsk)\n\t\t\treturn NULL;\n\n\t\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\t\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\t\tnewinet = inet_sk(newsk);\n\t\tnewnp = inet6_sk(newsk);\n\t\tnewtp = tcp_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tnewnp->saddr = newsk->sk_v6_rcv_saddr;\n\n\t\tinet_csk(newsk)->icsk_af_ops = &ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\tnewtp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\tnewnp->ipv6_ac_list = NULL;\n\t\tnewnp->ipv6_fl_list = NULL;\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = tcp_v6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\t\tnewnp->rcv_flowinfo = ip6_flowinfo(ipv6_hdr(skb));\n\t\tif (np->repflow)\n\t\t\tnewnp->flow_label = ip6_flowlabel(ipv6_hdr(skb));\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, tcp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\ttcp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\tireq = inet_rsk(req);\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (!dst) {\n\t\tdst = inet6_csk_route_req(sk, &fl6, req, IPPROTO_TCP);\n\t\tif (!dst)\n\t\t\tgoto out;\n\t}\n\n\tnewsk = tcp_create_openreq_child(sk, req, skb);\n\tif (!newsk)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, tcp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\tnewsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\tinet6_sk_rx_dst_set(newsk, skb);\n\n\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\tnewtp = tcp_sk(newsk);\n\tnewinet = inet_sk(newsk);\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tnewsk->sk_v6_daddr = ireq->ir_v6_rmt_addr;\n\tnewnp->saddr = ireq->ir_v6_loc_addr;\n\tnewsk->sk_v6_rcv_saddr = ireq->ir_v6_loc_addr;\n\tnewsk->sk_bound_dev_if = ireq->ir_iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->inet_opt = NULL;\n\tnewnp->ipv6_ac_list = NULL;\n\tnewnp->ipv6_fl_list = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\tnewnp->pktoptions = NULL;\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = tcp_v6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\tnewnp->rcv_flowinfo = ip6_flowinfo(ipv6_hdr(skb));\n\tif (np->repflow)\n\t\tnewnp->flow_label = ip6_flowlabel(ipv6_hdr(skb));\n\n\t/* Clone native IPv6 options from listening socket (if any)\n\n\t   Yes, keeping reference count would be much more clever,\n\t   but we make one more one thing there: reattach optmem\n\t   to newsk.\n\t */\n\tif (np->opt)\n\t\tnewnp->opt = ipv6_dup_options(newsk, np->opt);\n\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (newnp->opt)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n\t\t\t\t\t\t     newnp->opt->opt_flen);\n\n\ttcp_ca_openreq_child(newsk, dst);\n\n\ttcp_sync_mss(newsk, dst_mtu(dst));\n\tnewtp->advmss = dst_metric_advmss(dst);\n\tif (tcp_sk(sk)->rx_opt.user_mss &&\n\t    tcp_sk(sk)->rx_opt.user_mss < newtp->advmss)\n\t\tnewtp->advmss = tcp_sk(sk)->rx_opt.user_mss;\n\n\ttcp_initialize_rcv_mss(newsk);\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Copy over the MD5 key from the original socket */\n\tkey = tcp_v6_md5_do_lookup(sk, &newsk->sk_v6_daddr);\n\tif (key) {\n\t\t/* We're using one, so create a matching key\n\t\t * on the newsk structure. If we fail to get\n\t\t * memory, then we end up not copying the key\n\t\t * across. Shucks.\n\t\t */\n\t\ttcp_md5_do_add(newsk, (union tcp_md5_addr *)&newsk->sk_v6_daddr,\n\t\t\t       AF_INET6, key->key, key->keylen,\n\t\t\t       sk_gfp_atomic(sk, GFP_ATOMIC));\n\t}\n#endif\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tinet_csk_prepare_forced_close(newsk);\n\t\ttcp_done(newsk);\n\t\tgoto out;\n\t}\n\t*own_req = inet_ehash_nolisten(newsk, req_to_sk(req_unhash));\n\tif (*own_req) {\n\t\ttcp_move_syn(newtp, req);\n\n\t\t/* Clone pktoptions received with SYN, if we own the req */\n\t\tif (ireq->pktopts) {\n\t\t\tnewnp->pktoptions = skb_clone(ireq->pktopts,\n\t\t\t\t\t\t      sk_gfp_atomic(sk, GFP_ATOMIC));\n\t\t\tconsume_skb(ireq->pktopts);\n\t\t\tireq->pktopts = NULL;\n\t\t\tif (newnp->pktoptions)\n\t\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t\t}\n\t}\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}",
        "code_after_change": "static struct sock *tcp_v6_syn_recv_sock(const struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t struct request_sock *req,\n\t\t\t\t\t struct dst_entry *dst,\n\t\t\t\t\t struct request_sock *req_unhash,\n\t\t\t\t\t bool *own_req)\n{\n\tstruct inet_request_sock *ireq;\n\tstruct ipv6_pinfo *newnp;\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt;\n\tstruct tcp6_sock *newtcp6sk;\n\tstruct inet_sock *newinet;\n\tstruct tcp_sock *newtp;\n\tstruct sock *newsk;\n#ifdef CONFIG_TCP_MD5SIG\n\tstruct tcp_md5sig_key *key;\n#endif\n\tstruct flowi6 fl6;\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\n\t\tnewsk = tcp_v4_syn_recv_sock(sk, skb, req, dst,\n\t\t\t\t\t     req_unhash, own_req);\n\n\t\tif (!newsk)\n\t\t\treturn NULL;\n\n\t\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\t\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\t\tnewinet = inet_sk(newsk);\n\t\tnewnp = inet6_sk(newsk);\n\t\tnewtp = tcp_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tnewnp->saddr = newsk->sk_v6_rcv_saddr;\n\n\t\tinet_csk(newsk)->icsk_af_ops = &ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\tnewtp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\tnewnp->ipv6_ac_list = NULL;\n\t\tnewnp->ipv6_fl_list = NULL;\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = tcp_v6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\t\tnewnp->rcv_flowinfo = ip6_flowinfo(ipv6_hdr(skb));\n\t\tif (np->repflow)\n\t\t\tnewnp->flow_label = ip6_flowlabel(ipv6_hdr(skb));\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, tcp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\ttcp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\tireq = inet_rsk(req);\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (!dst) {\n\t\tdst = inet6_csk_route_req(sk, &fl6, req, IPPROTO_TCP);\n\t\tif (!dst)\n\t\t\tgoto out;\n\t}\n\n\tnewsk = tcp_create_openreq_child(sk, req, skb);\n\tif (!newsk)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, tcp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\tnewsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\tinet6_sk_rx_dst_set(newsk, skb);\n\n\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\tnewtp = tcp_sk(newsk);\n\tnewinet = inet_sk(newsk);\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tnewsk->sk_v6_daddr = ireq->ir_v6_rmt_addr;\n\tnewnp->saddr = ireq->ir_v6_loc_addr;\n\tnewsk->sk_v6_rcv_saddr = ireq->ir_v6_loc_addr;\n\tnewsk->sk_bound_dev_if = ireq->ir_iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->inet_opt = NULL;\n\tnewnp->ipv6_ac_list = NULL;\n\tnewnp->ipv6_fl_list = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\tnewnp->pktoptions = NULL;\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = tcp_v6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\tnewnp->rcv_flowinfo = ip6_flowinfo(ipv6_hdr(skb));\n\tif (np->repflow)\n\t\tnewnp->flow_label = ip6_flowlabel(ipv6_hdr(skb));\n\n\t/* Clone native IPv6 options from listening socket (if any)\n\n\t   Yes, keeping reference count would be much more clever,\n\t   but we make one more one thing there: reattach optmem\n\t   to newsk.\n\t */\n\topt = rcu_dereference(np->opt);\n\tif (opt) {\n\t\topt = ipv6_dup_options(newsk, opt);\n\t\tRCU_INIT_POINTER(newnp->opt, opt);\n\t}\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = opt->opt_nflen +\n\t\t\t\t\t\t    opt->opt_flen;\n\n\ttcp_ca_openreq_child(newsk, dst);\n\n\ttcp_sync_mss(newsk, dst_mtu(dst));\n\tnewtp->advmss = dst_metric_advmss(dst);\n\tif (tcp_sk(sk)->rx_opt.user_mss &&\n\t    tcp_sk(sk)->rx_opt.user_mss < newtp->advmss)\n\t\tnewtp->advmss = tcp_sk(sk)->rx_opt.user_mss;\n\n\ttcp_initialize_rcv_mss(newsk);\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Copy over the MD5 key from the original socket */\n\tkey = tcp_v6_md5_do_lookup(sk, &newsk->sk_v6_daddr);\n\tif (key) {\n\t\t/* We're using one, so create a matching key\n\t\t * on the newsk structure. If we fail to get\n\t\t * memory, then we end up not copying the key\n\t\t * across. Shucks.\n\t\t */\n\t\ttcp_md5_do_add(newsk, (union tcp_md5_addr *)&newsk->sk_v6_daddr,\n\t\t\t       AF_INET6, key->key, key->keylen,\n\t\t\t       sk_gfp_atomic(sk, GFP_ATOMIC));\n\t}\n#endif\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tinet_csk_prepare_forced_close(newsk);\n\t\ttcp_done(newsk);\n\t\tgoto out;\n\t}\n\t*own_req = inet_ehash_nolisten(newsk, req_to_sk(req_unhash));\n\tif (*own_req) {\n\t\ttcp_move_syn(newtp, req);\n\n\t\t/* Clone pktoptions received with SYN, if we own the req */\n\t\tif (ireq->pktopts) {\n\t\t\tnewnp->pktoptions = skb_clone(ireq->pktopts,\n\t\t\t\t\t\t      sk_gfp_atomic(sk, GFP_ATOMIC));\n\t\t\tconsume_skb(ireq->pktopts);\n\t\t\tireq->pktopts = NULL;\n\t\t\tif (newnp->pktoptions)\n\t\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t\t}\n\t}\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \tstruct inet_request_sock *ireq;\n \tstruct ipv6_pinfo *newnp;\n \tconst struct ipv6_pinfo *np = inet6_sk(sk);\n+\tstruct ipv6_txoptions *opt;\n \tstruct tcp6_sock *newtcp6sk;\n \tstruct inet_sock *newinet;\n \tstruct tcp_sock *newtp;\n@@ -133,13 +134,15 @@\n \t   but we make one more one thing there: reattach optmem\n \t   to newsk.\n \t */\n-\tif (np->opt)\n-\t\tnewnp->opt = ipv6_dup_options(newsk, np->opt);\n-\n+\topt = rcu_dereference(np->opt);\n+\tif (opt) {\n+\t\topt = ipv6_dup_options(newsk, opt);\n+\t\tRCU_INIT_POINTER(newnp->opt, opt);\n+\t}\n \tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n-\tif (newnp->opt)\n-\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n-\t\t\t\t\t\t     newnp->opt->opt_flen);\n+\tif (opt)\n+\t\tinet_csk(newsk)->icsk_ext_hdr_len = opt->opt_nflen +\n+\t\t\t\t\t\t    opt->opt_flen;\n \n \ttcp_ca_openreq_child(newsk, dst);\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct ipv6_txoptions *opt;",
                "\topt = rcu_dereference(np->opt);",
                "\tif (opt) {",
                "\t\topt = ipv6_dup_options(newsk, opt);",
                "\t\tRCU_INIT_POINTER(newnp->opt, opt);",
                "\t}",
                "\tif (opt)",
                "\t\tinet_csk(newsk)->icsk_ext_hdr_len = opt->opt_nflen +",
                "\t\t\t\t\t\t    opt->opt_flen;"
            ],
            "deleted": [
                "\tif (np->opt)",
                "\t\tnewnp->opt = ipv6_dup_options(newsk, np->opt);",
                "",
                "\tif (newnp->opt)",
                "\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +",
                "\t\t\t\t\t\t     newnp->opt->opt_flen);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1006
    },
    {
        "cve_id": "CVE-2015-8709",
        "code_before_change": "static int __ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\tint dumpable = 0;\n\tkuid_t caller_uid;\n\tkgid_t caller_gid;\n\n\tif (!(mode & PTRACE_MODE_FSCREDS) == !(mode & PTRACE_MODE_REALCREDS)) {\n\t\tWARN(1, \"denying ptrace access check without PTRACE_MODE_*CREDS\\n\");\n\t\treturn -EPERM;\n\t}\n\n\t/* May we inspect the given task?\n\t * This check is used both for attaching with ptrace\n\t * and for allowing access to sensitive information in /proc.\n\t *\n\t * ptrace_attach denies several cases that /proc allows\n\t * because setting up the necessary parent/child relationship\n\t * or halting the specified task is impossible.\n\t */\n\n\t/* Don't let security modules deny introspection */\n\tif (same_thread_group(task, current))\n\t\treturn 0;\n\trcu_read_lock();\n\tif (mode & PTRACE_MODE_FSCREDS) {\n\t\tcaller_uid = cred->fsuid;\n\t\tcaller_gid = cred->fsgid;\n\t} else {\n\t\t/*\n\t\t * Using the euid would make more sense here, but something\n\t\t * in userland might rely on the old behavior, and this\n\t\t * shouldn't be a security problem since\n\t\t * PTRACE_MODE_REALCREDS implies that the caller explicitly\n\t\t * used a syscall that requests access to another process\n\t\t * (and not a filesystem syscall to procfs).\n\t\t */\n\t\tcaller_uid = cred->uid;\n\t\tcaller_gid = cred->gid;\n\t}\n\ttcred = __task_cred(task);\n\tif (uid_eq(caller_uid, tcred->euid) &&\n\t    uid_eq(caller_uid, tcred->suid) &&\n\t    uid_eq(caller_uid, tcred->uid)  &&\n\t    gid_eq(caller_gid, tcred->egid) &&\n\t    gid_eq(caller_gid, tcred->sgid) &&\n\t    gid_eq(caller_gid, tcred->gid))\n\t\tgoto ok;\n\tif (ptrace_has_cap(tcred->user_ns, mode))\n\t\tgoto ok;\n\trcu_read_unlock();\n\treturn -EPERM;\nok:\n\trcu_read_unlock();\n\tsmp_rmb();\n\tif (task->mm)\n\t\tdumpable = get_dumpable(task->mm);\n\trcu_read_lock();\n\tif (dumpable != SUID_DUMP_USER &&\n\t    !ptrace_has_cap(__task_cred(task)->user_ns, mode)) {\n\t\trcu_read_unlock();\n\t\treturn -EPERM;\n\t}\n\trcu_read_unlock();\n\n\treturn security_ptrace_access_check(task, mode);\n}",
        "code_after_change": "static int __ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\tstruct mm_struct *mm;\n\tkuid_t caller_uid;\n\tkgid_t caller_gid;\n\n\tif (!(mode & PTRACE_MODE_FSCREDS) == !(mode & PTRACE_MODE_REALCREDS)) {\n\t\tWARN(1, \"denying ptrace access check without PTRACE_MODE_*CREDS\\n\");\n\t\treturn -EPERM;\n\t}\n\n\t/* May we inspect the given task?\n\t * This check is used both for attaching with ptrace\n\t * and for allowing access to sensitive information in /proc.\n\t *\n\t * ptrace_attach denies several cases that /proc allows\n\t * because setting up the necessary parent/child relationship\n\t * or halting the specified task is impossible.\n\t */\n\n\t/* Don't let security modules deny introspection */\n\tif (same_thread_group(task, current))\n\t\treturn 0;\n\trcu_read_lock();\n\tif (mode & PTRACE_MODE_FSCREDS) {\n\t\tcaller_uid = cred->fsuid;\n\t\tcaller_gid = cred->fsgid;\n\t} else {\n\t\t/*\n\t\t * Using the euid would make more sense here, but something\n\t\t * in userland might rely on the old behavior, and this\n\t\t * shouldn't be a security problem since\n\t\t * PTRACE_MODE_REALCREDS implies that the caller explicitly\n\t\t * used a syscall that requests access to another process\n\t\t * (and not a filesystem syscall to procfs).\n\t\t */\n\t\tcaller_uid = cred->uid;\n\t\tcaller_gid = cred->gid;\n\t}\n\ttcred = __task_cred(task);\n\tif (uid_eq(caller_uid, tcred->euid) &&\n\t    uid_eq(caller_uid, tcred->suid) &&\n\t    uid_eq(caller_uid, tcred->uid)  &&\n\t    gid_eq(caller_gid, tcred->egid) &&\n\t    gid_eq(caller_gid, tcred->sgid) &&\n\t    gid_eq(caller_gid, tcred->gid))\n\t\tgoto ok;\n\tif (ptrace_has_cap(tcred->user_ns, mode))\n\t\tgoto ok;\n\trcu_read_unlock();\n\treturn -EPERM;\nok:\n\trcu_read_unlock();\n\tmm = task->mm;\n\tif (mm &&\n\t    ((get_dumpable(mm) != SUID_DUMP_USER) &&\n\t     !ptrace_has_cap(mm->user_ns, mode)))\n\t    return -EPERM;\n\n\treturn security_ptrace_access_check(task, mode);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,7 @@\n static int __ptrace_may_access(struct task_struct *task, unsigned int mode)\n {\n \tconst struct cred *cred = current_cred(), *tcred;\n-\tint dumpable = 0;\n+\tstruct mm_struct *mm;\n \tkuid_t caller_uid;\n \tkgid_t caller_gid;\n \n@@ -52,16 +52,11 @@\n \treturn -EPERM;\n ok:\n \trcu_read_unlock();\n-\tsmp_rmb();\n-\tif (task->mm)\n-\t\tdumpable = get_dumpable(task->mm);\n-\trcu_read_lock();\n-\tif (dumpable != SUID_DUMP_USER &&\n-\t    !ptrace_has_cap(__task_cred(task)->user_ns, mode)) {\n-\t\trcu_read_unlock();\n-\t\treturn -EPERM;\n-\t}\n-\trcu_read_unlock();\n+\tmm = task->mm;\n+\tif (mm &&\n+\t    ((get_dumpable(mm) != SUID_DUMP_USER) &&\n+\t     !ptrace_has_cap(mm->user_ns, mode)))\n+\t    return -EPERM;\n \n \treturn security_ptrace_access_check(task, mode);\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct mm_struct *mm;",
                "\tmm = task->mm;",
                "\tif (mm &&",
                "\t    ((get_dumpable(mm) != SUID_DUMP_USER) &&",
                "\t     !ptrace_has_cap(mm->user_ns, mode)))",
                "\t    return -EPERM;"
            ],
            "deleted": [
                "\tint dumpable = 0;",
                "\tsmp_rmb();",
                "\tif (task->mm)",
                "\t\tdumpable = get_dumpable(task->mm);",
                "\trcu_read_lock();",
                "\tif (dumpable != SUID_DUMP_USER &&",
                "\t    !ptrace_has_cap(__task_cred(task)->user_ns, mode)) {",
                "\t\trcu_read_unlock();",
                "\t\treturn -EPERM;",
                "\t}",
                "\trcu_read_unlock();"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "kernel/ptrace.c in the Linux kernel through 4.4.1 mishandles uid and gid mappings, which allows local users to gain privileges by establishing a user namespace, waiting for a root process to enter that namespace with an unsafe uid or gid, and then using the ptrace system call.  NOTE: the vendor states \"there is no kernel bug here.",
        "id": 838
    },
    {
        "cve_id": "CVE-2016-3857",
        "code_before_change": "asmlinkage long sys_oabi_semtimedop(int semid,\n\t\t\t\t    struct oabi_sembuf __user *tsops,\n\t\t\t\t    unsigned nsops,\n\t\t\t\t    const struct timespec __user *timeout)\n{\n\tstruct sembuf *sops;\n\tstruct timespec local_timeout;\n\tlong err;\n\tint i;\n\n\tif (nsops < 1 || nsops > SEMOPM)\n\t\treturn -EINVAL;\n\tsops = kmalloc(sizeof(*sops) * nsops, GFP_KERNEL);\n\tif (!sops)\n\t\treturn -ENOMEM;\n\terr = 0;\n\tfor (i = 0; i < nsops; i++) {\n\t\t__get_user_error(sops[i].sem_num, &tsops->sem_num, err);\n\t\t__get_user_error(sops[i].sem_op,  &tsops->sem_op,  err);\n\t\t__get_user_error(sops[i].sem_flg, &tsops->sem_flg, err);\n\t\ttsops++;\n\t}\n\tif (timeout) {\n\t\t/* copy this as well before changing domain protection */\n\t\terr |= copy_from_user(&local_timeout, timeout, sizeof(*timeout));\n\t\ttimeout = &local_timeout;\n\t}\n\tif (err) {\n\t\terr = -EFAULT;\n\t} else {\n\t\tmm_segment_t fs = get_fs();\n\t\tset_fs(KERNEL_DS);\n\t\terr = sys_semtimedop(semid, sops, nsops, timeout);\n\t\tset_fs(fs);\n\t}\n\tkfree(sops);\n\treturn err;\n}",
        "code_after_change": "asmlinkage long sys_oabi_semtimedop(int semid,\n\t\t\t\t    struct oabi_sembuf __user *tsops,\n\t\t\t\t    unsigned nsops,\n\t\t\t\t    const struct timespec __user *timeout)\n{\n\tstruct sembuf *sops;\n\tstruct timespec local_timeout;\n\tlong err;\n\tint i;\n\n\tif (nsops < 1 || nsops > SEMOPM)\n\t\treturn -EINVAL;\n\tif (!access_ok(VERIFY_READ, tsops, sizeof(*tsops) * nsops))\n\t\treturn -EFAULT;\n\tsops = kmalloc(sizeof(*sops) * nsops, GFP_KERNEL);\n\tif (!sops)\n\t\treturn -ENOMEM;\n\terr = 0;\n\tfor (i = 0; i < nsops; i++) {\n\t\t__get_user_error(sops[i].sem_num, &tsops->sem_num, err);\n\t\t__get_user_error(sops[i].sem_op,  &tsops->sem_op,  err);\n\t\t__get_user_error(sops[i].sem_flg, &tsops->sem_flg, err);\n\t\ttsops++;\n\t}\n\tif (timeout) {\n\t\t/* copy this as well before changing domain protection */\n\t\terr |= copy_from_user(&local_timeout, timeout, sizeof(*timeout));\n\t\ttimeout = &local_timeout;\n\t}\n\tif (err) {\n\t\terr = -EFAULT;\n\t} else {\n\t\tmm_segment_t fs = get_fs();\n\t\tset_fs(KERNEL_DS);\n\t\terr = sys_semtimedop(semid, sops, nsops, timeout);\n\t\tset_fs(fs);\n\t}\n\tkfree(sops);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,6 +10,8 @@\n \n \tif (nsops < 1 || nsops > SEMOPM)\n \t\treturn -EINVAL;\n+\tif (!access_ok(VERIFY_READ, tsops, sizeof(*tsops) * nsops))\n+\t\treturn -EFAULT;\n \tsops = kmalloc(sizeof(*sops) * nsops, GFP_KERNEL);\n \tif (!sops)\n \t\treturn -ENOMEM;",
        "function_modified_lines": {
            "added": [
                "\tif (!access_ok(VERIFY_READ, tsops, sizeof(*tsops) * nsops))",
                "\t\treturn -EFAULT;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The kernel in Android before 2016-08-05 on Nexus 7 (2013) devices allows attackers to gain privileges via a crafted application, aka internal bug 28522518.",
        "id": 1011
    },
    {
        "cve_id": "CVE-2013-1957",
        "code_before_change": "static int change_mount_flags(struct vfsmount *mnt, int ms_flags)\n{\n\tint error = 0;\n\tint readonly_request = 0;\n\n\tif (ms_flags & MS_RDONLY)\n\t\treadonly_request = 1;\n\tif (readonly_request == __mnt_is_readonly(mnt))\n\t\treturn 0;\n\n\tif (readonly_request)\n\t\terror = mnt_make_readonly(real_mount(mnt));\n\telse\n\t\t__mnt_unmake_readonly(real_mount(mnt));\n\treturn error;\n}",
        "code_after_change": "static int change_mount_flags(struct vfsmount *mnt, int ms_flags)\n{\n\tint error = 0;\n\tint readonly_request = 0;\n\n\tif (ms_flags & MS_RDONLY)\n\t\treadonly_request = 1;\n\tif (readonly_request == __mnt_is_readonly(mnt))\n\t\treturn 0;\n\n\tif (mnt->mnt_flags & MNT_LOCK_READONLY)\n\t\treturn -EPERM;\n\n\tif (readonly_request)\n\t\terror = mnt_make_readonly(real_mount(mnt));\n\telse\n\t\t__mnt_unmake_readonly(real_mount(mnt));\n\treturn error;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,6 +8,9 @@\n \tif (readonly_request == __mnt_is_readonly(mnt))\n \t\treturn 0;\n \n+\tif (mnt->mnt_flags & MNT_LOCK_READONLY)\n+\t\treturn -EPERM;\n+\n \tif (readonly_request)\n \t\terror = mnt_make_readonly(real_mount(mnt));\n \telse",
        "function_modified_lines": {
            "added": [
                "\tif (mnt->mnt_flags & MNT_LOCK_READONLY)",
                "\t\treturn -EPERM;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The clone_mnt function in fs/namespace.c in the Linux kernel before 3.8.6 does not properly restrict changes to the MNT_READONLY flag, which allows local users to bypass an intended read-only property of a filesystem by leveraging a separate mount namespace.",
        "id": 211
    },
    {
        "cve_id": "CVE-2016-6786",
        "code_before_change": "static int perf_event_read_group(struct perf_event *event,\n\t\t\t\t   u64 read_format, char __user *buf)\n{\n\tstruct perf_event *leader = event->group_leader, *sub;\n\tint n = 0, size = 0, ret = -EFAULT;\n\tstruct perf_event_context *ctx = leader->ctx;\n\tu64 values[5];\n\tu64 count, enabled, running;\n\n\tmutex_lock(&ctx->mutex);\n\tcount = perf_event_read_value(leader, &enabled, &running);\n\n\tvalues[n++] = 1 + leader->nr_siblings;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\n\t\tvalues[n++] = enabled;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\n\t\tvalues[n++] = running;\n\tvalues[n++] = count;\n\tif (read_format & PERF_FORMAT_ID)\n\t\tvalues[n++] = primary_event_id(leader);\n\n\tsize = n * sizeof(u64);\n\n\tif (copy_to_user(buf, values, size))\n\t\tgoto unlock;\n\n\tret = size;\n\n\tlist_for_each_entry(sub, &leader->sibling_list, group_entry) {\n\t\tn = 0;\n\n\t\tvalues[n++] = perf_event_read_value(sub, &enabled, &running);\n\t\tif (read_format & PERF_FORMAT_ID)\n\t\t\tvalues[n++] = primary_event_id(sub);\n\n\t\tsize = n * sizeof(u64);\n\n\t\tif (copy_to_user(buf + ret, values, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tret += size;\n\t}\nunlock:\n\tmutex_unlock(&ctx->mutex);\n\n\treturn ret;\n}",
        "code_after_change": "static int perf_event_read_group(struct perf_event *event,\n\t\t\t\t   u64 read_format, char __user *buf)\n{\n\tstruct perf_event *leader = event->group_leader, *sub;\n\tstruct perf_event_context *ctx = leader->ctx;\n\tint n = 0, size = 0, ret;\n\tu64 count, enabled, running;\n\tu64 values[5];\n\n\tlockdep_assert_held(&ctx->mutex);\n\n\tcount = perf_event_read_value(leader, &enabled, &running);\n\n\tvalues[n++] = 1 + leader->nr_siblings;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\n\t\tvalues[n++] = enabled;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\n\t\tvalues[n++] = running;\n\tvalues[n++] = count;\n\tif (read_format & PERF_FORMAT_ID)\n\t\tvalues[n++] = primary_event_id(leader);\n\n\tsize = n * sizeof(u64);\n\n\tif (copy_to_user(buf, values, size))\n\t\treturn -EFAULT;\n\n\tret = size;\n\n\tlist_for_each_entry(sub, &leader->sibling_list, group_entry) {\n\t\tn = 0;\n\n\t\tvalues[n++] = perf_event_read_value(sub, &enabled, &running);\n\t\tif (read_format & PERF_FORMAT_ID)\n\t\t\tvalues[n++] = primary_event_id(sub);\n\n\t\tsize = n * sizeof(u64);\n\n\t\tif (copy_to_user(buf + ret, values, size)) {\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tret += size;\n\t}\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,12 +2,13 @@\n \t\t\t\t   u64 read_format, char __user *buf)\n {\n \tstruct perf_event *leader = event->group_leader, *sub;\n-\tint n = 0, size = 0, ret = -EFAULT;\n \tstruct perf_event_context *ctx = leader->ctx;\n+\tint n = 0, size = 0, ret;\n+\tu64 count, enabled, running;\n \tu64 values[5];\n-\tu64 count, enabled, running;\n \n-\tmutex_lock(&ctx->mutex);\n+\tlockdep_assert_held(&ctx->mutex);\n+\n \tcount = perf_event_read_value(leader, &enabled, &running);\n \n \tvalues[n++] = 1 + leader->nr_siblings;\n@@ -22,7 +23,7 @@\n \tsize = n * sizeof(u64);\n \n \tif (copy_to_user(buf, values, size))\n-\t\tgoto unlock;\n+\t\treturn -EFAULT;\n \n \tret = size;\n \n@@ -36,14 +37,11 @@\n \t\tsize = n * sizeof(u64);\n \n \t\tif (copy_to_user(buf + ret, values, size)) {\n-\t\t\tret = -EFAULT;\n-\t\t\tgoto unlock;\n+\t\t\treturn -EFAULT;\n \t\t}\n \n \t\tret += size;\n \t}\n-unlock:\n-\tmutex_unlock(&ctx->mutex);\n \n \treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tint n = 0, size = 0, ret;",
                "\tu64 count, enabled, running;",
                "\tlockdep_assert_held(&ctx->mutex);",
                "",
                "\t\treturn -EFAULT;",
                "\t\t\treturn -EFAULT;"
            ],
            "deleted": [
                "\tint n = 0, size = 0, ret = -EFAULT;",
                "\tu64 count, enabled, running;",
                "\tmutex_lock(&ctx->mutex);",
                "\t\tgoto unlock;",
                "\t\t\tret = -EFAULT;",
                "\t\t\tgoto unlock;",
                "unlock:",
                "\tmutex_unlock(&ctx->mutex);"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "kernel/events/core.c in the performance subsystem in the Linux kernel before 4.0 mismanages locks during certain migrations, which allows local users to gain privileges via a crafted application, aka Android internal bug 30955111.",
        "id": 1085
    },
    {
        "cve_id": "CVE-2015-9016",
        "code_before_change": "static void bt_for_each(struct blk_mq_hw_ctx *hctx,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t     \trq = blk_mq_tag_to_rq(hctx->tags, off + bit);\n\t\t\tif (rq->q == hctx->queue)\n\t\t\t\tfn(hctx, rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}",
        "code_after_change": "static void bt_for_each(struct blk_mq_hw_ctx *hctx,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = hctx->tags->rqs[off + bit];\n\t\t\tif (rq->q == hctx->queue)\n\t\t\t\tfn(hctx, rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,7 @@\n \t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n \t\t     bit < bm->depth;\n \t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n-\t\t     \trq = blk_mq_tag_to_rq(hctx->tags, off + bit);\n+\t\t\trq = hctx->tags->rqs[off + bit];\n \t\t\tif (rq->q == hctx->queue)\n \t\t\t\tfn(hctx, rq, data, reserved);\n \t\t}",
        "function_modified_lines": {
            "added": [
                "\t\t\trq = hctx->tags->rqs[off + bit];"
            ],
            "deleted": [
                "\t\t     \trq = blk_mq_tag_to_rq(hctx->tags, off + bit);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-362"
        ],
        "cve_description": "In blk_mq_tag_to_rq in blk-mq.c in the upstream kernel, there is a possible use after free due to a race condition when a request has been previously freed by blk_mq_complete_request. This could lead to local escalation of privilege. Product: Android. Versions: Android kernel. Android ID: A-63083046.",
        "id": 883
    },
    {
        "cve_id": "CVE-2014-8989",
        "code_before_change": "bool may_setgroups(void)\n{\n\tstruct user_namespace *user_ns = current_user_ns();\n\n\treturn ns_capable(user_ns, CAP_SETGID);\n}",
        "code_after_change": "bool may_setgroups(void)\n{\n\tstruct user_namespace *user_ns = current_user_ns();\n\n\treturn ns_capable(user_ns, CAP_SETGID) &&\n\t\tuserns_may_setgroups(user_ns);\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,5 +2,6 @@\n {\n \tstruct user_namespace *user_ns = current_user_ns();\n \n-\treturn ns_capable(user_ns, CAP_SETGID);\n+\treturn ns_capable(user_ns, CAP_SETGID) &&\n+\t\tuserns_may_setgroups(user_ns);\n }",
        "function_modified_lines": {
            "added": [
                "\treturn ns_capable(user_ns, CAP_SETGID) &&",
                "\t\tuserns_may_setgroups(user_ns);"
            ],
            "deleted": [
                "\treturn ns_capable(user_ns, CAP_SETGID);"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Linux kernel through 3.17.4 does not properly restrict dropping of supplemental group memberships in certain namespace scenarios, which allows local users to bypass intended file permissions by leveraging a POSIX ACL containing an entry for the group category that is more restrictive than the entry for the other category, aka a \"negative groups\" issue, related to kernel/groups.c, kernel/uid16.c, and kernel/user_namespace.c.",
        "id": 677
    },
    {
        "cve_id": "CVE-2014-0181",
        "code_before_change": "static int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}",
        "code_after_change": "static int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -22,7 +22,7 @@\n \t\treturn -EOPNOTSUPP;\n \n \tif ((ops->flags & GENL_ADMIN_PERM) &&\n-\t    !capable(CAP_NET_ADMIN))\n+\t    !netlink_capable(skb, CAP_NET_ADMIN))\n \t\treturn -EPERM;\n \n \tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {",
        "function_modified_lines": {
            "added": [
                "\t    !netlink_capable(skb, CAP_NET_ADMIN))"
            ],
            "deleted": [
                "\t    !capable(CAP_NET_ADMIN))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Netlink implementation in the Linux kernel through 3.14.1 does not provide a mechanism for authorizing socket operations based on the opener of a socket, which allows local users to bypass intended access restrictions and modify network configurations by using a Netlink socket for the (1) stdout or (2) stderr of a setuid program.",
        "id": 450
    },
    {
        "cve_id": "CVE-2014-0181",
        "code_before_change": "static int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}",
        "code_after_change": "static int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,7 @@\n \tstruct Qdisc *p = NULL;\n \tint err;\n \n-\tif ((n->nlmsg_type != RTM_GETQDISC) && !capable(CAP_NET_ADMIN))\n+\tif ((n->nlmsg_type != RTM_GETQDISC) && !netlink_capable(skb, CAP_NET_ADMIN))\n \t\treturn -EPERM;\n \n \terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);",
        "function_modified_lines": {
            "added": [
                "\tif ((n->nlmsg_type != RTM_GETQDISC) && !netlink_capable(skb, CAP_NET_ADMIN))"
            ],
            "deleted": [
                "\tif ((n->nlmsg_type != RTM_GETQDISC) && !capable(CAP_NET_ADMIN))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Netlink implementation in the Linux kernel through 3.14.1 does not provide a mechanism for authorizing socket operations based on the opener of a socket, which allows local users to bypass intended access restrictions and modify network configurations by using a Netlink socket for the (1) stdout or (2) stderr of a setuid program.",
        "id": 457
    },
    {
        "cve_id": "CVE-2016-4440",
        "code_before_change": "static __init int hardware_setup(void)\n{\n\tint r = -ENOMEM, i, msr;\n\n\trdmsrl_safe(MSR_EFER, &host_efer);\n\n\tfor (i = 0; i < ARRAY_SIZE(vmx_msr_index); ++i)\n\t\tkvm_define_shared_msr(i, vmx_msr_index[i]);\n\n\tvmx_io_bitmap_a = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_a)\n\t\treturn r;\n\n\tvmx_io_bitmap_b = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_b)\n\t\tgoto out;\n\n\tvmx_msr_bitmap_legacy = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy)\n\t\tgoto out1;\n\n\tvmx_msr_bitmap_legacy_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy_x2apic)\n\t\tgoto out2;\n\n\tvmx_msr_bitmap_longmode = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode)\n\t\tgoto out3;\n\n\tvmx_msr_bitmap_longmode_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode_x2apic)\n\t\tgoto out4;\n\n\tif (nested) {\n\t\tvmx_msr_bitmap_nested =\n\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\t\tif (!vmx_msr_bitmap_nested)\n\t\t\tgoto out5;\n\t}\n\n\tvmx_vmread_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmread_bitmap)\n\t\tgoto out6;\n\n\tvmx_vmwrite_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmwrite_bitmap)\n\t\tgoto out7;\n\n\tmemset(vmx_vmread_bitmap, 0xff, PAGE_SIZE);\n\tmemset(vmx_vmwrite_bitmap, 0xff, PAGE_SIZE);\n\n\t/*\n\t * Allow direct access to the PC debug port (it is often used for I/O\n\t * delays, but the vmexits simply slow things down).\n\t */\n\tmemset(vmx_io_bitmap_a, 0xff, PAGE_SIZE);\n\tclear_bit(0x80, vmx_io_bitmap_a);\n\n\tmemset(vmx_io_bitmap_b, 0xff, PAGE_SIZE);\n\n\tmemset(vmx_msr_bitmap_legacy, 0xff, PAGE_SIZE);\n\tmemset(vmx_msr_bitmap_longmode, 0xff, PAGE_SIZE);\n\tif (nested)\n\t\tmemset(vmx_msr_bitmap_nested, 0xff, PAGE_SIZE);\n\n\tif (setup_vmcs_config(&vmcs_config) < 0) {\n\t\tr = -EIO;\n\t\tgoto out8;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_NX))\n\t\tkvm_enable_efer_bits(EFER_NX);\n\n\tif (!cpu_has_vmx_vpid())\n\t\tenable_vpid = 0;\n\tif (!cpu_has_vmx_shadow_vmcs())\n\t\tenable_shadow_vmcs = 0;\n\tif (enable_shadow_vmcs)\n\t\tinit_vmcs_shadow_fields();\n\n\tif (!cpu_has_vmx_ept() ||\n\t    !cpu_has_vmx_ept_4levels()) {\n\t\tenable_ept = 0;\n\t\tenable_unrestricted_guest = 0;\n\t\tenable_ept_ad_bits = 0;\n\t}\n\n\tif (!cpu_has_vmx_ept_ad_bits())\n\t\tenable_ept_ad_bits = 0;\n\n\tif (!cpu_has_vmx_unrestricted_guest())\n\t\tenable_unrestricted_guest = 0;\n\n\tif (!cpu_has_vmx_flexpriority())\n\t\tflexpriority_enabled = 0;\n\n\t/*\n\t * set_apic_access_page_addr() is used to reload apic access\n\t * page upon invalidation.  No need to do anything if not\n\t * using the APIC_ACCESS_ADDR VMCS field.\n\t */\n\tif (!flexpriority_enabled)\n\t\tkvm_x86_ops->set_apic_access_page_addr = NULL;\n\n\tif (!cpu_has_vmx_tpr_shadow())\n\t\tkvm_x86_ops->update_cr8_intercept = NULL;\n\n\tif (enable_ept && !cpu_has_vmx_ept_2m_page())\n\t\tkvm_disable_largepages();\n\n\tif (!cpu_has_vmx_ple())\n\t\tple_gap = 0;\n\n\tif (!cpu_has_vmx_apicv())\n\t\tenable_apicv = 0;\n\n\tif (cpu_has_vmx_tsc_scaling()) {\n\t\tkvm_has_tsc_control = true;\n\t\tkvm_max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;\n\t\tkvm_tsc_scaling_ratio_frac_bits = 48;\n\t}\n\n\tvmx_disable_intercept_for_msr(MSR_FS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_GS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_KERNEL_GS_BASE, true);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_CS, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_ESP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_EIP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_BNDCFGS, true);\n\n\tmemcpy(vmx_msr_bitmap_legacy_x2apic,\n\t\t\tvmx_msr_bitmap_legacy, PAGE_SIZE);\n\tmemcpy(vmx_msr_bitmap_longmode_x2apic,\n\t\t\tvmx_msr_bitmap_longmode, PAGE_SIZE);\n\n\tset_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */\n\n\tif (enable_apicv) {\n\t\tfor (msr = 0x800; msr <= 0x8ff; msr++)\n\t\t\tvmx_disable_intercept_msr_read_x2apic(msr);\n\n\t\t/* According SDM, in x2apic mode, the whole id reg is used.\n\t\t * But in KVM, it only use the highest eight bits. Need to\n\t\t * intercept it */\n\t\tvmx_enable_intercept_msr_read_x2apic(0x802);\n\t\t/* TMCCT */\n\t\tvmx_enable_intercept_msr_read_x2apic(0x839);\n\t\t/* TPR */\n\t\tvmx_disable_intercept_msr_write_x2apic(0x808);\n\t\t/* EOI */\n\t\tvmx_disable_intercept_msr_write_x2apic(0x80b);\n\t\t/* SELF-IPI */\n\t\tvmx_disable_intercept_msr_write_x2apic(0x83f);\n\t}\n\n\tif (enable_ept) {\n\t\tkvm_mmu_set_mask_ptes(0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_ACCESS_BIT : 0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_DIRTY_BIT : 0ull,\n\t\t\t0ull, VMX_EPT_EXECUTABLE_MASK);\n\t\tept_set_mmio_spte_mask();\n\t\tkvm_enable_tdp();\n\t} else\n\t\tkvm_disable_tdp();\n\n\tupdate_ple_window_actual_max();\n\n\t/*\n\t * Only enable PML when hardware supports PML feature, and both EPT\n\t * and EPT A/D bit features are enabled -- PML depends on them to work.\n\t */\n\tif (!enable_ept || !enable_ept_ad_bits || !cpu_has_vmx_pml())\n\t\tenable_pml = 0;\n\n\tif (!enable_pml) {\n\t\tkvm_x86_ops->slot_enable_log_dirty = NULL;\n\t\tkvm_x86_ops->slot_disable_log_dirty = NULL;\n\t\tkvm_x86_ops->flush_log_dirty = NULL;\n\t\tkvm_x86_ops->enable_log_dirty_pt_masked = NULL;\n\t}\n\n\tkvm_set_posted_intr_wakeup_handler(wakeup_handler);\n\n\treturn alloc_kvm_area();\n\nout8:\n\tfree_page((unsigned long)vmx_vmwrite_bitmap);\nout7:\n\tfree_page((unsigned long)vmx_vmread_bitmap);\nout6:\n\tif (nested)\n\t\tfree_page((unsigned long)vmx_msr_bitmap_nested);\nout5:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode_x2apic);\nout4:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode);\nout3:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy_x2apic);\nout2:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy);\nout1:\n\tfree_page((unsigned long)vmx_io_bitmap_b);\nout:\n\tfree_page((unsigned long)vmx_io_bitmap_a);\n\n    return r;\n}",
        "code_after_change": "static __init int hardware_setup(void)\n{\n\tint r = -ENOMEM, i, msr;\n\n\trdmsrl_safe(MSR_EFER, &host_efer);\n\n\tfor (i = 0; i < ARRAY_SIZE(vmx_msr_index); ++i)\n\t\tkvm_define_shared_msr(i, vmx_msr_index[i]);\n\n\tvmx_io_bitmap_a = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_a)\n\t\treturn r;\n\n\tvmx_io_bitmap_b = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_b)\n\t\tgoto out;\n\n\tvmx_msr_bitmap_legacy = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy)\n\t\tgoto out1;\n\n\tvmx_msr_bitmap_legacy_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy_x2apic)\n\t\tgoto out2;\n\n\tvmx_msr_bitmap_longmode = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode)\n\t\tgoto out3;\n\n\tvmx_msr_bitmap_longmode_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode_x2apic)\n\t\tgoto out4;\n\n\tif (nested) {\n\t\tvmx_msr_bitmap_nested =\n\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\t\tif (!vmx_msr_bitmap_nested)\n\t\t\tgoto out5;\n\t}\n\n\tvmx_vmread_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmread_bitmap)\n\t\tgoto out6;\n\n\tvmx_vmwrite_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmwrite_bitmap)\n\t\tgoto out7;\n\n\tmemset(vmx_vmread_bitmap, 0xff, PAGE_SIZE);\n\tmemset(vmx_vmwrite_bitmap, 0xff, PAGE_SIZE);\n\n\t/*\n\t * Allow direct access to the PC debug port (it is often used for I/O\n\t * delays, but the vmexits simply slow things down).\n\t */\n\tmemset(vmx_io_bitmap_a, 0xff, PAGE_SIZE);\n\tclear_bit(0x80, vmx_io_bitmap_a);\n\n\tmemset(vmx_io_bitmap_b, 0xff, PAGE_SIZE);\n\n\tmemset(vmx_msr_bitmap_legacy, 0xff, PAGE_SIZE);\n\tmemset(vmx_msr_bitmap_longmode, 0xff, PAGE_SIZE);\n\tif (nested)\n\t\tmemset(vmx_msr_bitmap_nested, 0xff, PAGE_SIZE);\n\n\tif (setup_vmcs_config(&vmcs_config) < 0) {\n\t\tr = -EIO;\n\t\tgoto out8;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_NX))\n\t\tkvm_enable_efer_bits(EFER_NX);\n\n\tif (!cpu_has_vmx_vpid())\n\t\tenable_vpid = 0;\n\tif (!cpu_has_vmx_shadow_vmcs())\n\t\tenable_shadow_vmcs = 0;\n\tif (enable_shadow_vmcs)\n\t\tinit_vmcs_shadow_fields();\n\n\tif (!cpu_has_vmx_ept() ||\n\t    !cpu_has_vmx_ept_4levels()) {\n\t\tenable_ept = 0;\n\t\tenable_unrestricted_guest = 0;\n\t\tenable_ept_ad_bits = 0;\n\t}\n\n\tif (!cpu_has_vmx_ept_ad_bits())\n\t\tenable_ept_ad_bits = 0;\n\n\tif (!cpu_has_vmx_unrestricted_guest())\n\t\tenable_unrestricted_guest = 0;\n\n\tif (!cpu_has_vmx_flexpriority())\n\t\tflexpriority_enabled = 0;\n\n\t/*\n\t * set_apic_access_page_addr() is used to reload apic access\n\t * page upon invalidation.  No need to do anything if not\n\t * using the APIC_ACCESS_ADDR VMCS field.\n\t */\n\tif (!flexpriority_enabled)\n\t\tkvm_x86_ops->set_apic_access_page_addr = NULL;\n\n\tif (!cpu_has_vmx_tpr_shadow())\n\t\tkvm_x86_ops->update_cr8_intercept = NULL;\n\n\tif (enable_ept && !cpu_has_vmx_ept_2m_page())\n\t\tkvm_disable_largepages();\n\n\tif (!cpu_has_vmx_ple())\n\t\tple_gap = 0;\n\n\tif (!cpu_has_vmx_apicv())\n\t\tenable_apicv = 0;\n\n\tif (cpu_has_vmx_tsc_scaling()) {\n\t\tkvm_has_tsc_control = true;\n\t\tkvm_max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;\n\t\tkvm_tsc_scaling_ratio_frac_bits = 48;\n\t}\n\n\tvmx_disable_intercept_for_msr(MSR_FS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_GS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_KERNEL_GS_BASE, true);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_CS, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_ESP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_EIP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_BNDCFGS, true);\n\n\tmemcpy(vmx_msr_bitmap_legacy_x2apic,\n\t\t\tvmx_msr_bitmap_legacy, PAGE_SIZE);\n\tmemcpy(vmx_msr_bitmap_longmode_x2apic,\n\t\t\tvmx_msr_bitmap_longmode, PAGE_SIZE);\n\n\tset_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */\n\n\tfor (msr = 0x800; msr <= 0x8ff; msr++)\n\t\tvmx_disable_intercept_msr_read_x2apic(msr);\n\n\t/* According SDM, in x2apic mode, the whole id reg is used.  But in\n\t * KVM, it only use the highest eight bits. Need to intercept it */\n\tvmx_enable_intercept_msr_read_x2apic(0x802);\n\t/* TMCCT */\n\tvmx_enable_intercept_msr_read_x2apic(0x839);\n\t/* TPR */\n\tvmx_disable_intercept_msr_write_x2apic(0x808);\n\t/* EOI */\n\tvmx_disable_intercept_msr_write_x2apic(0x80b);\n\t/* SELF-IPI */\n\tvmx_disable_intercept_msr_write_x2apic(0x83f);\n\n\tif (enable_ept) {\n\t\tkvm_mmu_set_mask_ptes(0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_ACCESS_BIT : 0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_DIRTY_BIT : 0ull,\n\t\t\t0ull, VMX_EPT_EXECUTABLE_MASK);\n\t\tept_set_mmio_spte_mask();\n\t\tkvm_enable_tdp();\n\t} else\n\t\tkvm_disable_tdp();\n\n\tupdate_ple_window_actual_max();\n\n\t/*\n\t * Only enable PML when hardware supports PML feature, and both EPT\n\t * and EPT A/D bit features are enabled -- PML depends on them to work.\n\t */\n\tif (!enable_ept || !enable_ept_ad_bits || !cpu_has_vmx_pml())\n\t\tenable_pml = 0;\n\n\tif (!enable_pml) {\n\t\tkvm_x86_ops->slot_enable_log_dirty = NULL;\n\t\tkvm_x86_ops->slot_disable_log_dirty = NULL;\n\t\tkvm_x86_ops->flush_log_dirty = NULL;\n\t\tkvm_x86_ops->enable_log_dirty_pt_masked = NULL;\n\t}\n\n\tkvm_set_posted_intr_wakeup_handler(wakeup_handler);\n\n\treturn alloc_kvm_area();\n\nout8:\n\tfree_page((unsigned long)vmx_vmwrite_bitmap);\nout7:\n\tfree_page((unsigned long)vmx_vmread_bitmap);\nout6:\n\tif (nested)\n\t\tfree_page((unsigned long)vmx_msr_bitmap_nested);\nout5:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode_x2apic);\nout4:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode);\nout3:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy_x2apic);\nout2:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy);\nout1:\n\tfree_page((unsigned long)vmx_io_bitmap_b);\nout:\n\tfree_page((unsigned long)vmx_io_bitmap_a);\n\n    return r;\n}",
        "patch": "--- code before\n+++ code after\n@@ -137,23 +137,20 @@\n \n \tset_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */\n \n-\tif (enable_apicv) {\n-\t\tfor (msr = 0x800; msr <= 0x8ff; msr++)\n-\t\t\tvmx_disable_intercept_msr_read_x2apic(msr);\n-\n-\t\t/* According SDM, in x2apic mode, the whole id reg is used.\n-\t\t * But in KVM, it only use the highest eight bits. Need to\n-\t\t * intercept it */\n-\t\tvmx_enable_intercept_msr_read_x2apic(0x802);\n-\t\t/* TMCCT */\n-\t\tvmx_enable_intercept_msr_read_x2apic(0x839);\n-\t\t/* TPR */\n-\t\tvmx_disable_intercept_msr_write_x2apic(0x808);\n-\t\t/* EOI */\n-\t\tvmx_disable_intercept_msr_write_x2apic(0x80b);\n-\t\t/* SELF-IPI */\n-\t\tvmx_disable_intercept_msr_write_x2apic(0x83f);\n-\t}\n+\tfor (msr = 0x800; msr <= 0x8ff; msr++)\n+\t\tvmx_disable_intercept_msr_read_x2apic(msr);\n+\n+\t/* According SDM, in x2apic mode, the whole id reg is used.  But in\n+\t * KVM, it only use the highest eight bits. Need to intercept it */\n+\tvmx_enable_intercept_msr_read_x2apic(0x802);\n+\t/* TMCCT */\n+\tvmx_enable_intercept_msr_read_x2apic(0x839);\n+\t/* TPR */\n+\tvmx_disable_intercept_msr_write_x2apic(0x808);\n+\t/* EOI */\n+\tvmx_disable_intercept_msr_write_x2apic(0x80b);\n+\t/* SELF-IPI */\n+\tvmx_disable_intercept_msr_write_x2apic(0x83f);\n \n \tif (enable_ept) {\n \t\tkvm_mmu_set_mask_ptes(0ull,",
        "function_modified_lines": {
            "added": [
                "\tfor (msr = 0x800; msr <= 0x8ff; msr++)",
                "\t\tvmx_disable_intercept_msr_read_x2apic(msr);",
                "",
                "\t/* According SDM, in x2apic mode, the whole id reg is used.  But in",
                "\t * KVM, it only use the highest eight bits. Need to intercept it */",
                "\tvmx_enable_intercept_msr_read_x2apic(0x802);",
                "\t/* TMCCT */",
                "\tvmx_enable_intercept_msr_read_x2apic(0x839);",
                "\t/* TPR */",
                "\tvmx_disable_intercept_msr_write_x2apic(0x808);",
                "\t/* EOI */",
                "\tvmx_disable_intercept_msr_write_x2apic(0x80b);",
                "\t/* SELF-IPI */",
                "\tvmx_disable_intercept_msr_write_x2apic(0x83f);"
            ],
            "deleted": [
                "\tif (enable_apicv) {",
                "\t\tfor (msr = 0x800; msr <= 0x8ff; msr++)",
                "\t\t\tvmx_disable_intercept_msr_read_x2apic(msr);",
                "",
                "\t\t/* According SDM, in x2apic mode, the whole id reg is used.",
                "\t\t * But in KVM, it only use the highest eight bits. Need to",
                "\t\t * intercept it */",
                "\t\tvmx_enable_intercept_msr_read_x2apic(0x802);",
                "\t\t/* TMCCT */",
                "\t\tvmx_enable_intercept_msr_read_x2apic(0x839);",
                "\t\t/* TPR */",
                "\t\tvmx_disable_intercept_msr_write_x2apic(0x808);",
                "\t\t/* EOI */",
                "\t\tvmx_disable_intercept_msr_write_x2apic(0x80b);",
                "\t\t/* SELF-IPI */",
                "\t\tvmx_disable_intercept_msr_write_x2apic(0x83f);",
                "\t}"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "arch/x86/kvm/vmx.c in the Linux kernel through 4.6.3 mishandles the APICv on/off state, which allows guest OS users to obtain direct APIC MSR access on the host OS, and consequently cause a denial of service (host OS crash) or possibly execute arbitrary code on the host OS, via x2APIC mode.",
        "id": 1016
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "int inet6_sk_rebuild_header(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dst_entry *dst;\n\n\tdst = __sk_dst_check(sk, np->dst_cookie);\n\n\tif (!dst) {\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = sk->sk_protocol;\n\t\tfl6.daddr = sk->sk_v6_daddr;\n\t\tfl6.saddr = np->saddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tfl6.fl6_sport = inet->inet_sport;\n\t\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\t\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\t\tif (IS_ERR(dst)) {\n\t\t\tsk->sk_route_caps = 0;\n\t\t\tsk->sk_err_soft = -PTR_ERR(dst);\n\t\t\treturn PTR_ERR(dst);\n\t\t}\n\n\t\t__ip6_dst_store(sk, dst, NULL, NULL);\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "int inet6_sk_rebuild_header(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dst_entry *dst;\n\n\tdst = __sk_dst_check(sk, np->dst_cookie);\n\n\tif (!dst) {\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = sk->sk_protocol;\n\t\tfl6.daddr = sk->sk_v6_daddr;\n\t\tfl6.saddr = np->saddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tfl6.fl6_sport = inet->inet_sport;\n\t\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\t\trcu_read_lock();\n\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt),\n\t\t\t\t\t &final);\n\t\trcu_read_unlock();\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\t\tif (IS_ERR(dst)) {\n\t\t\tsk->sk_route_caps = 0;\n\t\t\tsk->sk_err_soft = -PTR_ERR(dst);\n\t\t\treturn PTR_ERR(dst);\n\t\t}\n\n\t\t__ip6_dst_store(sk, dst, NULL, NULL);\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -21,7 +21,10 @@\n \t\tfl6.fl6_sport = inet->inet_sport;\n \t\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n \n-\t\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n+\t\trcu_read_lock();\n+\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt),\n+\t\t\t\t\t &final);\n+\t\trcu_read_unlock();\n \n \t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n \t\tif (IS_ERR(dst)) {",
        "function_modified_lines": {
            "added": [
                "\t\trcu_read_lock();",
                "\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt),",
                "\t\t\t\t\t &final);",
                "\t\trcu_read_unlock();"
            ],
            "deleted": [
                "\t\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 993
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static struct sock *dccp_v6_request_recv_sock(const struct sock *sk,\n\t\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t\t      struct request_sock *req,\n\t\t\t\t\t      struct dst_entry *dst,\n\t\t\t\t\t      struct request_sock *req_unhash,\n\t\t\t\t\t      bool *own_req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *newnp;\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct inet_sock *newinet;\n\tstruct dccp6_sock *newdp6;\n\tstruct sock *newsk;\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\t\tnewsk = dccp_v4_request_recv_sock(sk, skb, req, dst,\n\t\t\t\t\t\t  req_unhash, own_req);\n\t\tif (newsk == NULL)\n\t\t\treturn NULL;\n\n\t\tnewdp6 = (struct dccp6_sock *)newsk;\n\t\tnewinet = inet_sk(newsk);\n\t\tnewinet->pinet6 = &newdp6->inet6;\n\t\tnewnp = inet6_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tnewnp->saddr = newsk->sk_v6_rcv_saddr;\n\n\t\tinet_csk(newsk)->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = inet6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, dccp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\tdccp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (!dst) {\n\t\tstruct flowi6 fl6;\n\n\t\tdst = inet6_csk_route_req(sk, &fl6, req, IPPROTO_DCCP);\n\t\tif (!dst)\n\t\t\tgoto out;\n\t}\n\n\tnewsk = dccp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, dccp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\tnewsk->sk_route_caps = dst->dev->features & ~(NETIF_F_IP_CSUM |\n\t\t\t\t\t\t      NETIF_F_TSO);\n\tnewdp6 = (struct dccp6_sock *)newsk;\n\tnewinet = inet_sk(newsk);\n\tnewinet->pinet6 = &newdp6->inet6;\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tnewsk->sk_v6_daddr\t= ireq->ir_v6_rmt_addr;\n\tnewnp->saddr\t\t= ireq->ir_v6_loc_addr;\n\tnewsk->sk_v6_rcv_saddr\t= ireq->ir_v6_loc_addr;\n\tnewsk->sk_bound_dev_if\t= ireq->ir_iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->inet_opt = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\tnewnp->pktoptions = NULL;\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = inet6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\n\t/*\n\t * Clone native IPv6 options from listening socket (if any)\n\t *\n\t * Yes, keeping reference count would be much more clever, but we make\n\t * one more one thing there: reattach optmem to newsk.\n\t */\n\tif (np->opt != NULL)\n\t\tnewnp->opt = ipv6_dup_options(newsk, np->opt);\n\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (newnp->opt != NULL)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n\t\t\t\t\t\t     newnp->opt->opt_flen);\n\n\tdccp_sync_mss(newsk, dst_mtu(dst));\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tinet_csk_prepare_forced_close(newsk);\n\t\tdccp_done(newsk);\n\t\tgoto out;\n\t}\n\t*own_req = inet_ehash_nolisten(newsk, req_to_sk(req_unhash));\n\t/* Clone pktoptions received with SYN, if we own the req */\n\tif (*own_req && ireq->pktopts) {\n\t\tnewnp->pktoptions = skb_clone(ireq->pktopts, GFP_ATOMIC);\n\t\tconsume_skb(ireq->pktopts);\n\t\tireq->pktopts = NULL;\n\t\tif (newnp->pktoptions)\n\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t}\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}",
        "code_after_change": "static struct sock *dccp_v6_request_recv_sock(const struct sock *sk,\n\t\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t\t      struct request_sock *req,\n\t\t\t\t\t      struct dst_entry *dst,\n\t\t\t\t\t      struct request_sock *req_unhash,\n\t\t\t\t\t      bool *own_req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *newnp;\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt;\n\tstruct inet_sock *newinet;\n\tstruct dccp6_sock *newdp6;\n\tstruct sock *newsk;\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\t\tnewsk = dccp_v4_request_recv_sock(sk, skb, req, dst,\n\t\t\t\t\t\t  req_unhash, own_req);\n\t\tif (newsk == NULL)\n\t\t\treturn NULL;\n\n\t\tnewdp6 = (struct dccp6_sock *)newsk;\n\t\tnewinet = inet_sk(newsk);\n\t\tnewinet->pinet6 = &newdp6->inet6;\n\t\tnewnp = inet6_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tnewnp->saddr = newsk->sk_v6_rcv_saddr;\n\n\t\tinet_csk(newsk)->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = inet6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, dccp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\tdccp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (!dst) {\n\t\tstruct flowi6 fl6;\n\n\t\tdst = inet6_csk_route_req(sk, &fl6, req, IPPROTO_DCCP);\n\t\tif (!dst)\n\t\t\tgoto out;\n\t}\n\n\tnewsk = dccp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, dccp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\tnewsk->sk_route_caps = dst->dev->features & ~(NETIF_F_IP_CSUM |\n\t\t\t\t\t\t      NETIF_F_TSO);\n\tnewdp6 = (struct dccp6_sock *)newsk;\n\tnewinet = inet_sk(newsk);\n\tnewinet->pinet6 = &newdp6->inet6;\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tnewsk->sk_v6_daddr\t= ireq->ir_v6_rmt_addr;\n\tnewnp->saddr\t\t= ireq->ir_v6_loc_addr;\n\tnewsk->sk_v6_rcv_saddr\t= ireq->ir_v6_loc_addr;\n\tnewsk->sk_bound_dev_if\t= ireq->ir_iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->inet_opt = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\tnewnp->pktoptions = NULL;\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = inet6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\n\t/*\n\t * Clone native IPv6 options from listening socket (if any)\n\t *\n\t * Yes, keeping reference count would be much more clever, but we make\n\t * one more one thing there: reattach optmem to newsk.\n\t */\n\topt = rcu_dereference(np->opt);\n\tif (opt) {\n\t\topt = ipv6_dup_options(newsk, opt);\n\t\tRCU_INIT_POINTER(newnp->opt, opt);\n\t}\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = opt->opt_nflen +\n\t\t\t\t\t\t    opt->opt_flen;\n\n\tdccp_sync_mss(newsk, dst_mtu(dst));\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tinet_csk_prepare_forced_close(newsk);\n\t\tdccp_done(newsk);\n\t\tgoto out;\n\t}\n\t*own_req = inet_ehash_nolisten(newsk, req_to_sk(req_unhash));\n\t/* Clone pktoptions received with SYN, if we own the req */\n\tif (*own_req && ireq->pktopts) {\n\t\tnewnp->pktoptions = skb_clone(ireq->pktopts, GFP_ATOMIC);\n\t\tconsume_skb(ireq->pktopts);\n\t\tireq->pktopts = NULL;\n\t\tif (newnp->pktoptions)\n\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t}\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,6 +8,7 @@\n \tstruct inet_request_sock *ireq = inet_rsk(req);\n \tstruct ipv6_pinfo *newnp;\n \tconst struct ipv6_pinfo *np = inet6_sk(sk);\n+\tstruct ipv6_txoptions *opt;\n \tstruct inet_sock *newinet;\n \tstruct dccp6_sock *newdp6;\n \tstruct sock *newsk;\n@@ -109,13 +110,15 @@\n \t * Yes, keeping reference count would be much more clever, but we make\n \t * one more one thing there: reattach optmem to newsk.\n \t */\n-\tif (np->opt != NULL)\n-\t\tnewnp->opt = ipv6_dup_options(newsk, np->opt);\n-\n+\topt = rcu_dereference(np->opt);\n+\tif (opt) {\n+\t\topt = ipv6_dup_options(newsk, opt);\n+\t\tRCU_INIT_POINTER(newnp->opt, opt);\n+\t}\n \tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n-\tif (newnp->opt != NULL)\n-\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n-\t\t\t\t\t\t     newnp->opt->opt_flen);\n+\tif (opt)\n+\t\tinet_csk(newsk)->icsk_ext_hdr_len = opt->opt_nflen +\n+\t\t\t\t\t\t    opt->opt_flen;\n \n \tdccp_sync_mss(newsk, dst_mtu(dst));\n ",
        "function_modified_lines": {
            "added": [
                "\tstruct ipv6_txoptions *opt;",
                "\topt = rcu_dereference(np->opt);",
                "\tif (opt) {",
                "\t\topt = ipv6_dup_options(newsk, opt);",
                "\t\tRCU_INIT_POINTER(newnp->opt, opt);",
                "\t}",
                "\tif (opt)",
                "\t\tinet_csk(newsk)->icsk_ext_hdr_len = opt->opt_nflen +",
                "\t\t\t\t\t\t    opt->opt_flen;"
            ],
            "deleted": [
                "\tif (np->opt != NULL)",
                "\t\tnewnp->opt = ipv6_dup_options(newsk, np->opt);",
                "",
                "\tif (newnp->opt != NULL)",
                "\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +",
                "\t\t\t\t\t\t     newnp->opt->opt_flen);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 989
    },
    {
        "cve_id": "CVE-2016-9120",
        "code_before_change": "static struct ion_handle *ion_handle_get_by_id(struct ion_client *client,\n\t\t\t\t\t\tint id)\n{\n\tstruct ion_handle *handle;\n\n\tmutex_lock(&client->lock);\n\thandle = idr_find(&client->idr, id);\n\tif (handle)\n\t\tion_handle_get(handle);\n\tmutex_unlock(&client->lock);\n\n\treturn handle ? handle : ERR_PTR(-EINVAL);\n}",
        "code_after_change": "struct ion_handle *ion_handle_get_by_id(struct ion_client *client,\n\t\t\t\t\t\tint id)\n{\n\tstruct ion_handle *handle;\n\n\tmutex_lock(&client->lock);\n\thandle = ion_handle_get_by_id_nolock(client, id);\n\tmutex_unlock(&client->lock);\n\n\treturn handle;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,13 +1,11 @@\n-static struct ion_handle *ion_handle_get_by_id(struct ion_client *client,\n+struct ion_handle *ion_handle_get_by_id(struct ion_client *client,\n \t\t\t\t\t\tint id)\n {\n \tstruct ion_handle *handle;\n \n \tmutex_lock(&client->lock);\n-\thandle = idr_find(&client->idr, id);\n-\tif (handle)\n-\t\tion_handle_get(handle);\n+\thandle = ion_handle_get_by_id_nolock(client, id);\n \tmutex_unlock(&client->lock);\n \n-\treturn handle ? handle : ERR_PTR(-EINVAL);\n+\treturn handle;\n }",
        "function_modified_lines": {
            "added": [
                "struct ion_handle *ion_handle_get_by_id(struct ion_client *client,",
                "\thandle = ion_handle_get_by_id_nolock(client, id);",
                "\treturn handle;"
            ],
            "deleted": [
                "static struct ion_handle *ion_handle_get_by_id(struct ion_client *client,",
                "\thandle = idr_find(&client->idr, id);",
                "\tif (handle)",
                "\t\tion_handle_get(handle);",
                "\treturn handle ? handle : ERR_PTR(-EINVAL);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "Race condition in the ion_ioctl function in drivers/staging/android/ion/ion.c in the Linux kernel before 4.6 allows local users to gain privileges or cause a denial of service (use-after-free) by calling ION_IOC_FREE on two CPUs at the same time.",
        "id": 1142
    },
    {
        "cve_id": "CVE-2015-9016",
        "code_before_change": "static void flush_end_io(struct request *flush_rq, int error)\n{\n\tstruct request_queue *q = flush_rq->q;\n\tstruct list_head *running;\n\tbool queued = false;\n\tstruct request *rq, *n;\n\tunsigned long flags = 0;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);\n\n\tif (q->mq_ops) {\n\t\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n\t\tflush_rq->tag = -1;\n\t}\n\n\trunning = &fq->flush_queue[fq->flush_running_idx];\n\tBUG_ON(fq->flush_pending_idx == fq->flush_running_idx);\n\n\t/* account completion of the flush request */\n\tfq->flush_running_idx ^= 1;\n\n\tif (!q->mq_ops)\n\t\telv_completed_request(q, flush_rq);\n\n\t/* and push the waiting requests to the next stage */\n\tlist_for_each_entry_safe(rq, n, running, flush.list) {\n\t\tunsigned int seq = blk_flush_cur_seq(rq);\n\n\t\tBUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);\n\t\tqueued |= blk_flush_complete_seq(rq, fq, seq, error);\n\t}\n\n\t/*\n\t * Kick the queue to avoid stall for two cases:\n\t * 1. Moving a request silently to empty queue_head may stall the\n\t * queue.\n\t * 2. When flush request is running in non-queueable queue, the\n\t * queue is hold. Restart the queue after flush request is finished\n\t * to avoid stall.\n\t * This function is called from request completion path and calling\n\t * directly into request_fn may confuse the driver.  Always use\n\t * kblockd.\n\t */\n\tif (queued || fq->flush_queue_delayed) {\n\t\tWARN_ON(q->mq_ops);\n\t\tblk_run_queue_async(q);\n\t}\n\tfq->flush_queue_delayed = 0;\n\tif (q->mq_ops)\n\t\tspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\n}",
        "code_after_change": "static void flush_end_io(struct request *flush_rq, int error)\n{\n\tstruct request_queue *q = flush_rq->q;\n\tstruct list_head *running;\n\tbool queued = false;\n\tstruct request *rq, *n;\n\tunsigned long flags = 0;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);\n\n\tif (q->mq_ops) {\n\t\tstruct blk_mq_hw_ctx *hctx;\n\n\t\t/* release the tag's ownership to the req cloned from */\n\t\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n\t\thctx = q->mq_ops->map_queue(q, flush_rq->mq_ctx->cpu);\n\t\tblk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);\n\t\tflush_rq->tag = -1;\n\t}\n\n\trunning = &fq->flush_queue[fq->flush_running_idx];\n\tBUG_ON(fq->flush_pending_idx == fq->flush_running_idx);\n\n\t/* account completion of the flush request */\n\tfq->flush_running_idx ^= 1;\n\n\tif (!q->mq_ops)\n\t\telv_completed_request(q, flush_rq);\n\n\t/* and push the waiting requests to the next stage */\n\tlist_for_each_entry_safe(rq, n, running, flush.list) {\n\t\tunsigned int seq = blk_flush_cur_seq(rq);\n\n\t\tBUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);\n\t\tqueued |= blk_flush_complete_seq(rq, fq, seq, error);\n\t}\n\n\t/*\n\t * Kick the queue to avoid stall for two cases:\n\t * 1. Moving a request silently to empty queue_head may stall the\n\t * queue.\n\t * 2. When flush request is running in non-queueable queue, the\n\t * queue is hold. Restart the queue after flush request is finished\n\t * to avoid stall.\n\t * This function is called from request completion path and calling\n\t * directly into request_fn may confuse the driver.  Always use\n\t * kblockd.\n\t */\n\tif (queued || fq->flush_queue_delayed) {\n\t\tWARN_ON(q->mq_ops);\n\t\tblk_run_queue_async(q);\n\t}\n\tfq->flush_queue_delayed = 0;\n\tif (q->mq_ops)\n\t\tspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,7 +8,12 @@\n \tstruct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);\n \n \tif (q->mq_ops) {\n+\t\tstruct blk_mq_hw_ctx *hctx;\n+\n+\t\t/* release the tag's ownership to the req cloned from */\n \t\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n+\t\thctx = q->mq_ops->map_queue(q, flush_rq->mq_ctx->cpu);\n+\t\tblk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);\n \t\tflush_rq->tag = -1;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\tstruct blk_mq_hw_ctx *hctx;",
                "",
                "\t\t/* release the tag's ownership to the req cloned from */",
                "\t\thctx = q->mq_ops->map_queue(q, flush_rq->mq_ctx->cpu);",
                "\t\tblk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264",
            "CWE-362"
        ],
        "cve_description": "In blk_mq_tag_to_rq in blk-mq.c in the upstream kernel, there is a possible use after free due to a race condition when a request has been previously freed by blk_mq_complete_request. This could lead to local escalation of privilege. Product: Android. Versions: Android kernel. Android ID: A-63083046.",
        "id": 881
    },
    {
        "cve_id": "CVE-2014-0181",
        "code_before_change": "static void nfnetlink_rcv(struct sk_buff *skb)\n{\n\tstruct nlmsghdr *nlh = nlmsg_hdr(skb);\n\tstruct net *net = sock_net(skb->sk);\n\tint msglen;\n\n\tif (nlh->nlmsg_len < NLMSG_HDRLEN ||\n\t    skb->len < nlh->nlmsg_len)\n\t\treturn;\n\n\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN)) {\n\t\tnetlink_ack(skb, nlh, -EPERM);\n\t\treturn;\n\t}\n\n\tif (nlh->nlmsg_type == NFNL_MSG_BATCH_BEGIN) {\n\t\tstruct nfgenmsg *nfgenmsg;\n\n\t\tmsglen = NLMSG_ALIGN(nlh->nlmsg_len);\n\t\tif (msglen > skb->len)\n\t\t\tmsglen = skb->len;\n\n\t\tif (nlh->nlmsg_len < NLMSG_HDRLEN ||\n\t\t    skb->len < NLMSG_HDRLEN + sizeof(struct nfgenmsg))\n\t\t\treturn;\n\n\t\tnfgenmsg = nlmsg_data(nlh);\n\t\tskb_pull(skb, msglen);\n\t\tnfnetlink_rcv_batch(skb, nlh, nfgenmsg->res_id);\n\t} else {\n\t\tnetlink_rcv_skb(skb, &nfnetlink_rcv_msg);\n\t}\n}",
        "code_after_change": "static void nfnetlink_rcv(struct sk_buff *skb)\n{\n\tstruct nlmsghdr *nlh = nlmsg_hdr(skb);\n\tstruct net *net = sock_net(skb->sk);\n\tint msglen;\n\n\tif (nlh->nlmsg_len < NLMSG_HDRLEN ||\n\t    skb->len < nlh->nlmsg_len)\n\t\treturn;\n\n\tif (!netlink_net_capable(skb, CAP_NET_ADMIN)) {\n\t\tnetlink_ack(skb, nlh, -EPERM);\n\t\treturn;\n\t}\n\n\tif (nlh->nlmsg_type == NFNL_MSG_BATCH_BEGIN) {\n\t\tstruct nfgenmsg *nfgenmsg;\n\n\t\tmsglen = NLMSG_ALIGN(nlh->nlmsg_len);\n\t\tif (msglen > skb->len)\n\t\t\tmsglen = skb->len;\n\n\t\tif (nlh->nlmsg_len < NLMSG_HDRLEN ||\n\t\t    skb->len < NLMSG_HDRLEN + sizeof(struct nfgenmsg))\n\t\t\treturn;\n\n\t\tnfgenmsg = nlmsg_data(nlh);\n\t\tskb_pull(skb, msglen);\n\t\tnfnetlink_rcv_batch(skb, nlh, nfgenmsg->res_id);\n\t} else {\n\t\tnetlink_rcv_skb(skb, &nfnetlink_rcv_msg);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,7 +8,7 @@\n \t    skb->len < nlh->nlmsg_len)\n \t\treturn;\n \n-\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN)) {\n+\tif (!netlink_net_capable(skb, CAP_NET_ADMIN)) {\n \t\tnetlink_ack(skb, nlh, -EPERM);\n \t\treturn;\n \t}",
        "function_modified_lines": {
            "added": [
                "\tif (!netlink_net_capable(skb, CAP_NET_ADMIN)) {"
            ],
            "deleted": [
                "\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN)) {"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Netlink implementation in the Linux kernel through 3.14.1 does not provide a mechanism for authorizing socket operations based on the opener of a socket, which allows local users to bypass intended access restrictions and modify network configurations by using a Netlink socket for the (1) stdout or (2) stderr of a setuid program.",
        "id": 449
    },
    {
        "cve_id": "CVE-2015-8966",
        "code_before_change": "asmlinkage long sys_oabi_fcntl64(unsigned int fd, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tstruct oabi_flock64 user;\n\tstruct flock64 kernel;\n\tmm_segment_t fs = USER_DS; /* initialized to kill a warning */\n\tunsigned long local_arg = arg;\n\tint ret;\n\n\tswitch (cmd) {\n\tcase F_OFD_GETLK:\n\tcase F_OFD_SETLK:\n\tcase F_OFD_SETLKW:\n\tcase F_GETLK64:\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\tif (copy_from_user(&user, (struct oabi_flock64 __user *)arg,\n\t\t\t\t   sizeof(user)))\n\t\t\treturn -EFAULT;\n\t\tkernel.l_type\t= user.l_type;\n\t\tkernel.l_whence\t= user.l_whence;\n\t\tkernel.l_start\t= user.l_start;\n\t\tkernel.l_len\t= user.l_len;\n\t\tkernel.l_pid\t= user.l_pid;\n\t\tlocal_arg = (unsigned long)&kernel;\n\t\tfs = get_fs();\n\t\tset_fs(KERNEL_DS);\n\t}\n\n\tret = sys_fcntl64(fd, cmd, local_arg);\n\n\tswitch (cmd) {\n\tcase F_GETLK64:\n\t\tif (!ret) {\n\t\t\tuser.l_type\t= kernel.l_type;\n\t\t\tuser.l_whence\t= kernel.l_whence;\n\t\t\tuser.l_start\t= kernel.l_start;\n\t\t\tuser.l_len\t= kernel.l_len;\n\t\t\tuser.l_pid\t= kernel.l_pid;\n\t\t\tif (copy_to_user((struct oabi_flock64 __user *)arg,\n\t\t\t\t\t &user, sizeof(user)))\n\t\t\t\tret = -EFAULT;\n\t\t}\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\tset_fs(fs);\n\t}\n\n\treturn ret;\n}",
        "code_after_change": "asmlinkage long sys_oabi_fcntl64(unsigned int fd, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase F_OFD_GETLK:\n\tcase F_OFD_SETLK:\n\tcase F_OFD_SETLKW:\n\tcase F_GETLK64:\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\treturn do_locks(fd, cmd, arg);\n\n\tdefault:\n\t\treturn sys_fcntl64(fd, cmd, arg);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,12 +1,6 @@\n asmlinkage long sys_oabi_fcntl64(unsigned int fd, unsigned int cmd,\n \t\t\t\t unsigned long arg)\n {\n-\tstruct oabi_flock64 user;\n-\tstruct flock64 kernel;\n-\tmm_segment_t fs = USER_DS; /* initialized to kill a warning */\n-\tunsigned long local_arg = arg;\n-\tint ret;\n-\n \tswitch (cmd) {\n \tcase F_OFD_GETLK:\n \tcase F_OFD_SETLK:\n@@ -14,37 +8,9 @@\n \tcase F_GETLK64:\n \tcase F_SETLK64:\n \tcase F_SETLKW64:\n-\t\tif (copy_from_user(&user, (struct oabi_flock64 __user *)arg,\n-\t\t\t\t   sizeof(user)))\n-\t\t\treturn -EFAULT;\n-\t\tkernel.l_type\t= user.l_type;\n-\t\tkernel.l_whence\t= user.l_whence;\n-\t\tkernel.l_start\t= user.l_start;\n-\t\tkernel.l_len\t= user.l_len;\n-\t\tkernel.l_pid\t= user.l_pid;\n-\t\tlocal_arg = (unsigned long)&kernel;\n-\t\tfs = get_fs();\n-\t\tset_fs(KERNEL_DS);\n+\t\treturn do_locks(fd, cmd, arg);\n+\n+\tdefault:\n+\t\treturn sys_fcntl64(fd, cmd, arg);\n \t}\n-\n-\tret = sys_fcntl64(fd, cmd, local_arg);\n-\n-\tswitch (cmd) {\n-\tcase F_GETLK64:\n-\t\tif (!ret) {\n-\t\t\tuser.l_type\t= kernel.l_type;\n-\t\t\tuser.l_whence\t= kernel.l_whence;\n-\t\t\tuser.l_start\t= kernel.l_start;\n-\t\t\tuser.l_len\t= kernel.l_len;\n-\t\t\tuser.l_pid\t= kernel.l_pid;\n-\t\t\tif (copy_to_user((struct oabi_flock64 __user *)arg,\n-\t\t\t\t\t &user, sizeof(user)))\n-\t\t\t\tret = -EFAULT;\n-\t\t}\n-\tcase F_SETLK64:\n-\tcase F_SETLKW64:\n-\t\tset_fs(fs);\n-\t}\n-\n-\treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\t\treturn do_locks(fd, cmd, arg);",
                "",
                "\tdefault:",
                "\t\treturn sys_fcntl64(fd, cmd, arg);"
            ],
            "deleted": [
                "\tstruct oabi_flock64 user;",
                "\tstruct flock64 kernel;",
                "\tmm_segment_t fs = USER_DS; /* initialized to kill a warning */",
                "\tunsigned long local_arg = arg;",
                "\tint ret;",
                "",
                "\t\tif (copy_from_user(&user, (struct oabi_flock64 __user *)arg,",
                "\t\t\t\t   sizeof(user)))",
                "\t\t\treturn -EFAULT;",
                "\t\tkernel.l_type\t= user.l_type;",
                "\t\tkernel.l_whence\t= user.l_whence;",
                "\t\tkernel.l_start\t= user.l_start;",
                "\t\tkernel.l_len\t= user.l_len;",
                "\t\tkernel.l_pid\t= user.l_pid;",
                "\t\tlocal_arg = (unsigned long)&kernel;",
                "\t\tfs = get_fs();",
                "\t\tset_fs(KERNEL_DS);",
                "",
                "\tret = sys_fcntl64(fd, cmd, local_arg);",
                "",
                "\tswitch (cmd) {",
                "\tcase F_GETLK64:",
                "\t\tif (!ret) {",
                "\t\t\tuser.l_type\t= kernel.l_type;",
                "\t\t\tuser.l_whence\t= kernel.l_whence;",
                "\t\t\tuser.l_start\t= kernel.l_start;",
                "\t\t\tuser.l_len\t= kernel.l_len;",
                "\t\t\tuser.l_pid\t= kernel.l_pid;",
                "\t\t\tif (copy_to_user((struct oabi_flock64 __user *)arg,",
                "\t\t\t\t\t &user, sizeof(user)))",
                "\t\t\t\tret = -EFAULT;",
                "\t\t}",
                "\tcase F_SETLK64:",
                "\tcase F_SETLKW64:",
                "\t\tset_fs(fs);",
                "\t}",
                "",
                "\treturn ret;"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "arch/arm/kernel/sys_oabi-compat.c in the Linux kernel before 4.4 allows local users to gain privileges via a crafted (1) F_OFD_GETLK, (2) F_OFD_SETLK, or (3) F_OFD_SETLKW command in an fcntl64 system call.",
        "id": 875
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "struct dst_entry *inet6_csk_route_req(const struct sock *sk,\n\t\t\t\t      struct flowi6 *fl6,\n\t\t\t\t      const struct request_sock *req,\n\t\t\t\t      u8 proto)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = proto;\n\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\tfinal_p = fl6_update_dst(fl6, np->opt, &final);\n\tfl6->saddr = ireq->ir_v6_loc_addr;\n\tfl6->flowi6_oif = ireq->ir_iif;\n\tfl6->flowi6_mark = ireq->ir_mark;\n\tfl6->fl6_dport = ireq->ir_rmt_port;\n\tfl6->fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\tif (IS_ERR(dst))\n\t\treturn NULL;\n\n\treturn dst;\n}",
        "code_after_change": "struct dst_entry *inet6_csk_route_req(const struct sock *sk,\n\t\t\t\t      struct flowi6 *fl6,\n\t\t\t\t      const struct request_sock *req,\n\t\t\t\t      u8 proto)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = proto;\n\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\tfl6->saddr = ireq->ir_v6_loc_addr;\n\tfl6->flowi6_oif = ireq->ir_iif;\n\tfl6->flowi6_mark = ireq->ir_mark;\n\tfl6->fl6_dport = ireq->ir_rmt_port;\n\tfl6->fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\tif (IS_ERR(dst))\n\t\treturn NULL;\n\n\treturn dst;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,7 +11,9 @@\n \tmemset(fl6, 0, sizeof(*fl6));\n \tfl6->flowi6_proto = proto;\n \tfl6->daddr = ireq->ir_v6_rmt_addr;\n-\tfinal_p = fl6_update_dst(fl6, np->opt, &final);\n+\trcu_read_lock();\n+\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n+\trcu_read_unlock();\n \tfl6->saddr = ireq->ir_v6_loc_addr;\n \tfl6->flowi6_oif = ireq->ir_iif;\n \tfl6->flowi6_mark = ireq->ir_mark;",
        "function_modified_lines": {
            "added": [
                "\trcu_read_lock();",
                "\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);",
                "\trcu_read_unlock();"
            ],
            "deleted": [
                "\tfinal_p = fl6_update_dst(fl6, np->opt, &final);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 998
    },
    {
        "cve_id": "CVE-2013-1774",
        "code_before_change": "static void chase_port(struct edgeport_port *port, unsigned long timeout,\n\t\t\t\t\t\t\t\tint flush)\n{\n\tint baud_rate;\n\tstruct tty_struct *tty = tty_port_tty_get(&port->port->port);\n\tstruct usb_serial *serial = port->port->serial;\n\twait_queue_t wait;\n\tunsigned long flags;\n\n\tif (!timeout)\n\t\ttimeout = (HZ * EDGE_CLOSING_WAIT)/100;\n\n\t/* wait for data to drain from the buffer */\n\tspin_lock_irqsave(&port->ep_lock, flags);\n\tinit_waitqueue_entry(&wait, current);\n\tadd_wait_queue(&tty->write_wait, &wait);\n\tfor (;;) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tif (kfifo_len(&port->write_fifo) == 0\n\t\t|| timeout == 0 || signal_pending(current)\n\t\t|| serial->disconnected)\n\t\t\t/* disconnect */\n\t\t\tbreak;\n\t\tspin_unlock_irqrestore(&port->ep_lock, flags);\n\t\ttimeout = schedule_timeout(timeout);\n\t\tspin_lock_irqsave(&port->ep_lock, flags);\n\t}\n\tset_current_state(TASK_RUNNING);\n\tremove_wait_queue(&tty->write_wait, &wait);\n\tif (flush)\n\t\tkfifo_reset_out(&port->write_fifo);\n\tspin_unlock_irqrestore(&port->ep_lock, flags);\n\ttty_kref_put(tty);\n\n\t/* wait for data to drain from the device */\n\ttimeout += jiffies;\n\twhile ((long)(jiffies - timeout) < 0 && !signal_pending(current)\n\t\t\t\t\t\t&& !serial->disconnected) {\n\t\t/* not disconnected */\n\t\tif (!tx_active(port))\n\t\t\tbreak;\n\t\tmsleep(10);\n\t}\n\n\t/* disconnected */\n\tif (serial->disconnected)\n\t\treturn;\n\n\t/* wait one more character time, based on baud rate */\n\t/* (tx_active doesn't seem to wait for the last byte) */\n\tbaud_rate = port->baud_rate;\n\tif (baud_rate == 0)\n\t\tbaud_rate = 50;\n\tmsleep(max(1, DIV_ROUND_UP(10000, baud_rate)));\n}",
        "code_after_change": "static void chase_port(struct edgeport_port *port, unsigned long timeout,\n\t\t\t\t\t\t\t\tint flush)\n{\n\tint baud_rate;\n\tstruct tty_struct *tty = tty_port_tty_get(&port->port->port);\n\tstruct usb_serial *serial = port->port->serial;\n\twait_queue_t wait;\n\tunsigned long flags;\n\n\tif (!tty)\n\t\treturn;\n\n\tif (!timeout)\n\t\ttimeout = (HZ * EDGE_CLOSING_WAIT)/100;\n\n\t/* wait for data to drain from the buffer */\n\tspin_lock_irqsave(&port->ep_lock, flags);\n\tinit_waitqueue_entry(&wait, current);\n\tadd_wait_queue(&tty->write_wait, &wait);\n\tfor (;;) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tif (kfifo_len(&port->write_fifo) == 0\n\t\t|| timeout == 0 || signal_pending(current)\n\t\t|| serial->disconnected)\n\t\t\t/* disconnect */\n\t\t\tbreak;\n\t\tspin_unlock_irqrestore(&port->ep_lock, flags);\n\t\ttimeout = schedule_timeout(timeout);\n\t\tspin_lock_irqsave(&port->ep_lock, flags);\n\t}\n\tset_current_state(TASK_RUNNING);\n\tremove_wait_queue(&tty->write_wait, &wait);\n\tif (flush)\n\t\tkfifo_reset_out(&port->write_fifo);\n\tspin_unlock_irqrestore(&port->ep_lock, flags);\n\ttty_kref_put(tty);\n\n\t/* wait for data to drain from the device */\n\ttimeout += jiffies;\n\twhile ((long)(jiffies - timeout) < 0 && !signal_pending(current)\n\t\t\t\t\t\t&& !serial->disconnected) {\n\t\t/* not disconnected */\n\t\tif (!tx_active(port))\n\t\t\tbreak;\n\t\tmsleep(10);\n\t}\n\n\t/* disconnected */\n\tif (serial->disconnected)\n\t\treturn;\n\n\t/* wait one more character time, based on baud rate */\n\t/* (tx_active doesn't seem to wait for the last byte) */\n\tbaud_rate = port->baud_rate;\n\tif (baud_rate == 0)\n\t\tbaud_rate = 50;\n\tmsleep(max(1, DIV_ROUND_UP(10000, baud_rate)));\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,6 +6,9 @@\n \tstruct usb_serial *serial = port->port->serial;\n \twait_queue_t wait;\n \tunsigned long flags;\n+\n+\tif (!tty)\n+\t\treturn;\n \n \tif (!timeout)\n \t\ttimeout = (HZ * EDGE_CLOSING_WAIT)/100;",
        "function_modified_lines": {
            "added": [
                "",
                "\tif (!tty)",
                "\t\treturn;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The chase_port function in drivers/usb/serial/io_ti.c in the Linux kernel before 3.7.4 allows local users to cause a denial of service (NULL pointer dereference and system crash) via an attempted /dev/ttyUSB read or write operation on a disconnected Edgeport USB serial converter.",
        "id": 183
    },
    {
        "cve_id": "CVE-2014-4014",
        "code_before_change": "bool inode_owner_or_capable(const struct inode *inode)\n{\n\tif (uid_eq(current_fsuid(), inode->i_uid))\n\t\treturn true;\n\tif (inode_capable(inode, CAP_FOWNER))\n\t\treturn true;\n\treturn false;\n}",
        "code_after_change": "bool inode_owner_or_capable(const struct inode *inode)\n{\n\tstruct user_namespace *ns;\n\n\tif (uid_eq(current_fsuid(), inode->i_uid))\n\t\treturn true;\n\n\tns = current_user_ns();\n\tif (ns_capable(ns, CAP_FOWNER) && kuid_has_mapping(ns, inode->i_uid))\n\t\treturn true;\n\treturn false;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,8 +1,12 @@\n bool inode_owner_or_capable(const struct inode *inode)\n {\n+\tstruct user_namespace *ns;\n+\n \tif (uid_eq(current_fsuid(), inode->i_uid))\n \t\treturn true;\n-\tif (inode_capable(inode, CAP_FOWNER))\n+\n+\tns = current_user_ns();\n+\tif (ns_capable(ns, CAP_FOWNER) && kuid_has_mapping(ns, inode->i_uid))\n \t\treturn true;\n \treturn false;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct user_namespace *ns;",
                "",
                "",
                "\tns = current_user_ns();",
                "\tif (ns_capable(ns, CAP_FOWNER) && kuid_has_mapping(ns, inode->i_uid))"
            ],
            "deleted": [
                "\tif (inode_capable(inode, CAP_FOWNER))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The capabilities implementation in the Linux kernel before 3.14.8 does not properly consider that namespaces are inapplicable to inodes, which allows local users to bypass intended chmod restrictions by first creating a user namespace, as demonstrated by setting the setgid bit on a file with group ownership of root.",
        "id": 552
    },
    {
        "cve_id": "CVE-2014-4014",
        "code_before_change": "int generic_permission(struct inode *inode, int mask)\n{\n\tint ret;\n\n\t/*\n\t * Do the basic permission checks.\n\t */\n\tret = acl_permission_check(inode, mask);\n\tif (ret != -EACCES)\n\t\treturn ret;\n\n\tif (S_ISDIR(inode->i_mode)) {\n\t\t/* DACs are overridable for directories */\n\t\tif (inode_capable(inode, CAP_DAC_OVERRIDE))\n\t\t\treturn 0;\n\t\tif (!(mask & MAY_WRITE))\n\t\t\tif (inode_capable(inode, CAP_DAC_READ_SEARCH))\n\t\t\t\treturn 0;\n\t\treturn -EACCES;\n\t}\n\t/*\n\t * Read/write DACs are always overridable.\n\t * Executable DACs are overridable when there is\n\t * at least one exec bit set.\n\t */\n\tif (!(mask & MAY_EXEC) || (inode->i_mode & S_IXUGO))\n\t\tif (inode_capable(inode, CAP_DAC_OVERRIDE))\n\t\t\treturn 0;\n\n\t/*\n\t * Searching includes executable on directories, else just read.\n\t */\n\tmask &= MAY_READ | MAY_WRITE | MAY_EXEC;\n\tif (mask == MAY_READ)\n\t\tif (inode_capable(inode, CAP_DAC_READ_SEARCH))\n\t\t\treturn 0;\n\n\treturn -EACCES;\n}",
        "code_after_change": "int generic_permission(struct inode *inode, int mask)\n{\n\tint ret;\n\n\t/*\n\t * Do the basic permission checks.\n\t */\n\tret = acl_permission_check(inode, mask);\n\tif (ret != -EACCES)\n\t\treturn ret;\n\n\tif (S_ISDIR(inode->i_mode)) {\n\t\t/* DACs are overridable for directories */\n\t\tif (capable_wrt_inode_uidgid(inode, CAP_DAC_OVERRIDE))\n\t\t\treturn 0;\n\t\tif (!(mask & MAY_WRITE))\n\t\t\tif (capable_wrt_inode_uidgid(inode,\n\t\t\t\t\t\t     CAP_DAC_READ_SEARCH))\n\t\t\t\treturn 0;\n\t\treturn -EACCES;\n\t}\n\t/*\n\t * Read/write DACs are always overridable.\n\t * Executable DACs are overridable when there is\n\t * at least one exec bit set.\n\t */\n\tif (!(mask & MAY_EXEC) || (inode->i_mode & S_IXUGO))\n\t\tif (capable_wrt_inode_uidgid(inode, CAP_DAC_OVERRIDE))\n\t\t\treturn 0;\n\n\t/*\n\t * Searching includes executable on directories, else just read.\n\t */\n\tmask &= MAY_READ | MAY_WRITE | MAY_EXEC;\n\tif (mask == MAY_READ)\n\t\tif (capable_wrt_inode_uidgid(inode, CAP_DAC_READ_SEARCH))\n\t\t\treturn 0;\n\n\treturn -EACCES;\n}",
        "patch": "--- code before\n+++ code after\n@@ -11,10 +11,11 @@\n \n \tif (S_ISDIR(inode->i_mode)) {\n \t\t/* DACs are overridable for directories */\n-\t\tif (inode_capable(inode, CAP_DAC_OVERRIDE))\n+\t\tif (capable_wrt_inode_uidgid(inode, CAP_DAC_OVERRIDE))\n \t\t\treturn 0;\n \t\tif (!(mask & MAY_WRITE))\n-\t\t\tif (inode_capable(inode, CAP_DAC_READ_SEARCH))\n+\t\t\tif (capable_wrt_inode_uidgid(inode,\n+\t\t\t\t\t\t     CAP_DAC_READ_SEARCH))\n \t\t\t\treturn 0;\n \t\treturn -EACCES;\n \t}\n@@ -24,7 +25,7 @@\n \t * at least one exec bit set.\n \t */\n \tif (!(mask & MAY_EXEC) || (inode->i_mode & S_IXUGO))\n-\t\tif (inode_capable(inode, CAP_DAC_OVERRIDE))\n+\t\tif (capable_wrt_inode_uidgid(inode, CAP_DAC_OVERRIDE))\n \t\t\treturn 0;\n \n \t/*\n@@ -32,7 +33,7 @@\n \t */\n \tmask &= MAY_READ | MAY_WRITE | MAY_EXEC;\n \tif (mask == MAY_READ)\n-\t\tif (inode_capable(inode, CAP_DAC_READ_SEARCH))\n+\t\tif (capable_wrt_inode_uidgid(inode, CAP_DAC_READ_SEARCH))\n \t\t\treturn 0;\n \n \treturn -EACCES;",
        "function_modified_lines": {
            "added": [
                "\t\tif (capable_wrt_inode_uidgid(inode, CAP_DAC_OVERRIDE))",
                "\t\t\tif (capable_wrt_inode_uidgid(inode,",
                "\t\t\t\t\t\t     CAP_DAC_READ_SEARCH))",
                "\t\tif (capable_wrt_inode_uidgid(inode, CAP_DAC_OVERRIDE))",
                "\t\tif (capable_wrt_inode_uidgid(inode, CAP_DAC_READ_SEARCH))"
            ],
            "deleted": [
                "\t\tif (inode_capable(inode, CAP_DAC_OVERRIDE))",
                "\t\t\tif (inode_capable(inode, CAP_DAC_READ_SEARCH))",
                "\t\tif (inode_capable(inode, CAP_DAC_OVERRIDE))",
                "\t\tif (inode_capable(inode, CAP_DAC_READ_SEARCH))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The capabilities implementation in the Linux kernel before 3.14.8 does not properly consider that namespaces are inapplicable to inodes, which allows local users to bypass intended chmod restrictions by first creating a user namespace, as demonstrated by setting the setgid bit on a file with group ownership of root.",
        "id": 554
    },
    {
        "cve_id": "CVE-2016-4997",
        "code_before_change": "static inline int check_entry_size_and_hooks(struct arpt_entry *e,\n\t\t\t\t\t     struct xt_table_info *newinfo,\n\t\t\t\t\t     const unsigned char *base,\n\t\t\t\t\t     const unsigned char *limit,\n\t\t\t\t\t     const unsigned int *hook_entries,\n\t\t\t\t\t     const unsigned int *underflows,\n\t\t\t\t\t     unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct arpt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}",
        "code_after_change": "static inline int check_entry_size_and_hooks(struct arpt_entry *e,\n\t\t\t\t\t     struct xt_table_info *newinfo,\n\t\t\t\t\t     const unsigned char *base,\n\t\t\t\t\t     const unsigned char *limit,\n\t\t\t\t\t     const unsigned int *hook_entries,\n\t\t\t\t\t     const unsigned int *underflows,\n\t\t\t\t\t     unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct arpt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t     e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -26,7 +26,8 @@\n \tif (!arp_checkentry(&e->arp))\n \t\treturn -EINVAL;\n \n-\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n+\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n+\t\t\t\t     e->next_offset);\n \tif (err)\n \t\treturn err;\n ",
        "function_modified_lines": {
            "added": [
                "\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,",
                "\t\t\t\t     e->next_offset);"
            ],
            "deleted": [
                "\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The compat IPT_SO_SET_REPLACE and IP6T_SO_SET_REPLACE setsockopt implementations in the netfilter subsystem in the Linux kernel before 4.6.3 allow local users to gain privileges or cause a denial of service (memory corruption) by leveraging in-container root access to provide a crafted offset value that triggers an unintended decrement.",
        "id": 1040
    },
    {
        "cve_id": "CVE-2016-6187",
        "code_before_change": "static int apparmor_setprocattr(struct task_struct *task, char *name,\n\t\t\t\tvoid *value, size_t size)\n{\n\tstruct common_audit_data sa;\n\tstruct apparmor_audit_data aad = {0,};\n\tchar *command, *args = value;\n\tsize_t arg_size;\n\tint error;\n\n\tif (size == 0)\n\t\treturn -EINVAL;\n\t/* args points to a PAGE_SIZE buffer, AppArmor requires that\n\t * the buffer must be null terminated or have size <= PAGE_SIZE -1\n\t * so that AppArmor can null terminate them\n\t */\n\tif (args[size - 1] != '\\0') {\n\t\tif (size == PAGE_SIZE)\n\t\t\treturn -EINVAL;\n\t\targs[size] = '\\0';\n\t}\n\n\t/* task can only write its own attributes */\n\tif (current != task)\n\t\treturn -EACCES;\n\n\targs = value;\n\targs = strim(args);\n\tcommand = strsep(&args, \" \");\n\tif (!args)\n\t\treturn -EINVAL;\n\targs = skip_spaces(args);\n\tif (!*args)\n\t\treturn -EINVAL;\n\n\targ_size = size - (args - (char *) value);\n\tif (strcmp(name, \"current\") == 0) {\n\t\tif (strcmp(command, \"changehat\") == 0) {\n\t\t\terror = aa_setprocattr_changehat(args, arg_size,\n\t\t\t\t\t\t\t !AA_DO_TEST);\n\t\t} else if (strcmp(command, \"permhat\") == 0) {\n\t\t\terror = aa_setprocattr_changehat(args, arg_size,\n\t\t\t\t\t\t\t AA_DO_TEST);\n\t\t} else if (strcmp(command, \"changeprofile\") == 0) {\n\t\t\terror = aa_setprocattr_changeprofile(args, !AA_ONEXEC,\n\t\t\t\t\t\t\t     !AA_DO_TEST);\n\t\t} else if (strcmp(command, \"permprofile\") == 0) {\n\t\t\terror = aa_setprocattr_changeprofile(args, !AA_ONEXEC,\n\t\t\t\t\t\t\t     AA_DO_TEST);\n\t\t} else\n\t\t\tgoto fail;\n\t} else if (strcmp(name, \"exec\") == 0) {\n\t\tif (strcmp(command, \"exec\") == 0)\n\t\t\terror = aa_setprocattr_changeprofile(args, AA_ONEXEC,\n\t\t\t\t\t\t\t     !AA_DO_TEST);\n\t\telse\n\t\t\tgoto fail;\n\t} else\n\t\t/* only support the \"current\" and \"exec\" process attributes */\n\t\treturn -EINVAL;\n\n\tif (!error)\n\t\terror = size;\n\treturn error;\n\nfail:\n\tsa.type = LSM_AUDIT_DATA_NONE;\n\tsa.aad = &aad;\n\taad.profile = aa_current_profile();\n\taad.op = OP_SETPROCATTR;\n\taad.info = name;\n\taad.error = -EINVAL;\n\taa_audit_msg(AUDIT_APPARMOR_DENIED, &sa, NULL);\n\treturn -EINVAL;\n}",
        "code_after_change": "static int apparmor_setprocattr(struct task_struct *task, char *name,\n\t\t\t\tvoid *value, size_t size)\n{\n\tstruct common_audit_data sa;\n\tstruct apparmor_audit_data aad = {0,};\n\tchar *command, *largs = NULL, *args = value;\n\tsize_t arg_size;\n\tint error;\n\n\tif (size == 0)\n\t\treturn -EINVAL;\n\t/* task can only write its own attributes */\n\tif (current != task)\n\t\treturn -EACCES;\n\n\t/* AppArmor requires that the buffer must be null terminated atm */\n\tif (args[size - 1] != '\\0') {\n\t\t/* null terminate */\n\t\tlargs = args = kmalloc(size + 1, GFP_KERNEL);\n\t\tif (!args)\n\t\t\treturn -ENOMEM;\n\t\tmemcpy(args, value, size);\n\t\targs[size] = '\\0';\n\t}\n\n\terror = -EINVAL;\n\targs = strim(args);\n\tcommand = strsep(&args, \" \");\n\tif (!args)\n\t\tgoto out;\n\targs = skip_spaces(args);\n\tif (!*args)\n\t\tgoto out;\n\n\targ_size = size - (args - (char *) value);\n\tif (strcmp(name, \"current\") == 0) {\n\t\tif (strcmp(command, \"changehat\") == 0) {\n\t\t\terror = aa_setprocattr_changehat(args, arg_size,\n\t\t\t\t\t\t\t !AA_DO_TEST);\n\t\t} else if (strcmp(command, \"permhat\") == 0) {\n\t\t\terror = aa_setprocattr_changehat(args, arg_size,\n\t\t\t\t\t\t\t AA_DO_TEST);\n\t\t} else if (strcmp(command, \"changeprofile\") == 0) {\n\t\t\terror = aa_setprocattr_changeprofile(args, !AA_ONEXEC,\n\t\t\t\t\t\t\t     !AA_DO_TEST);\n\t\t} else if (strcmp(command, \"permprofile\") == 0) {\n\t\t\terror = aa_setprocattr_changeprofile(args, !AA_ONEXEC,\n\t\t\t\t\t\t\t     AA_DO_TEST);\n\t\t} else\n\t\t\tgoto fail;\n\t} else if (strcmp(name, \"exec\") == 0) {\n\t\tif (strcmp(command, \"exec\") == 0)\n\t\t\terror = aa_setprocattr_changeprofile(args, AA_ONEXEC,\n\t\t\t\t\t\t\t     !AA_DO_TEST);\n\t\telse\n\t\t\tgoto fail;\n\t} else\n\t\t/* only support the \"current\" and \"exec\" process attributes */\n\t\tgoto fail;\n\n\tif (!error)\n\t\terror = size;\nout:\n\tkfree(largs);\n\treturn error;\n\nfail:\n\tsa.type = LSM_AUDIT_DATA_NONE;\n\tsa.aad = &aad;\n\taad.profile = aa_current_profile();\n\taad.op = OP_SETPROCATTR;\n\taad.info = name;\n\taad.error = error = -EINVAL;\n\taa_audit_msg(AUDIT_APPARMOR_DENIED, &sa, NULL);\n\tgoto out;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,34 +3,34 @@\n {\n \tstruct common_audit_data sa;\n \tstruct apparmor_audit_data aad = {0,};\n-\tchar *command, *args = value;\n+\tchar *command, *largs = NULL, *args = value;\n \tsize_t arg_size;\n \tint error;\n \n \tif (size == 0)\n \t\treturn -EINVAL;\n-\t/* args points to a PAGE_SIZE buffer, AppArmor requires that\n-\t * the buffer must be null terminated or have size <= PAGE_SIZE -1\n-\t * so that AppArmor can null terminate them\n-\t */\n-\tif (args[size - 1] != '\\0') {\n-\t\tif (size == PAGE_SIZE)\n-\t\t\treturn -EINVAL;\n-\t\targs[size] = '\\0';\n-\t}\n-\n \t/* task can only write its own attributes */\n \tif (current != task)\n \t\treturn -EACCES;\n \n-\targs = value;\n+\t/* AppArmor requires that the buffer must be null terminated atm */\n+\tif (args[size - 1] != '\\0') {\n+\t\t/* null terminate */\n+\t\tlargs = args = kmalloc(size + 1, GFP_KERNEL);\n+\t\tif (!args)\n+\t\t\treturn -ENOMEM;\n+\t\tmemcpy(args, value, size);\n+\t\targs[size] = '\\0';\n+\t}\n+\n+\terror = -EINVAL;\n \targs = strim(args);\n \tcommand = strsep(&args, \" \");\n \tif (!args)\n-\t\treturn -EINVAL;\n+\t\tgoto out;\n \targs = skip_spaces(args);\n \tif (!*args)\n-\t\treturn -EINVAL;\n+\t\tgoto out;\n \n \targ_size = size - (args - (char *) value);\n \tif (strcmp(name, \"current\") == 0) {\n@@ -56,10 +56,12 @@\n \t\t\tgoto fail;\n \t} else\n \t\t/* only support the \"current\" and \"exec\" process attributes */\n-\t\treturn -EINVAL;\n+\t\tgoto fail;\n \n \tif (!error)\n \t\terror = size;\n+out:\n+\tkfree(largs);\n \treturn error;\n \n fail:\n@@ -68,7 +70,7 @@\n \taad.profile = aa_current_profile();\n \taad.op = OP_SETPROCATTR;\n \taad.info = name;\n-\taad.error = -EINVAL;\n+\taad.error = error = -EINVAL;\n \taa_audit_msg(AUDIT_APPARMOR_DENIED, &sa, NULL);\n-\treturn -EINVAL;\n+\tgoto out;\n }",
        "function_modified_lines": {
            "added": [
                "\tchar *command, *largs = NULL, *args = value;",
                "\t/* AppArmor requires that the buffer must be null terminated atm */",
                "\tif (args[size - 1] != '\\0') {",
                "\t\t/* null terminate */",
                "\t\tlargs = args = kmalloc(size + 1, GFP_KERNEL);",
                "\t\tif (!args)",
                "\t\t\treturn -ENOMEM;",
                "\t\tmemcpy(args, value, size);",
                "\t\targs[size] = '\\0';",
                "\t}",
                "",
                "\terror = -EINVAL;",
                "\t\tgoto out;",
                "\t\tgoto out;",
                "\t\tgoto fail;",
                "out:",
                "\tkfree(largs);",
                "\taad.error = error = -EINVAL;",
                "\tgoto out;"
            ],
            "deleted": [
                "\tchar *command, *args = value;",
                "\t/* args points to a PAGE_SIZE buffer, AppArmor requires that",
                "\t * the buffer must be null terminated or have size <= PAGE_SIZE -1",
                "\t * so that AppArmor can null terminate them",
                "\t */",
                "\tif (args[size - 1] != '\\0') {",
                "\t\tif (size == PAGE_SIZE)",
                "\t\t\treturn -EINVAL;",
                "\t\targs[size] = '\\0';",
                "\t}",
                "",
                "\targs = value;",
                "\t\treturn -EINVAL;",
                "\t\treturn -EINVAL;",
                "\t\treturn -EINVAL;",
                "\taad.error = -EINVAL;",
                "\treturn -EINVAL;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-119"
        ],
        "cve_description": "The apparmor_setprocattr function in security/apparmor/lsm.c in the Linux kernel before 4.6.5 does not validate the buffer size, which allows local users to gain privileges by triggering an AppArmor setprocattr hook.",
        "id": 1064
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (!optval)\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (get_user(val, (int __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tstruct ipv6_txoptions *opt;\n\t\t\tstruct sk_buff *pktopt;\n\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol != IPPROTO_TCP)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfl6_free_socklist(sk);\n\t\t\tipv6_sock_mc_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = &tcp_prot;\n\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = prot;\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\t\t\topt = xchg(&np->opt, NULL);\n\t\t\tif (opt)\n\t\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t\t\tpktopt = xchg(&np->pktoptions, NULL);\n\t\t\tkfree_skb(pktopt);\n\n\t\t\tsk->sk_destruct = inet_sock_destruct;\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tnp->tclass = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !ns_capable(net->user_ns, CAP_NET_ADMIN) &&\n\t\t    !ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\t/* remove any sticky options header with a zero option\n\t\t * length, per RFC3542.\n\t\t */\n\t\tif (optlen == 0)\n\t\t\toptval = NULL;\n\t\telse if (!optval)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct ipv6_opt_hdr) ||\n\t\t\t optlen & 0x7 || optlen > 8 * 255)\n\t\t\tgoto e_inval;\n\n\t\t/* hop-by-hop / destination options are privileged option */\n\t\tretv = -EPERM;\n\t\tif (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\t\tbreak;\n\n\t\topt = ipv6_renew_options(sk, np->opt, optname,\n\t\t\t\t\t (struct ipv6_opt_hdr __user *)optval,\n\t\t\t\t\t optlen);\n\t\tif (IS_ERR(opt)) {\n\t\t\tretv = PTR_ERR(opt);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* routing header option needs extra check */\n\t\tretv = -EINVAL;\n\t\tif (optname == IPV6_RTHDR && opt && opt->srcrt) {\n\t\t\tstruct ipv6_rt_hdr *rthdr = opt->srcrt;\n\t\t\tswitch (rthdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\t\tif (rthdr->hdrlen != 2 ||\n\t\t\t\t    rthdr->segments_left != 1)\n\t\t\t\t\tgoto sticky_done;\n\n\t\t\t\tbreak;\n#endif\n\t\t\tdefault:\n\t\t\t\tgoto sticky_done;\n\t\t\t}\n\t\t}\n\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\nsticky_done:\n\t\tif (opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t\tbreak;\n\t}\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) || !optval)\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_user(&pkt, optval, sizeof(struct in6_pktinfo))) {\n\t\t\t\tretv = -EFAULT;\n\t\t\t\tbreak;\n\t\t}\n\t\tif (sk->sk_bound_dev_if && pkt.ipi6_ifindex != sk->sk_bound_dev_if)\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tint junk;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(opt+1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, opt, &junk,\n\t\t\t\t\t     &junk, &junk);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\n\t\t\tif (sk->sk_bound_dev_if && sk->sk_bound_dev_if != val)\n\t\t\t\tgoto e_inval;\n\n\t\t\tdev = dev_get_by_index(net, val);\n\t\t\tif (!dev) {\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdev_put(dev);\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t{\n\t\tstruct group_req greq;\n\t\tstruct sockaddr_in6 *psin6;\n\n\t\tif (optlen < sizeof(struct group_req))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&greq, optval, sizeof(struct group_req)))\n\t\t\tbreak;\n\t\tif (greq.gr_group.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tpsin6 = (struct sockaddr_in6 *)&greq.gr_group;\n\t\tif (optname == MCAST_JOIN_GROUP)\n\t\t\tretv = ipv6_sock_mc_join(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t{\n\t\tstruct group_source_req greqs;\n\t\tint omode, add;\n\n\t\tif (optlen < sizeof(struct group_source_req))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&greqs, optval, sizeof(greqs))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (greqs.gsr_group.ss_family != AF_INET6 ||\n\t\t    greqs.gsr_source.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\t\tstruct sockaddr_in6 *psin6;\n\n\t\t\tpsin6 = (struct sockaddr_in6 *)&greqs.gsr_group;\n\t\t\tretv = ipv6_sock_mc_join(sk, greqs.gsr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\t\t/* prior join w/ different source is ok */\n\t\t\tif (retv && retv != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\tretv = ip6_mc_source(add, omode, sk, &greqs);\n\t\tbreak;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter *gsf;\n\n\t\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tgsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!gsf) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(gsf, optval, optlen)) {\n\t\t\tkfree(gsf);\n\t\t\tbreak;\n\t\t}\n\t\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\t\tif (gsf->gf_numsrc >= 0x1ffffffU ||\n\t\t    gsf->gf_numsrc > sysctl_mld_max_msf) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tretv = ip6_mc_msfilter(sk, gsf);\n\t\tkfree(gsf);\n\n\t\tbreak;\n\t}\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t    {\n\t\tunsigned int pref = 0;\n\t\tunsigned int prefmask = ~0;\n\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EINVAL;\n\n\t\t/* check PUBLIC/TMP/PUBTMP_DEFAULT conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_PUBLIC|\n\t\t\t       IPV6_PREFER_SRC_TMP|\n\t\t\t       IPV6_PREFER_SRC_PUBTMP_DEFAULT)) {\n\t\tcase IPV6_PREFER_SRC_PUBLIC:\n\t\t\tpref |= IPV6_PREFER_SRC_PUBLIC;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_TMP:\n\t\t\tpref |= IPV6_PREFER_SRC_TMP;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_PUBTMP_DEFAULT:\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tgoto pref_skip_pubtmp;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~(IPV6_PREFER_SRC_PUBLIC|\n\t\t\t      IPV6_PREFER_SRC_TMP);\npref_skip_pubtmp:\n\n\t\t/* check HOME/COA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_HOME|IPV6_PREFER_SRC_COA)) {\n\t\tcase IPV6_PREFER_SRC_HOME:\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_COA:\n\t\t\tpref |= IPV6_PREFER_SRC_COA;\n\t\tcase 0:\n\t\t\tgoto pref_skip_coa;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~IPV6_PREFER_SRC_COA;\npref_skip_coa:\n\n\t\t/* check CGA/NONCGA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_CGA|IPV6_PREFER_SRC_NONCGA)) {\n\t\tcase IPV6_PREFER_SRC_CGA:\n\t\tcase IPV6_PREFER_SRC_NONCGA:\n\t\tcase 0:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tnp->srcprefs = (np->srcprefs & prefmask) | pref;\n\t\tretv = 0;\n\n\t\tbreak;\n\t    }\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tnp->min_hopcount = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}",
        "code_after_change": "static int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (!optval)\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (get_user(val, (int __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tstruct ipv6_txoptions *opt;\n\t\t\tstruct sk_buff *pktopt;\n\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol != IPPROTO_TCP)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfl6_free_socklist(sk);\n\t\t\tipv6_sock_mc_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = &tcp_prot;\n\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = prot;\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\t\t\topt = xchg((__force struct ipv6_txoptions **)&np->opt,\n\t\t\t\t   NULL);\n\t\t\tif (opt) {\n\t\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\t\ttxopt_put(opt);\n\t\t\t}\n\t\t\tpktopt = xchg(&np->pktoptions, NULL);\n\t\t\tkfree_skb(pktopt);\n\n\t\t\tsk->sk_destruct = inet_sock_destruct;\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tnp->tclass = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !ns_capable(net->user_ns, CAP_NET_ADMIN) &&\n\t\t    !ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\t/* remove any sticky options header with a zero option\n\t\t * length, per RFC3542.\n\t\t */\n\t\tif (optlen == 0)\n\t\t\toptval = NULL;\n\t\telse if (!optval)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct ipv6_opt_hdr) ||\n\t\t\t optlen & 0x7 || optlen > 8 * 255)\n\t\t\tgoto e_inval;\n\n\t\t/* hop-by-hop / destination options are privileged option */\n\t\tretv = -EPERM;\n\t\tif (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\t\tbreak;\n\n\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\t\topt = ipv6_renew_options(sk, opt, optname,\n\t\t\t\t\t (struct ipv6_opt_hdr __user *)optval,\n\t\t\t\t\t optlen);\n\t\tif (IS_ERR(opt)) {\n\t\t\tretv = PTR_ERR(opt);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* routing header option needs extra check */\n\t\tretv = -EINVAL;\n\t\tif (optname == IPV6_RTHDR && opt && opt->srcrt) {\n\t\t\tstruct ipv6_rt_hdr *rthdr = opt->srcrt;\n\t\t\tswitch (rthdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\t\tif (rthdr->hdrlen != 2 ||\n\t\t\t\t    rthdr->segments_left != 1)\n\t\t\t\t\tgoto sticky_done;\n\n\t\t\t\tbreak;\n#endif\n\t\t\tdefault:\n\t\t\t\tgoto sticky_done;\n\t\t\t}\n\t\t}\n\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\nsticky_done:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) || !optval)\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_user(&pkt, optval, sizeof(struct in6_pktinfo))) {\n\t\t\t\tretv = -EFAULT;\n\t\t\t\tbreak;\n\t\t}\n\t\tif (sk->sk_bound_dev_if && pkt.ipi6_ifindex != sk->sk_bound_dev_if)\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tint junk;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\tatomic_set(&opt->refcnt, 1);\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(opt+1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, opt, &junk,\n\t\t\t\t\t     &junk, &junk);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\n\t\t\tif (sk->sk_bound_dev_if && sk->sk_bound_dev_if != val)\n\t\t\t\tgoto e_inval;\n\n\t\t\tdev = dev_get_by_index(net, val);\n\t\t\tif (!dev) {\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdev_put(dev);\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t{\n\t\tstruct group_req greq;\n\t\tstruct sockaddr_in6 *psin6;\n\n\t\tif (optlen < sizeof(struct group_req))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&greq, optval, sizeof(struct group_req)))\n\t\t\tbreak;\n\t\tif (greq.gr_group.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tpsin6 = (struct sockaddr_in6 *)&greq.gr_group;\n\t\tif (optname == MCAST_JOIN_GROUP)\n\t\t\tretv = ipv6_sock_mc_join(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t{\n\t\tstruct group_source_req greqs;\n\t\tint omode, add;\n\n\t\tif (optlen < sizeof(struct group_source_req))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&greqs, optval, sizeof(greqs))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (greqs.gsr_group.ss_family != AF_INET6 ||\n\t\t    greqs.gsr_source.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\t\tstruct sockaddr_in6 *psin6;\n\n\t\t\tpsin6 = (struct sockaddr_in6 *)&greqs.gsr_group;\n\t\t\tretv = ipv6_sock_mc_join(sk, greqs.gsr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\t\t/* prior join w/ different source is ok */\n\t\t\tif (retv && retv != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\tretv = ip6_mc_source(add, omode, sk, &greqs);\n\t\tbreak;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter *gsf;\n\n\t\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tgsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!gsf) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(gsf, optval, optlen)) {\n\t\t\tkfree(gsf);\n\t\t\tbreak;\n\t\t}\n\t\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\t\tif (gsf->gf_numsrc >= 0x1ffffffU ||\n\t\t    gsf->gf_numsrc > sysctl_mld_max_msf) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tretv = ip6_mc_msfilter(sk, gsf);\n\t\tkfree(gsf);\n\n\t\tbreak;\n\t}\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t    {\n\t\tunsigned int pref = 0;\n\t\tunsigned int prefmask = ~0;\n\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EINVAL;\n\n\t\t/* check PUBLIC/TMP/PUBTMP_DEFAULT conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_PUBLIC|\n\t\t\t       IPV6_PREFER_SRC_TMP|\n\t\t\t       IPV6_PREFER_SRC_PUBTMP_DEFAULT)) {\n\t\tcase IPV6_PREFER_SRC_PUBLIC:\n\t\t\tpref |= IPV6_PREFER_SRC_PUBLIC;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_TMP:\n\t\t\tpref |= IPV6_PREFER_SRC_TMP;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_PUBTMP_DEFAULT:\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tgoto pref_skip_pubtmp;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~(IPV6_PREFER_SRC_PUBLIC|\n\t\t\t      IPV6_PREFER_SRC_TMP);\npref_skip_pubtmp:\n\n\t\t/* check HOME/COA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_HOME|IPV6_PREFER_SRC_COA)) {\n\t\tcase IPV6_PREFER_SRC_HOME:\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_COA:\n\t\t\tpref |= IPV6_PREFER_SRC_COA;\n\t\tcase 0:\n\t\t\tgoto pref_skip_coa;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~IPV6_PREFER_SRC_COA;\npref_skip_coa:\n\n\t\t/* check CGA/NONCGA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_CGA|IPV6_PREFER_SRC_NONCGA)) {\n\t\tcase IPV6_PREFER_SRC_CGA:\n\t\tcase IPV6_PREFER_SRC_NONCGA:\n\t\tcase 0:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tnp->srcprefs = (np->srcprefs & prefmask) | pref;\n\t\tretv = 0;\n\n\t\tbreak;\n\t    }\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tnp->min_hopcount = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -93,9 +93,12 @@\n \t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n \t\t\t\tsk->sk_family = PF_INET;\n \t\t\t}\n-\t\t\topt = xchg(&np->opt, NULL);\n-\t\t\tif (opt)\n-\t\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n+\t\t\topt = xchg((__force struct ipv6_txoptions **)&np->opt,\n+\t\t\t\t   NULL);\n+\t\t\tif (opt) {\n+\t\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n+\t\t\t\ttxopt_put(opt);\n+\t\t\t}\n \t\t\tpktopt = xchg(&np->pktoptions, NULL);\n \t\t\tkfree_skb(pktopt);\n \n@@ -265,7 +268,8 @@\n \t\tif (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))\n \t\t\tbreak;\n \n-\t\topt = ipv6_renew_options(sk, np->opt, optname,\n+\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n+\t\topt = ipv6_renew_options(sk, opt, optname,\n \t\t\t\t\t (struct ipv6_opt_hdr __user *)optval,\n \t\t\t\t\t optlen);\n \t\tif (IS_ERR(opt)) {\n@@ -294,8 +298,10 @@\n \t\tretv = 0;\n \t\topt = ipv6_update_options(sk, opt);\n sticky_done:\n-\t\tif (opt)\n-\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n+\t\tif (opt) {\n+\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n+\t\t\ttxopt_put(opt);\n+\t\t}\n \t\tbreak;\n \t}\n \n@@ -348,6 +354,7 @@\n \t\t\tbreak;\n \n \t\tmemset(opt, 0, sizeof(*opt));\n+\t\tatomic_set(&opt->refcnt, 1);\n \t\topt->tot_len = sizeof(*opt) + optlen;\n \t\tretv = -EFAULT;\n \t\tif (copy_from_user(opt+1, optval, optlen))\n@@ -364,8 +371,10 @@\n \t\tretv = 0;\n \t\topt = ipv6_update_options(sk, opt);\n done:\n-\t\tif (opt)\n-\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n+\t\tif (opt) {\n+\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n+\t\t\ttxopt_put(opt);\n+\t\t}\n \t\tbreak;\n \t}\n \tcase IPV6_UNICAST_HOPS:",
        "function_modified_lines": {
            "added": [
                "\t\t\topt = xchg((__force struct ipv6_txoptions **)&np->opt,",
                "\t\t\t\t   NULL);",
                "\t\t\tif (opt) {",
                "\t\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);",
                "\t\t\t\ttxopt_put(opt);",
                "\t\t\t}",
                "\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));",
                "\t\topt = ipv6_renew_options(sk, opt, optname,",
                "\t\tif (opt) {",
                "\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);",
                "\t\t\ttxopt_put(opt);",
                "\t\t}",
                "\t\tatomic_set(&opt->refcnt, 1);",
                "\t\tif (opt) {",
                "\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);",
                "\t\t\ttxopt_put(opt);",
                "\t\t}"
            ],
            "deleted": [
                "\t\t\topt = xchg(&np->opt, NULL);",
                "\t\t\tif (opt)",
                "\t\t\t\tsock_kfree_s(sk, opt, opt->tot_len);",
                "\t\topt = ipv6_renew_options(sk, np->opt, optname,",
                "\t\tif (opt)",
                "\t\t\tsock_kfree_s(sk, opt, opt->tot_len);",
                "\t\tif (opt)",
                "\t\t\tsock_kfree_s(sk, opt, opt->tot_len);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1001
    },
    {
        "cve_id": "CVE-2012-2319",
        "code_before_change": "static int hfsplus_readdir(struct file *filp, void *dirent, filldir_t filldir)\n{\n\tstruct inode *inode = filp->f_path.dentry->d_inode;\n\tstruct super_block *sb = inode->i_sb;\n\tint len, err;\n\tchar strbuf[HFSPLUS_MAX_STRLEN + 1];\n\thfsplus_cat_entry entry;\n\tstruct hfs_find_data fd;\n\tstruct hfsplus_readdir_data *rd;\n\tu16 type;\n\n\tif (filp->f_pos >= inode->i_size)\n\t\treturn 0;\n\n\terr = hfs_find_init(HFSPLUS_SB(sb)->cat_tree, &fd);\n\tif (err)\n\t\treturn err;\n\thfsplus_cat_build_key(sb, fd.search_key, inode->i_ino, NULL);\n\terr = hfs_brec_find(&fd);\n\tif (err)\n\t\tgoto out;\n\n\tswitch ((u32)filp->f_pos) {\n\tcase 0:\n\t\t/* This is completely artificial... */\n\t\tif (filldir(dirent, \".\", 1, 0, inode->i_ino, DT_DIR))\n\t\t\tgoto out;\n\t\tfilp->f_pos++;\n\t\t/* fall through */\n\tcase 1:\n\t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n\t\t\tfd.entrylength);\n\t\tif (be16_to_cpu(entry.type) != HFSPLUS_FOLDER_THREAD) {\n\t\t\tprintk(KERN_ERR \"hfs: bad catalog folder thread\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (fd.entrylength < HFSPLUS_MIN_THREAD_SZ) {\n\t\t\tprintk(KERN_ERR \"hfs: truncated catalog thread\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (filldir(dirent, \"..\", 2, 1,\n\t\t\t    be32_to_cpu(entry.thread.parentID), DT_DIR))\n\t\t\tgoto out;\n\t\tfilp->f_pos++;\n\t\t/* fall through */\n\tdefault:\n\t\tif (filp->f_pos >= inode->i_size)\n\t\t\tgoto out;\n\t\terr = hfs_brec_goto(&fd, filp->f_pos - 1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tfor (;;) {\n\t\tif (be32_to_cpu(fd.key->cat.parent) != inode->i_ino) {\n\t\t\tprintk(KERN_ERR \"hfs: walked past end of dir\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n\t\t\tfd.entrylength);\n\t\ttype = be16_to_cpu(entry.type);\n\t\tlen = HFSPLUS_MAX_STRLEN;\n\t\terr = hfsplus_uni2asc(sb, &fd.key->cat.name, strbuf, &len);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (type == HFSPLUS_FOLDER) {\n\t\t\tif (fd.entrylength <\n\t\t\t\t\tsizeof(struct hfsplus_cat_folder)) {\n\t\t\t\tprintk(KERN_ERR \"hfs: small dir entry\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (HFSPLUS_SB(sb)->hidden_dir &&\n\t\t\t    HFSPLUS_SB(sb)->hidden_dir->i_ino ==\n\t\t\t\t\tbe32_to_cpu(entry.folder.id))\n\t\t\t\tgoto next;\n\t\t\tif (filldir(dirent, strbuf, len, filp->f_pos,\n\t\t\t\t    be32_to_cpu(entry.folder.id), DT_DIR))\n\t\t\t\tbreak;\n\t\t} else if (type == HFSPLUS_FILE) {\n\t\t\tif (fd.entrylength < sizeof(struct hfsplus_cat_file)) {\n\t\t\t\tprintk(KERN_ERR \"hfs: small file entry\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (filldir(dirent, strbuf, len, filp->f_pos,\n\t\t\t\t    be32_to_cpu(entry.file.id), DT_REG))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tprintk(KERN_ERR \"hfs: bad catalog entry type\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\nnext:\n\t\tfilp->f_pos++;\n\t\tif (filp->f_pos >= inode->i_size)\n\t\t\tgoto out;\n\t\terr = hfs_brec_goto(&fd, 1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\trd = filp->private_data;\n\tif (!rd) {\n\t\trd = kmalloc(sizeof(struct hfsplus_readdir_data), GFP_KERNEL);\n\t\tif (!rd) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tfilp->private_data = rd;\n\t\trd->file = filp;\n\t\tlist_add(&rd->list, &HFSPLUS_I(inode)->open_dir_list);\n\t}\n\tmemcpy(&rd->key, fd.key, sizeof(struct hfsplus_cat_key));\nout:\n\thfs_find_exit(&fd);\n\treturn err;\n}",
        "code_after_change": "static int hfsplus_readdir(struct file *filp, void *dirent, filldir_t filldir)\n{\n\tstruct inode *inode = filp->f_path.dentry->d_inode;\n\tstruct super_block *sb = inode->i_sb;\n\tint len, err;\n\tchar strbuf[HFSPLUS_MAX_STRLEN + 1];\n\thfsplus_cat_entry entry;\n\tstruct hfs_find_data fd;\n\tstruct hfsplus_readdir_data *rd;\n\tu16 type;\n\n\tif (filp->f_pos >= inode->i_size)\n\t\treturn 0;\n\n\terr = hfs_find_init(HFSPLUS_SB(sb)->cat_tree, &fd);\n\tif (err)\n\t\treturn err;\n\thfsplus_cat_build_key(sb, fd.search_key, inode->i_ino, NULL);\n\terr = hfs_brec_find(&fd);\n\tif (err)\n\t\tgoto out;\n\n\tswitch ((u32)filp->f_pos) {\n\tcase 0:\n\t\t/* This is completely artificial... */\n\t\tif (filldir(dirent, \".\", 1, 0, inode->i_ino, DT_DIR))\n\t\t\tgoto out;\n\t\tfilp->f_pos++;\n\t\t/* fall through */\n\tcase 1:\n\t\tif (fd.entrylength > sizeof(entry) || fd.entrylength < 0) {\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n\t\t\tfd.entrylength);\n\t\tif (be16_to_cpu(entry.type) != HFSPLUS_FOLDER_THREAD) {\n\t\t\tprintk(KERN_ERR \"hfs: bad catalog folder thread\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (fd.entrylength < HFSPLUS_MIN_THREAD_SZ) {\n\t\t\tprintk(KERN_ERR \"hfs: truncated catalog thread\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (filldir(dirent, \"..\", 2, 1,\n\t\t\t    be32_to_cpu(entry.thread.parentID), DT_DIR))\n\t\t\tgoto out;\n\t\tfilp->f_pos++;\n\t\t/* fall through */\n\tdefault:\n\t\tif (filp->f_pos >= inode->i_size)\n\t\t\tgoto out;\n\t\terr = hfs_brec_goto(&fd, filp->f_pos - 1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tfor (;;) {\n\t\tif (be32_to_cpu(fd.key->cat.parent) != inode->i_ino) {\n\t\t\tprintk(KERN_ERR \"hfs: walked past end of dir\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (fd.entrylength > sizeof(entry) || fd.entrylength < 0) {\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n\t\t\tfd.entrylength);\n\t\ttype = be16_to_cpu(entry.type);\n\t\tlen = HFSPLUS_MAX_STRLEN;\n\t\terr = hfsplus_uni2asc(sb, &fd.key->cat.name, strbuf, &len);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (type == HFSPLUS_FOLDER) {\n\t\t\tif (fd.entrylength <\n\t\t\t\t\tsizeof(struct hfsplus_cat_folder)) {\n\t\t\t\tprintk(KERN_ERR \"hfs: small dir entry\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (HFSPLUS_SB(sb)->hidden_dir &&\n\t\t\t    HFSPLUS_SB(sb)->hidden_dir->i_ino ==\n\t\t\t\t\tbe32_to_cpu(entry.folder.id))\n\t\t\t\tgoto next;\n\t\t\tif (filldir(dirent, strbuf, len, filp->f_pos,\n\t\t\t\t    be32_to_cpu(entry.folder.id), DT_DIR))\n\t\t\t\tbreak;\n\t\t} else if (type == HFSPLUS_FILE) {\n\t\t\tif (fd.entrylength < sizeof(struct hfsplus_cat_file)) {\n\t\t\t\tprintk(KERN_ERR \"hfs: small file entry\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (filldir(dirent, strbuf, len, filp->f_pos,\n\t\t\t\t    be32_to_cpu(entry.file.id), DT_REG))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tprintk(KERN_ERR \"hfs: bad catalog entry type\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\nnext:\n\t\tfilp->f_pos++;\n\t\tif (filp->f_pos >= inode->i_size)\n\t\t\tgoto out;\n\t\terr = hfs_brec_goto(&fd, 1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\trd = filp->private_data;\n\tif (!rd) {\n\t\trd = kmalloc(sizeof(struct hfsplus_readdir_data), GFP_KERNEL);\n\t\tif (!rd) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tfilp->private_data = rd;\n\t\trd->file = filp;\n\t\tlist_add(&rd->list, &HFSPLUS_I(inode)->open_dir_list);\n\t}\n\tmemcpy(&rd->key, fd.key, sizeof(struct hfsplus_cat_key));\nout:\n\thfs_find_exit(&fd);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -28,6 +28,11 @@\n \t\tfilp->f_pos++;\n \t\t/* fall through */\n \tcase 1:\n+\t\tif (fd.entrylength > sizeof(entry) || fd.entrylength < 0) {\n+\t\t\terr = -EIO;\n+\t\t\tgoto out;\n+\t\t}\n+\n \t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n \t\t\tfd.entrylength);\n \t\tif (be16_to_cpu(entry.type) != HFSPLUS_FOLDER_THREAD) {\n@@ -59,6 +64,12 @@\n \t\t\terr = -EIO;\n \t\t\tgoto out;\n \t\t}\n+\n+\t\tif (fd.entrylength > sizeof(entry) || fd.entrylength < 0) {\n+\t\t\terr = -EIO;\n+\t\t\tgoto out;\n+\t\t}\n+\n \t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n \t\t\tfd.entrylength);\n \t\ttype = be16_to_cpu(entry.type);",
        "function_modified_lines": {
            "added": [
                "\t\tif (fd.entrylength > sizeof(entry) || fd.entrylength < 0) {",
                "\t\t\terr = -EIO;",
                "\t\t\tgoto out;",
                "\t\t}",
                "",
                "",
                "\t\tif (fd.entrylength > sizeof(entry) || fd.entrylength < 0) {",
                "\t\t\terr = -EIO;",
                "\t\t\tgoto out;",
                "\t\t}",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "Multiple buffer overflows in the hfsplus filesystem implementation in the Linux kernel before 3.3.5 allow local users to gain privileges via a crafted HFS plus filesystem, a related issue to CVE-2009-4020.",
        "id": 45
    },
    {
        "cve_id": "CVE-2016-6786",
        "code_before_change": "static void perf_event_for_each(struct perf_event *event,\n\t\t\t\t  void (*func)(struct perf_event *))\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_event *sibling;\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\tevent = event->group_leader;\n\n\tperf_event_for_each_child(event, func);\n\tlist_for_each_entry(sibling, &event->sibling_list, group_entry)\n\t\tperf_event_for_each_child(sibling, func);\n\tmutex_unlock(&ctx->mutex);\n}",
        "code_after_change": "static void perf_event_for_each(struct perf_event *event,\n\t\t\t\t  void (*func)(struct perf_event *))\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_event *sibling;\n\n\tlockdep_assert_held(&ctx->mutex);\n\n\tevent = event->group_leader;\n\n\tperf_event_for_each_child(event, func);\n\tlist_for_each_entry(sibling, &event->sibling_list, group_entry)\n\t\tperf_event_for_each_child(sibling, func);\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,12 +4,11 @@\n \tstruct perf_event_context *ctx = event->ctx;\n \tstruct perf_event *sibling;\n \n-\tWARN_ON_ONCE(ctx->parent_ctx);\n-\tmutex_lock(&ctx->mutex);\n+\tlockdep_assert_held(&ctx->mutex);\n+\n \tevent = event->group_leader;\n \n \tperf_event_for_each_child(event, func);\n \tlist_for_each_entry(sibling, &event->sibling_list, group_entry)\n \t\tperf_event_for_each_child(sibling, func);\n-\tmutex_unlock(&ctx->mutex);\n }",
        "function_modified_lines": {
            "added": [
                "\tlockdep_assert_held(&ctx->mutex);",
                ""
            ],
            "deleted": [
                "\tWARN_ON_ONCE(ctx->parent_ctx);",
                "\tmutex_lock(&ctx->mutex);",
                "\tmutex_unlock(&ctx->mutex);"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "kernel/events/core.c in the performance subsystem in the Linux kernel before 4.0 mismanages locks during certain migrations, which allows local users to gain privileges via a crafted application, aka Android internal bug 30955111.",
        "id": 1090
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "struct ipv6_txoptions *ipv6_update_options(struct sock *sk,\n\t\t\t\t\t   struct ipv6_txoptions *opt)\n{\n\tif (inet_sk(sk)->is_icsk) {\n\t\tif (opt &&\n\t\t    !((1 << sk->sk_state) & (TCPF_LISTEN | TCPF_CLOSE)) &&\n\t\t    inet_sk(sk)->inet_daddr != LOOPBACK4_IPV6) {\n\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n\t\t\ticsk->icsk_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t}\n\t}\n\topt = xchg(&inet6_sk(sk)->opt, opt);\n\tsk_dst_reset(sk);\n\n\treturn opt;\n}",
        "code_after_change": "struct ipv6_txoptions *ipv6_update_options(struct sock *sk,\n\t\t\t\t\t   struct ipv6_txoptions *opt)\n{\n\tif (inet_sk(sk)->is_icsk) {\n\t\tif (opt &&\n\t\t    !((1 << sk->sk_state) & (TCPF_LISTEN | TCPF_CLOSE)) &&\n\t\t    inet_sk(sk)->inet_daddr != LOOPBACK4_IPV6) {\n\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n\t\t\ticsk->icsk_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t}\n\t}\n\topt = xchg((__force struct ipv6_txoptions **)&inet6_sk(sk)->opt,\n\t\t   opt);\n\tsk_dst_reset(sk);\n\n\treturn opt;\n}",
        "patch": "--- code before\n+++ code after\n@@ -10,7 +10,8 @@\n \t\t\ticsk->icsk_sync_mss(sk, icsk->icsk_pmtu_cookie);\n \t\t}\n \t}\n-\topt = xchg(&inet6_sk(sk)->opt, opt);\n+\topt = xchg((__force struct ipv6_txoptions **)&inet6_sk(sk)->opt,\n+\t\t   opt);\n \tsk_dst_reset(sk);\n \n \treturn opt;",
        "function_modified_lines": {
            "added": [
                "\topt = xchg((__force struct ipv6_txoptions **)&inet6_sk(sk)->opt,",
                "\t\t   opt);"
            ],
            "deleted": [
                "\topt = xchg(&inet6_sk(sk)->opt, opt);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1000
    },
    {
        "cve_id": "CVE-2015-9016",
        "code_before_change": "static bool blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq)\n{\n\tstruct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];\n\tstruct request *first_rq =\n\t\tlist_first_entry(pending, struct request, flush.list);\n\tstruct request *flush_rq = fq->flush_rq;\n\n\t/* C1 described at the top of this file */\n\tif (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))\n\t\treturn false;\n\n\t/* C2 and C3 */\n\tif (!list_empty(&fq->flush_data_in_flight) &&\n\t    time_before(jiffies,\n\t\t\tfq->flush_pending_since + FLUSH_PENDING_TIMEOUT))\n\t\treturn false;\n\n\t/*\n\t * Issue flush and toggle pending_idx.  This makes pending_idx\n\t * different from running_idx, which means flush is in flight.\n\t */\n\tfq->flush_pending_idx ^= 1;\n\n\tblk_rq_init(q, flush_rq);\n\n\t/*\n\t * Borrow tag from the first request since they can't\n\t * be in flight at the same time.\n\t */\n\tif (q->mq_ops) {\n\t\tflush_rq->mq_ctx = first_rq->mq_ctx;\n\t\tflush_rq->tag = first_rq->tag;\n\t}\n\n\tflush_rq->cmd_type = REQ_TYPE_FS;\n\tflush_rq->cmd_flags = WRITE_FLUSH | REQ_FLUSH_SEQ;\n\tflush_rq->rq_disk = first_rq->rq_disk;\n\tflush_rq->end_io = flush_end_io;\n\n\treturn blk_flush_queue_rq(flush_rq, false);\n}",
        "code_after_change": "static bool blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq)\n{\n\tstruct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];\n\tstruct request *first_rq =\n\t\tlist_first_entry(pending, struct request, flush.list);\n\tstruct request *flush_rq = fq->flush_rq;\n\n\t/* C1 described at the top of this file */\n\tif (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))\n\t\treturn false;\n\n\t/* C2 and C3 */\n\tif (!list_empty(&fq->flush_data_in_flight) &&\n\t    time_before(jiffies,\n\t\t\tfq->flush_pending_since + FLUSH_PENDING_TIMEOUT))\n\t\treturn false;\n\n\t/*\n\t * Issue flush and toggle pending_idx.  This makes pending_idx\n\t * different from running_idx, which means flush is in flight.\n\t */\n\tfq->flush_pending_idx ^= 1;\n\n\tblk_rq_init(q, flush_rq);\n\n\t/*\n\t * Borrow tag from the first request since they can't\n\t * be in flight at the same time. And acquire the tag's\n\t * ownership for flush req.\n\t */\n\tif (q->mq_ops) {\n\t\tstruct blk_mq_hw_ctx *hctx;\n\n\t\tflush_rq->mq_ctx = first_rq->mq_ctx;\n\t\tflush_rq->tag = first_rq->tag;\n\t\tfq->orig_rq = first_rq;\n\n\t\thctx = q->mq_ops->map_queue(q, first_rq->mq_ctx->cpu);\n\t\tblk_mq_tag_set_rq(hctx, first_rq->tag, flush_rq);\n\t}\n\n\tflush_rq->cmd_type = REQ_TYPE_FS;\n\tflush_rq->cmd_flags = WRITE_FLUSH | REQ_FLUSH_SEQ;\n\tflush_rq->rq_disk = first_rq->rq_disk;\n\tflush_rq->end_io = flush_end_io;\n\n\treturn blk_flush_queue_rq(flush_rq, false);\n}",
        "patch": "--- code before\n+++ code after\n@@ -25,11 +25,18 @@\n \n \t/*\n \t * Borrow tag from the first request since they can't\n-\t * be in flight at the same time.\n+\t * be in flight at the same time. And acquire the tag's\n+\t * ownership for flush req.\n \t */\n \tif (q->mq_ops) {\n+\t\tstruct blk_mq_hw_ctx *hctx;\n+\n \t\tflush_rq->mq_ctx = first_rq->mq_ctx;\n \t\tflush_rq->tag = first_rq->tag;\n+\t\tfq->orig_rq = first_rq;\n+\n+\t\thctx = q->mq_ops->map_queue(q, first_rq->mq_ctx->cpu);\n+\t\tblk_mq_tag_set_rq(hctx, first_rq->tag, flush_rq);\n \t}\n \n \tflush_rq->cmd_type = REQ_TYPE_FS;",
        "function_modified_lines": {
            "added": [
                "\t * be in flight at the same time. And acquire the tag's",
                "\t * ownership for flush req.",
                "\t\tstruct blk_mq_hw_ctx *hctx;",
                "",
                "\t\tfq->orig_rq = first_rq;",
                "",
                "\t\thctx = q->mq_ops->map_queue(q, first_rq->mq_ctx->cpu);",
                "\t\tblk_mq_tag_set_rq(hctx, first_rq->tag, flush_rq);"
            ],
            "deleted": [
                "\t * be in flight at the same time."
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-362"
        ],
        "cve_description": "In blk_mq_tag_to_rq in blk-mq.c in the upstream kernel, there is a possible use after free due to a race condition when a request has been previously freed by blk_mq_complete_request. This could lead to local escalation of privilege. Product: Android. Versions: Android kernel. Android ID: A-63083046.",
        "id": 882
    },
    {
        "cve_id": "CVE-2015-1593",
        "code_before_change": "static unsigned long randomize_stack_top(unsigned long stack_top)\n{\n\tunsigned int random_variable = 0;\n\n\tif ((current->flags & PF_RANDOMIZE) &&\n\t\t!(current->personality & ADDR_NO_RANDOMIZE)) {\n\t\trandom_variable = get_random_int() & STACK_RND_MASK;\n\t\trandom_variable <<= PAGE_SHIFT;\n\t}\n#ifdef CONFIG_STACK_GROWSUP\n\treturn PAGE_ALIGN(stack_top) + random_variable;\n#else\n\treturn PAGE_ALIGN(stack_top) - random_variable;\n#endif\n}",
        "code_after_change": "static unsigned long randomize_stack_top(unsigned long stack_top)\n{\n\tunsigned long random_variable = 0;\n\n\tif ((current->flags & PF_RANDOMIZE) &&\n\t\t!(current->personality & ADDR_NO_RANDOMIZE)) {\n\t\trandom_variable = (unsigned long) get_random_int();\n\t\trandom_variable &= STACK_RND_MASK;\n\t\trandom_variable <<= PAGE_SHIFT;\n\t}\n#ifdef CONFIG_STACK_GROWSUP\n\treturn PAGE_ALIGN(stack_top) + random_variable;\n#else\n\treturn PAGE_ALIGN(stack_top) - random_variable;\n#endif\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,10 +1,11 @@\n static unsigned long randomize_stack_top(unsigned long stack_top)\n {\n-\tunsigned int random_variable = 0;\n+\tunsigned long random_variable = 0;\n \n \tif ((current->flags & PF_RANDOMIZE) &&\n \t\t!(current->personality & ADDR_NO_RANDOMIZE)) {\n-\t\trandom_variable = get_random_int() & STACK_RND_MASK;\n+\t\trandom_variable = (unsigned long) get_random_int();\n+\t\trandom_variable &= STACK_RND_MASK;\n \t\trandom_variable <<= PAGE_SHIFT;\n \t}\n #ifdef CONFIG_STACK_GROWSUP",
        "function_modified_lines": {
            "added": [
                "\tunsigned long random_variable = 0;",
                "\t\trandom_variable = (unsigned long) get_random_int();",
                "\t\trandom_variable &= STACK_RND_MASK;"
            ],
            "deleted": [
                "\tunsigned int random_variable = 0;",
                "\t\trandom_variable = get_random_int() & STACK_RND_MASK;"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The stack randomization feature in the Linux kernel before 3.19.1 on 64-bit platforms uses incorrect data types for the results of bitwise left-shift operations, which makes it easier for attackers to bypass the ASLR protection mechanism by predicting the address of the top of the stack, related to the randomize_stack_top function in fs/binfmt_elf.c and the stack_maxrandom_size function in arch/x86/mm/mmap.c.",
        "id": 739
    },
    {
        "cve_id": "CVE-2014-0181",
        "code_before_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
        "code_after_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,7 +8,7 @@\n \tstruct dn_ifaddr *ifa;\n \tint err;\n \n-\tif (!capable(CAP_NET_ADMIN))\n+\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n \t\treturn -EPERM;\n \n \tif (!net_eq(net, &init_net))",
        "function_modified_lines": {
            "added": [
                "\tif (!netlink_capable(skb, CAP_NET_ADMIN))"
            ],
            "deleted": [
                "\tif (!capable(CAP_NET_ADMIN))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Netlink implementation in the Linux kernel through 3.14.1 does not provide a mechanism for authorizing socket operations based on the opener of a socket, which allows local users to bypass intended access restrictions and modify network configurations by using a Netlink socket for the (1) stdout or (2) stderr of a setuid program.",
        "id": 445
    },
    {
        "cve_id": "CVE-2016-4997",
        "code_before_change": "static int\ncheck_entry_size_and_hooks(struct ip6t_entry *e,\n\t\t\t   struct xt_table_info *newinfo,\n\t\t\t   const unsigned char *base,\n\t\t\t   const unsigned char *limit,\n\t\t\t   const unsigned int *hook_entries,\n\t\t\t   const unsigned int *underflows,\n\t\t\t   unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct ip6t_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}",
        "code_after_change": "static int\ncheck_entry_size_and_hooks(struct ip6t_entry *e,\n\t\t\t   struct xt_table_info *newinfo,\n\t\t\t   const unsigned char *base,\n\t\t\t   const unsigned char *limit,\n\t\t\t   const unsigned int *hook_entries,\n\t\t\t   const unsigned int *underflows,\n\t\t\t   unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct ip6t_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t     e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -27,7 +27,8 @@\n \tif (!ip6_checkentry(&e->ipv6))\n \t\treturn -EINVAL;\n \n-\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n+\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n+\t\t\t\t     e->next_offset);\n \tif (err)\n \t\treturn err;\n ",
        "function_modified_lines": {
            "added": [
                "\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,",
                "\t\t\t\t     e->next_offset);"
            ],
            "deleted": [
                "\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The compat IPT_SO_SET_REPLACE and IP6T_SO_SET_REPLACE setsockopt implementations in the netfilter subsystem in the Linux kernel before 4.6.3 allow local users to gain privileges or cause a denial of service (memory corruption) by leveraging in-container root access to provide a crafted offset value that triggers an unintended decrement.",
        "id": 1043
    },
    {
        "cve_id": "CVE-2014-9870",
        "code_before_change": "static int get_tp_trap(struct pt_regs *regs, unsigned int instr)\n{\n\tint reg = (instr >> 12) & 15;\n\tif (reg == 15)\n\t\treturn 1;\n\tregs->uregs[reg] = current_thread_info()->tp_value;\n\tregs->ARM_pc += 4;\n\treturn 0;\n}",
        "code_after_change": "static int get_tp_trap(struct pt_regs *regs, unsigned int instr)\n{\n\tint reg = (instr >> 12) & 15;\n\tif (reg == 15)\n\t\treturn 1;\n\tregs->uregs[reg] = current_thread_info()->tp_value[0];\n\tregs->ARM_pc += 4;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -3,7 +3,7 @@\n \tint reg = (instr >> 12) & 15;\n \tif (reg == 15)\n \t\treturn 1;\n-\tregs->uregs[reg] = current_thread_info()->tp_value;\n+\tregs->uregs[reg] = current_thread_info()->tp_value[0];\n \tregs->ARM_pc += 4;\n \treturn 0;\n }",
        "function_modified_lines": {
            "added": [
                "\tregs->uregs[reg] = current_thread_info()->tp_value[0];"
            ],
            "deleted": [
                "\tregs->uregs[reg] = current_thread_info()->tp_value;"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Linux kernel before 3.11 on ARM platforms, as used in Android before 2016-08-05 on Nexus 5 and 7 (2013) devices, does not properly consider user-space access to the TPIDRURW register, which allows local users to gain privileges via a crafted application, aka Android internal bug 28749743 and Qualcomm internal bug CR561044.",
        "id": 705
    },
    {
        "cve_id": "CVE-2014-0181",
        "code_before_change": "static void cn_proc_mcast_ctl(struct cn_msg *msg,\n\t\t\t      struct netlink_skb_parms *nsp)\n{\n\tenum proc_cn_mcast_op *mc_op = NULL;\n\tint err = 0;\n\n\tif (msg->len != sizeof(*mc_op))\n\t\treturn;\n\n\t/* \n\t * Events are reported with respect to the initial pid\n\t * and user namespaces so ignore requestors from\n\t * other namespaces.\n\t */\n\tif ((current_user_ns() != &init_user_ns) ||\n\t    (task_active_pid_ns(current) != &init_pid_ns))\n\t\treturn;\n\n\t/* Can only change if privileged. */\n\tif (!capable(CAP_NET_ADMIN)) {\n\t\terr = EPERM;\n\t\tgoto out;\n\t}\n\n\tmc_op = (enum proc_cn_mcast_op *)msg->data;\n\tswitch (*mc_op) {\n\tcase PROC_CN_MCAST_LISTEN:\n\t\tatomic_inc(&proc_event_num_listeners);\n\t\tbreak;\n\tcase PROC_CN_MCAST_IGNORE:\n\t\tatomic_dec(&proc_event_num_listeners);\n\t\tbreak;\n\tdefault:\n\t\terr = EINVAL;\n\t\tbreak;\n\t}\n\nout:\n\tcn_proc_ack(err, msg->seq, msg->ack);\n}",
        "code_after_change": "static void cn_proc_mcast_ctl(struct cn_msg *msg,\n\t\t\t      struct netlink_skb_parms *nsp)\n{\n\tenum proc_cn_mcast_op *mc_op = NULL;\n\tint err = 0;\n\n\tif (msg->len != sizeof(*mc_op))\n\t\treturn;\n\n\t/* \n\t * Events are reported with respect to the initial pid\n\t * and user namespaces so ignore requestors from\n\t * other namespaces.\n\t */\n\tif ((current_user_ns() != &init_user_ns) ||\n\t    (task_active_pid_ns(current) != &init_pid_ns))\n\t\treturn;\n\n\t/* Can only change if privileged. */\n\tif (!__netlink_ns_capable(nsp, &init_user_ns, CAP_NET_ADMIN)) {\n\t\terr = EPERM;\n\t\tgoto out;\n\t}\n\n\tmc_op = (enum proc_cn_mcast_op *)msg->data;\n\tswitch (*mc_op) {\n\tcase PROC_CN_MCAST_LISTEN:\n\t\tatomic_inc(&proc_event_num_listeners);\n\t\tbreak;\n\tcase PROC_CN_MCAST_IGNORE:\n\t\tatomic_dec(&proc_event_num_listeners);\n\t\tbreak;\n\tdefault:\n\t\terr = EINVAL;\n\t\tbreak;\n\t}\n\nout:\n\tcn_proc_ack(err, msg->seq, msg->ack);\n}",
        "patch": "--- code before\n+++ code after\n@@ -17,7 +17,7 @@\n \t\treturn;\n \n \t/* Can only change if privileged. */\n-\tif (!capable(CAP_NET_ADMIN)) {\n+\tif (!__netlink_ns_capable(nsp, &init_user_ns, CAP_NET_ADMIN)) {\n \t\terr = EPERM;\n \t\tgoto out;\n \t}",
        "function_modified_lines": {
            "added": [
                "\tif (!__netlink_ns_capable(nsp, &init_user_ns, CAP_NET_ADMIN)) {"
            ],
            "deleted": [
                "\tif (!capable(CAP_NET_ADMIN)) {"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Netlink implementation in the Linux kernel through 3.14.1 does not provide a mechanism for authorizing socket operations based on the opener of a socket, which allows local users to bypass intended access restrictions and modify network configurations by using a Netlink socket for the (1) stdout or (2) stderr of a setuid program.",
        "id": 434
    },
    {
        "cve_id": "CVE-2014-0181",
        "code_before_change": "static int packet_diag_dump(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tint num = 0, s_num = cb->args[0];\n\tstruct packet_diag_req *req;\n\tstruct net *net;\n\tstruct sock *sk;\n\tbool may_report_filterinfo;\n\n\tnet = sock_net(skb->sk);\n\treq = nlmsg_data(cb->nlh);\n\tmay_report_filterinfo = ns_capable(net->user_ns, CAP_NET_ADMIN);\n\n\tmutex_lock(&net->packet.sklist_lock);\n\tsk_for_each(sk, &net->packet.sklist) {\n\t\tif (!net_eq(sock_net(sk), net))\n\t\t\tcontinue;\n\t\tif (num < s_num)\n\t\t\tgoto next;\n\n\t\tif (sk_diag_fill(sk, skb, req,\n\t\t\t\t may_report_filterinfo,\n\t\t\t\t sk_user_ns(NETLINK_CB(cb->skb).sk),\n\t\t\t\t NETLINK_CB(cb->skb).portid,\n\t\t\t\t cb->nlh->nlmsg_seq, NLM_F_MULTI,\n\t\t\t\t sock_i_ino(sk)) < 0)\n\t\t\tgoto done;\nnext:\n\t\tnum++;\n\t}\ndone:\n\tmutex_unlock(&net->packet.sklist_lock);\n\tcb->args[0] = num;\n\n\treturn skb->len;\n}",
        "code_after_change": "static int packet_diag_dump(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tint num = 0, s_num = cb->args[0];\n\tstruct packet_diag_req *req;\n\tstruct net *net;\n\tstruct sock *sk;\n\tbool may_report_filterinfo;\n\n\tnet = sock_net(skb->sk);\n\treq = nlmsg_data(cb->nlh);\n\tmay_report_filterinfo = netlink_net_capable(cb->skb, CAP_NET_ADMIN);\n\n\tmutex_lock(&net->packet.sklist_lock);\n\tsk_for_each(sk, &net->packet.sklist) {\n\t\tif (!net_eq(sock_net(sk), net))\n\t\t\tcontinue;\n\t\tif (num < s_num)\n\t\t\tgoto next;\n\n\t\tif (sk_diag_fill(sk, skb, req,\n\t\t\t\t may_report_filterinfo,\n\t\t\t\t sk_user_ns(NETLINK_CB(cb->skb).sk),\n\t\t\t\t NETLINK_CB(cb->skb).portid,\n\t\t\t\t cb->nlh->nlmsg_seq, NLM_F_MULTI,\n\t\t\t\t sock_i_ino(sk)) < 0)\n\t\t\tgoto done;\nnext:\n\t\tnum++;\n\t}\ndone:\n\tmutex_unlock(&net->packet.sklist_lock);\n\tcb->args[0] = num;\n\n\treturn skb->len;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,7 +8,7 @@\n \n \tnet = sock_net(skb->sk);\n \treq = nlmsg_data(cb->nlh);\n-\tmay_report_filterinfo = ns_capable(net->user_ns, CAP_NET_ADMIN);\n+\tmay_report_filterinfo = netlink_net_capable(cb->skb, CAP_NET_ADMIN);\n \n \tmutex_lock(&net->packet.sklist_lock);\n \tsk_for_each(sk, &net->packet.sklist) {",
        "function_modified_lines": {
            "added": [
                "\tmay_report_filterinfo = netlink_net_capable(cb->skb, CAP_NET_ADMIN);"
            ],
            "deleted": [
                "\tmay_report_filterinfo = ns_capable(net->user_ns, CAP_NET_ADMIN);"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Netlink implementation in the Linux kernel through 3.14.1 does not provide a mechanism for authorizing socket operations based on the opener of a socket, which allows local users to bypass intended access restrictions and modify network configurations by using a Netlink socket for the (1) stdout or (2) stderr of a setuid program.",
        "id": 451
    },
    {
        "cve_id": "CVE-2014-0181",
        "code_before_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
        "code_after_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,7 +6,7 @@\n \tstruct nlattr *attrs[RTA_MAX+1];\n \tint err;\n \n-\tif (!capable(CAP_NET_ADMIN))\n+\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n \t\treturn -EPERM;\n \n \tif (!net_eq(net, &init_net))",
        "function_modified_lines": {
            "added": [
                "\tif (!netlink_capable(skb, CAP_NET_ADMIN))"
            ],
            "deleted": [
                "\tif (!capable(CAP_NET_ADMIN))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Netlink implementation in the Linux kernel through 3.14.1 does not provide a mechanism for authorizing socket operations based on the opener of a socket, which allows local users to bypass intended access restrictions and modify network configurations by using a Netlink socket for the (1) stdout or (2) stderr of a setuid program.",
        "id": 446
    },
    {
        "cve_id": "CVE-2015-8709",
        "code_before_change": "static struct mm_struct *dup_mm(struct task_struct *tsk)\n{\n\tstruct mm_struct *mm, *oldmm = current->mm;\n\tint err;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\tgoto fail_nomem;\n\n\tmemcpy(mm, oldmm, sizeof(*mm));\n\n\tif (!mm_init(mm, tsk))\n\t\tgoto fail_nomem;\n\n\terr = dup_mmap(mm, oldmm);\n\tif (err)\n\t\tgoto free_pt;\n\n\tmm->hiwater_rss = get_mm_rss(mm);\n\tmm->hiwater_vm = mm->total_vm;\n\n\tif (mm->binfmt && !try_module_get(mm->binfmt->module))\n\t\tgoto free_pt;\n\n\treturn mm;\n\nfree_pt:\n\t/* don't put binfmt in mmput, we haven't got module yet */\n\tmm->binfmt = NULL;\n\tmmput(mm);\n\nfail_nomem:\n\treturn NULL;\n}",
        "code_after_change": "static struct mm_struct *dup_mm(struct task_struct *tsk)\n{\n\tstruct mm_struct *mm, *oldmm = current->mm;\n\tint err;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\tgoto fail_nomem;\n\n\tmemcpy(mm, oldmm, sizeof(*mm));\n\n\tif (!mm_init(mm, tsk, mm->user_ns))\n\t\tgoto fail_nomem;\n\n\terr = dup_mmap(mm, oldmm);\n\tif (err)\n\t\tgoto free_pt;\n\n\tmm->hiwater_rss = get_mm_rss(mm);\n\tmm->hiwater_vm = mm->total_vm;\n\n\tif (mm->binfmt && !try_module_get(mm->binfmt->module))\n\t\tgoto free_pt;\n\n\treturn mm;\n\nfree_pt:\n\t/* don't put binfmt in mmput, we haven't got module yet */\n\tmm->binfmt = NULL;\n\tmmput(mm);\n\nfail_nomem:\n\treturn NULL;\n}",
        "patch": "--- code before\n+++ code after\n@@ -9,7 +9,7 @@\n \n \tmemcpy(mm, oldmm, sizeof(*mm));\n \n-\tif (!mm_init(mm, tsk))\n+\tif (!mm_init(mm, tsk, mm->user_ns))\n \t\tgoto fail_nomem;\n \n \terr = dup_mmap(mm, oldmm);",
        "function_modified_lines": {
            "added": [
                "\tif (!mm_init(mm, tsk, mm->user_ns))"
            ],
            "deleted": [
                "\tif (!mm_init(mm, tsk))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "kernel/ptrace.c in the Linux kernel through 4.4.1 mishandles uid and gid mappings, which allows local users to gain privileges by establishing a user namespace, waiting for a root process to enter that namespace with an unsafe uid or gid, and then using the ptrace system call.  NOTE: the vendor states \"there is no kernel bug here.",
        "id": 837
    },
    {
        "cve_id": "CVE-2013-0914",
        "code_before_change": "void\nflush_signal_handlers(struct task_struct *t, int force_default)\n{\n\tint i;\n\tstruct k_sigaction *ka = &t->sighand->action[0];\n\tfor (i = _NSIG ; i != 0 ; i--) {\n\t\tif (force_default || ka->sa.sa_handler != SIG_IGN)\n\t\t\tka->sa.sa_handler = SIG_DFL;\n\t\tka->sa.sa_flags = 0;\n\t\tsigemptyset(&ka->sa.sa_mask);\n\t\tka++;\n\t}\n}",
        "code_after_change": "void\nflush_signal_handlers(struct task_struct *t, int force_default)\n{\n\tint i;\n\tstruct k_sigaction *ka = &t->sighand->action[0];\n\tfor (i = _NSIG ; i != 0 ; i--) {\n\t\tif (force_default || ka->sa.sa_handler != SIG_IGN)\n\t\t\tka->sa.sa_handler = SIG_DFL;\n\t\tka->sa.sa_flags = 0;\n#ifdef SA_RESTORER\n\t\tka->sa.sa_restorer = NULL;\n#endif\n\t\tsigemptyset(&ka->sa.sa_mask);\n\t\tka++;\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,9 @@\n \t\tif (force_default || ka->sa.sa_handler != SIG_IGN)\n \t\t\tka->sa.sa_handler = SIG_DFL;\n \t\tka->sa.sa_flags = 0;\n+#ifdef SA_RESTORER\n+\t\tka->sa.sa_restorer = NULL;\n+#endif\n \t\tsigemptyset(&ka->sa.sa_mask);\n \t\tka++;\n \t}",
        "function_modified_lines": {
            "added": [
                "#ifdef SA_RESTORER",
                "\t\tka->sa.sa_restorer = NULL;",
                "#endif"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The flush_signal_handlers function in kernel/signal.c in the Linux kernel before 3.8.4 preserves the value of the sa_restorer field across an exec operation, which makes it easier for local users to bypass the ASLR protection mechanism via a crafted application containing a sigaction system call.",
        "id": 167
    },
    {
        "cve_id": "CVE-2014-9888",
        "code_before_change": "void *arm_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,\n\t\t    gfp_t gfp, struct dma_attrs *attrs)\n{\n\tpgprot_t prot = __get_dma_pgprot(attrs, pgprot_kernel);\n\tvoid *memory;\n\n\tif (dma_alloc_from_coherent(dev, size, handle, &memory))\n\t\treturn memory;\n\n\treturn __dma_alloc(dev, size, handle, gfp, prot, false,\n\t\t\t   __builtin_return_address(0));\n}",
        "code_after_change": "void *arm_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,\n\t\t    gfp_t gfp, struct dma_attrs *attrs)\n{\n\tpgprot_t prot = __get_dma_pgprot(attrs, PAGE_KERNEL);\n\tvoid *memory;\n\n\tif (dma_alloc_from_coherent(dev, size, handle, &memory))\n\t\treturn memory;\n\n\treturn __dma_alloc(dev, size, handle, gfp, prot, false,\n\t\t\t   __builtin_return_address(0));\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,7 +1,7 @@\n void *arm_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,\n \t\t    gfp_t gfp, struct dma_attrs *attrs)\n {\n-\tpgprot_t prot = __get_dma_pgprot(attrs, pgprot_kernel);\n+\tpgprot_t prot = __get_dma_pgprot(attrs, PAGE_KERNEL);\n \tvoid *memory;\n \n \tif (dma_alloc_from_coherent(dev, size, handle, &memory))",
        "function_modified_lines": {
            "added": [
                "\tpgprot_t prot = __get_dma_pgprot(attrs, PAGE_KERNEL);"
            ],
            "deleted": [
                "\tpgprot_t prot = __get_dma_pgprot(attrs, pgprot_kernel);"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "arch/arm/mm/dma-mapping.c in the Linux kernel before 3.13 on ARM platforms, as used in Android before 2016-08-05 on Nexus 5 and 7 (2013) devices, does not prevent executable DMA mappings, which might allow local users to gain privileges via a crafted application, aka Android internal bug 28803642 and Qualcomm internal bug CR642735.",
        "id": 707
    },
    {
        "cve_id": "CVE-2014-4014",
        "code_before_change": "int inode_change_ok(const struct inode *inode, struct iattr *attr)\n{\n\tunsigned int ia_valid = attr->ia_valid;\n\n\t/*\n\t * First check size constraints.  These can't be overriden using\n\t * ATTR_FORCE.\n\t */\n\tif (ia_valid & ATTR_SIZE) {\n\t\tint error = inode_newsize_ok(inode, attr->ia_size);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\t/* If force is set do it anyway. */\n\tif (ia_valid & ATTR_FORCE)\n\t\treturn 0;\n\n\t/* Make sure a caller can chown. */\n\tif ((ia_valid & ATTR_UID) &&\n\t    (!uid_eq(current_fsuid(), inode->i_uid) ||\n\t     !uid_eq(attr->ia_uid, inode->i_uid)) &&\n\t    !inode_capable(inode, CAP_CHOWN))\n\t\treturn -EPERM;\n\n\t/* Make sure caller can chgrp. */\n\tif ((ia_valid & ATTR_GID) &&\n\t    (!uid_eq(current_fsuid(), inode->i_uid) ||\n\t    (!in_group_p(attr->ia_gid) && !gid_eq(attr->ia_gid, inode->i_gid))) &&\n\t    !inode_capable(inode, CAP_CHOWN))\n\t\treturn -EPERM;\n\n\t/* Make sure a caller can chmod. */\n\tif (ia_valid & ATTR_MODE) {\n\t\tif (!inode_owner_or_capable(inode))\n\t\t\treturn -EPERM;\n\t\t/* Also check the setgid bit! */\n\t\tif (!in_group_p((ia_valid & ATTR_GID) ? attr->ia_gid :\n\t\t\t\tinode->i_gid) &&\n\t\t    !inode_capable(inode, CAP_FSETID))\n\t\t\tattr->ia_mode &= ~S_ISGID;\n\t}\n\n\t/* Check for setting the inode time. */\n\tif (ia_valid & (ATTR_MTIME_SET | ATTR_ATIME_SET | ATTR_TIMES_SET)) {\n\t\tif (!inode_owner_or_capable(inode))\n\t\t\treturn -EPERM;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "int inode_change_ok(const struct inode *inode, struct iattr *attr)\n{\n\tunsigned int ia_valid = attr->ia_valid;\n\n\t/*\n\t * First check size constraints.  These can't be overriden using\n\t * ATTR_FORCE.\n\t */\n\tif (ia_valid & ATTR_SIZE) {\n\t\tint error = inode_newsize_ok(inode, attr->ia_size);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\t/* If force is set do it anyway. */\n\tif (ia_valid & ATTR_FORCE)\n\t\treturn 0;\n\n\t/* Make sure a caller can chown. */\n\tif ((ia_valid & ATTR_UID) &&\n\t    (!uid_eq(current_fsuid(), inode->i_uid) ||\n\t     !uid_eq(attr->ia_uid, inode->i_uid)) &&\n\t    !capable_wrt_inode_uidgid(inode, CAP_CHOWN))\n\t\treturn -EPERM;\n\n\t/* Make sure caller can chgrp. */\n\tif ((ia_valid & ATTR_GID) &&\n\t    (!uid_eq(current_fsuid(), inode->i_uid) ||\n\t    (!in_group_p(attr->ia_gid) && !gid_eq(attr->ia_gid, inode->i_gid))) &&\n\t    !capable_wrt_inode_uidgid(inode, CAP_CHOWN))\n\t\treturn -EPERM;\n\n\t/* Make sure a caller can chmod. */\n\tif (ia_valid & ATTR_MODE) {\n\t\tif (!inode_owner_or_capable(inode))\n\t\t\treturn -EPERM;\n\t\t/* Also check the setgid bit! */\n\t\tif (!in_group_p((ia_valid & ATTR_GID) ? attr->ia_gid :\n\t\t\t\tinode->i_gid) &&\n\t\t    !capable_wrt_inode_uidgid(inode, CAP_FSETID))\n\t\t\tattr->ia_mode &= ~S_ISGID;\n\t}\n\n\t/* Check for setting the inode time. */\n\tif (ia_valid & (ATTR_MTIME_SET | ATTR_ATIME_SET | ATTR_TIMES_SET)) {\n\t\tif (!inode_owner_or_capable(inode))\n\t\t\treturn -EPERM;\n\t}\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -20,14 +20,14 @@\n \tif ((ia_valid & ATTR_UID) &&\n \t    (!uid_eq(current_fsuid(), inode->i_uid) ||\n \t     !uid_eq(attr->ia_uid, inode->i_uid)) &&\n-\t    !inode_capable(inode, CAP_CHOWN))\n+\t    !capable_wrt_inode_uidgid(inode, CAP_CHOWN))\n \t\treturn -EPERM;\n \n \t/* Make sure caller can chgrp. */\n \tif ((ia_valid & ATTR_GID) &&\n \t    (!uid_eq(current_fsuid(), inode->i_uid) ||\n \t    (!in_group_p(attr->ia_gid) && !gid_eq(attr->ia_gid, inode->i_gid))) &&\n-\t    !inode_capable(inode, CAP_CHOWN))\n+\t    !capable_wrt_inode_uidgid(inode, CAP_CHOWN))\n \t\treturn -EPERM;\n \n \t/* Make sure a caller can chmod. */\n@@ -37,7 +37,7 @@\n \t\t/* Also check the setgid bit! */\n \t\tif (!in_group_p((ia_valid & ATTR_GID) ? attr->ia_gid :\n \t\t\t\tinode->i_gid) &&\n-\t\t    !inode_capable(inode, CAP_FSETID))\n+\t\t    !capable_wrt_inode_uidgid(inode, CAP_FSETID))\n \t\t\tattr->ia_mode &= ~S_ISGID;\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t    !capable_wrt_inode_uidgid(inode, CAP_CHOWN))",
                "\t    !capable_wrt_inode_uidgid(inode, CAP_CHOWN))",
                "\t\t    !capable_wrt_inode_uidgid(inode, CAP_FSETID))"
            ],
            "deleted": [
                "\t    !inode_capable(inode, CAP_CHOWN))",
                "\t    !inode_capable(inode, CAP_CHOWN))",
                "\t\t    !inode_capable(inode, CAP_FSETID))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The capabilities implementation in the Linux kernel before 3.14.8 does not properly consider that namespaces are inapplicable to inodes, which allows local users to bypass intended chmod restrictions by first creating a user namespace, as demonstrated by setting the setgid bit on a file with group ownership of root.",
        "id": 551
    },
    {
        "cve_id": "CVE-2013-4470",
        "code_before_change": "static inline int ip6_ufo_append_data(struct sock *sk,\n\t\t\tint getfrag(void *from, char *to, int offset, int len,\n\t\t\tint odd, struct sk_buff *skb),\n\t\t\tvoid *from, int length, int hh_len, int fragheaderlen,\n\t\t\tint transhdrlen, int mtu,unsigned int flags,\n\t\t\tstruct rt6_info *rt)\n\n{\n\tstruct sk_buff *skb;\n\tint err;\n\n\t/* There is support for UDP large send offload by network\n\t * device, so create one single skb packet containing complete\n\t * udp datagram\n\t */\n\tif ((skb = skb_peek_tail(&sk->sk_write_queue)) == NULL) {\n\t\tstruct frag_hdr fhdr;\n\n\t\tskb = sock_alloc_send_skb(sk,\n\t\t\thh_len + fragheaderlen + transhdrlen + 20,\n\t\t\t(flags & MSG_DONTWAIT), &err);\n\t\tif (skb == NULL)\n\t\t\treturn err;\n\n\t\t/* reserve space for Hardware header */\n\t\tskb_reserve(skb, hh_len);\n\n\t\t/* create space for UDP/IP header */\n\t\tskb_put(skb,fragheaderlen + transhdrlen);\n\n\t\t/* initialize network header pointer */\n\t\tskb_reset_network_header(skb);\n\n\t\t/* initialize protocol header pointer */\n\t\tskb->transport_header = skb->network_header + fragheaderlen;\n\n\t\tskb->protocol = htons(ETH_P_IPV6);\n\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\tskb->csum = 0;\n\n\t\t/* Specify the length of each IPv6 datagram fragment.\n\t\t * It has to be a multiple of 8.\n\t\t */\n\t\tskb_shinfo(skb)->gso_size = (mtu - fragheaderlen -\n\t\t\t\t\t     sizeof(struct frag_hdr)) & ~7;\n\t\tskb_shinfo(skb)->gso_type = SKB_GSO_UDP;\n\t\tipv6_select_ident(&fhdr, rt);\n\t\tskb_shinfo(skb)->ip6_frag_id = fhdr.identification;\n\t\t__skb_queue_tail(&sk->sk_write_queue, skb);\n\t}\n\n\treturn skb_append_datato_frags(sk, skb, getfrag, from,\n\t\t\t\t       (length - transhdrlen));\n}",
        "code_after_change": "static inline int ip6_ufo_append_data(struct sock *sk,\n\t\t\tint getfrag(void *from, char *to, int offset, int len,\n\t\t\tint odd, struct sk_buff *skb),\n\t\t\tvoid *from, int length, int hh_len, int fragheaderlen,\n\t\t\tint transhdrlen, int mtu,unsigned int flags,\n\t\t\tstruct rt6_info *rt)\n\n{\n\tstruct sk_buff *skb;\n\tstruct frag_hdr fhdr;\n\tint err;\n\n\t/* There is support for UDP large send offload by network\n\t * device, so create one single skb packet containing complete\n\t * udp datagram\n\t */\n\tif ((skb = skb_peek_tail(&sk->sk_write_queue)) == NULL) {\n\t\tskb = sock_alloc_send_skb(sk,\n\t\t\thh_len + fragheaderlen + transhdrlen + 20,\n\t\t\t(flags & MSG_DONTWAIT), &err);\n\t\tif (skb == NULL)\n\t\t\treturn err;\n\n\t\t/* reserve space for Hardware header */\n\t\tskb_reserve(skb, hh_len);\n\n\t\t/* create space for UDP/IP header */\n\t\tskb_put(skb,fragheaderlen + transhdrlen);\n\n\t\t/* initialize network header pointer */\n\t\tskb_reset_network_header(skb);\n\n\t\t/* initialize protocol header pointer */\n\t\tskb->transport_header = skb->network_header + fragheaderlen;\n\n\t\tskb->protocol = htons(ETH_P_IPV6);\n\t\tskb->csum = 0;\n\n\t\t__skb_queue_tail(&sk->sk_write_queue, skb);\n\t} else if (skb_is_gso(skb)) {\n\t\tgoto append;\n\t}\n\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t/* Specify the length of each IPv6 datagram fragment.\n\t * It has to be a multiple of 8.\n\t */\n\tskb_shinfo(skb)->gso_size = (mtu - fragheaderlen -\n\t\t\t\t     sizeof(struct frag_hdr)) & ~7;\n\tskb_shinfo(skb)->gso_type = SKB_GSO_UDP;\n\tipv6_select_ident(&fhdr, rt);\n\tskb_shinfo(skb)->ip6_frag_id = fhdr.identification;\n\nappend:\n\treturn skb_append_datato_frags(sk, skb, getfrag, from,\n\t\t\t\t       (length - transhdrlen));\n}",
        "patch": "--- code before\n+++ code after\n@@ -7,6 +7,7 @@\n \n {\n \tstruct sk_buff *skb;\n+\tstruct frag_hdr fhdr;\n \tint err;\n \n \t/* There is support for UDP large send offload by network\n@@ -14,8 +15,6 @@\n \t * udp datagram\n \t */\n \tif ((skb = skb_peek_tail(&sk->sk_write_queue)) == NULL) {\n-\t\tstruct frag_hdr fhdr;\n-\n \t\tskb = sock_alloc_send_skb(sk,\n \t\t\thh_len + fragheaderlen + transhdrlen + 20,\n \t\t\t(flags & MSG_DONTWAIT), &err);\n@@ -35,20 +34,24 @@\n \t\tskb->transport_header = skb->network_header + fragheaderlen;\n \n \t\tskb->protocol = htons(ETH_P_IPV6);\n-\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n \t\tskb->csum = 0;\n \n-\t\t/* Specify the length of each IPv6 datagram fragment.\n-\t\t * It has to be a multiple of 8.\n-\t\t */\n-\t\tskb_shinfo(skb)->gso_size = (mtu - fragheaderlen -\n-\t\t\t\t\t     sizeof(struct frag_hdr)) & ~7;\n-\t\tskb_shinfo(skb)->gso_type = SKB_GSO_UDP;\n-\t\tipv6_select_ident(&fhdr, rt);\n-\t\tskb_shinfo(skb)->ip6_frag_id = fhdr.identification;\n \t\t__skb_queue_tail(&sk->sk_write_queue, skb);\n+\t} else if (skb_is_gso(skb)) {\n+\t\tgoto append;\n \t}\n \n+\tskb->ip_summed = CHECKSUM_PARTIAL;\n+\t/* Specify the length of each IPv6 datagram fragment.\n+\t * It has to be a multiple of 8.\n+\t */\n+\tskb_shinfo(skb)->gso_size = (mtu - fragheaderlen -\n+\t\t\t\t     sizeof(struct frag_hdr)) & ~7;\n+\tskb_shinfo(skb)->gso_type = SKB_GSO_UDP;\n+\tipv6_select_ident(&fhdr, rt);\n+\tskb_shinfo(skb)->ip6_frag_id = fhdr.identification;\n+\n+append:\n \treturn skb_append_datato_frags(sk, skb, getfrag, from,\n \t\t\t\t       (length - transhdrlen));\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct frag_hdr fhdr;",
                "\t} else if (skb_is_gso(skb)) {",
                "\t\tgoto append;",
                "\tskb->ip_summed = CHECKSUM_PARTIAL;",
                "\t/* Specify the length of each IPv6 datagram fragment.",
                "\t * It has to be a multiple of 8.",
                "\t */",
                "\tskb_shinfo(skb)->gso_size = (mtu - fragheaderlen -",
                "\t\t\t\t     sizeof(struct frag_hdr)) & ~7;",
                "\tskb_shinfo(skb)->gso_type = SKB_GSO_UDP;",
                "\tipv6_select_ident(&fhdr, rt);",
                "\tskb_shinfo(skb)->ip6_frag_id = fhdr.identification;",
                "",
                "append:"
            ],
            "deleted": [
                "\t\tstruct frag_hdr fhdr;",
                "",
                "\t\tskb->ip_summed = CHECKSUM_PARTIAL;",
                "\t\t/* Specify the length of each IPv6 datagram fragment.",
                "\t\t * It has to be a multiple of 8.",
                "\t\t */",
                "\t\tskb_shinfo(skb)->gso_size = (mtu - fragheaderlen -",
                "\t\t\t\t\t     sizeof(struct frag_hdr)) & ~7;",
                "\t\tskb_shinfo(skb)->gso_type = SKB_GSO_UDP;",
                "\t\tipv6_select_ident(&fhdr, rt);",
                "\t\tskb_shinfo(skb)->ip6_frag_id = fhdr.identification;"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Linux kernel before 3.12, when UDP Fragmentation Offload (UFO) is enabled, does not properly initialize certain data structures, which allows local users to cause a denial of service (memory corruption and system crash) or possibly gain privileges via a crafted application that uses the UDP_CORK option in a setsockopt system call and sends both short and long packets, related to the ip_ufo_append_data function in net/ipv4/ip_output.c and the ip6_ufo_append_data function in net/ipv6/ip6_output.c.",
        "id": 303
    },
    {
        "cve_id": "CVE-2014-0181",
        "code_before_change": "static void\nscsi_nl_rcv_msg(struct sk_buff *skb)\n{\n\tstruct nlmsghdr *nlh;\n\tstruct scsi_nl_hdr *hdr;\n\tu32 rlen;\n\tint err, tport;\n\n\twhile (skb->len >= NLMSG_HDRLEN) {\n\t\terr = 0;\n\n\t\tnlh = nlmsg_hdr(skb);\n\t\tif ((nlh->nlmsg_len < (sizeof(*nlh) + sizeof(*hdr))) ||\n\t\t    (skb->len < nlh->nlmsg_len)) {\n\t\t\tprintk(KERN_WARNING \"%s: discarding partial skb\\n\",\n\t\t\t\t __func__);\n\t\t\treturn;\n\t\t}\n\n\t\trlen = NLMSG_ALIGN(nlh->nlmsg_len);\n\t\tif (rlen > skb->len)\n\t\t\trlen = skb->len;\n\n\t\tif (nlh->nlmsg_type != SCSI_TRANSPORT_MSG) {\n\t\t\terr = -EBADMSG;\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\thdr = nlmsg_data(nlh);\n\t\tif ((hdr->version != SCSI_NL_VERSION) ||\n\t\t    (hdr->magic != SCSI_NL_MAGIC)) {\n\t\t\terr = -EPROTOTYPE;\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\tif (!capable(CAP_SYS_ADMIN)) {\n\t\t\terr = -EPERM;\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\tif (nlh->nlmsg_len < (sizeof(*nlh) + hdr->msglen)) {\n\t\t\tprintk(KERN_WARNING \"%s: discarding partial message\\n\",\n\t\t\t\t __func__);\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\t/*\n\t\t * Deliver message to the appropriate transport\n\t\t */\n\t\ttport = hdr->transport;\n\t\tif (tport == SCSI_NL_TRANSPORT) {\n\t\t\tswitch (hdr->msgtype) {\n\t\t\tcase SCSI_NL_SHOST_VENDOR:\n\t\t\t\t/* Locate the driver that corresponds to the message */\n\t\t\t\terr = -ESRCH;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\terr = -EBADR;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (err)\n\t\t\t\tprintk(KERN_WARNING \"%s: Msgtype %d failed - err %d\\n\",\n\t\t\t\t       __func__, hdr->msgtype, err);\n\t\t}\n\t\telse\n\t\t\terr = -ENOENT;\n\nnext_msg:\n\t\tif ((err) || (nlh->nlmsg_flags & NLM_F_ACK))\n\t\t\tnetlink_ack(skb, nlh, err);\n\n\t\tskb_pull(skb, rlen);\n\t}\n}",
        "code_after_change": "static void\nscsi_nl_rcv_msg(struct sk_buff *skb)\n{\n\tstruct nlmsghdr *nlh;\n\tstruct scsi_nl_hdr *hdr;\n\tu32 rlen;\n\tint err, tport;\n\n\twhile (skb->len >= NLMSG_HDRLEN) {\n\t\terr = 0;\n\n\t\tnlh = nlmsg_hdr(skb);\n\t\tif ((nlh->nlmsg_len < (sizeof(*nlh) + sizeof(*hdr))) ||\n\t\t    (skb->len < nlh->nlmsg_len)) {\n\t\t\tprintk(KERN_WARNING \"%s: discarding partial skb\\n\",\n\t\t\t\t __func__);\n\t\t\treturn;\n\t\t}\n\n\t\trlen = NLMSG_ALIGN(nlh->nlmsg_len);\n\t\tif (rlen > skb->len)\n\t\t\trlen = skb->len;\n\n\t\tif (nlh->nlmsg_type != SCSI_TRANSPORT_MSG) {\n\t\t\terr = -EBADMSG;\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\thdr = nlmsg_data(nlh);\n\t\tif ((hdr->version != SCSI_NL_VERSION) ||\n\t\t    (hdr->magic != SCSI_NL_MAGIC)) {\n\t\t\terr = -EPROTOTYPE;\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\tif (!netlink_capable(skb, CAP_SYS_ADMIN)) {\n\t\t\terr = -EPERM;\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\tif (nlh->nlmsg_len < (sizeof(*nlh) + hdr->msglen)) {\n\t\t\tprintk(KERN_WARNING \"%s: discarding partial message\\n\",\n\t\t\t\t __func__);\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\t/*\n\t\t * Deliver message to the appropriate transport\n\t\t */\n\t\ttport = hdr->transport;\n\t\tif (tport == SCSI_NL_TRANSPORT) {\n\t\t\tswitch (hdr->msgtype) {\n\t\t\tcase SCSI_NL_SHOST_VENDOR:\n\t\t\t\t/* Locate the driver that corresponds to the message */\n\t\t\t\terr = -ESRCH;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\terr = -EBADR;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (err)\n\t\t\t\tprintk(KERN_WARNING \"%s: Msgtype %d failed - err %d\\n\",\n\t\t\t\t       __func__, hdr->msgtype, err);\n\t\t}\n\t\telse\n\t\t\terr = -ENOENT;\n\nnext_msg:\n\t\tif ((err) || (nlh->nlmsg_flags & NLM_F_ACK))\n\t\t\tnetlink_ack(skb, nlh, err);\n\n\t\tskb_pull(skb, rlen);\n\t}\n}",
        "patch": "--- code before\n+++ code after\n@@ -33,7 +33,7 @@\n \t\t\tgoto next_msg;\n \t\t}\n \n-\t\tif (!capable(CAP_SYS_ADMIN)) {\n+\t\tif (!netlink_capable(skb, CAP_SYS_ADMIN)) {\n \t\t\terr = -EPERM;\n \t\t\tgoto next_msg;\n \t\t}",
        "function_modified_lines": {
            "added": [
                "\t\tif (!netlink_capable(skb, CAP_SYS_ADMIN)) {"
            ],
            "deleted": [
                "\t\tif (!capable(CAP_SYS_ADMIN)) {"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Netlink implementation in the Linux kernel through 3.14.1 does not provide a mechanism for authorizing socket operations based on the opener of a socket, which allows local users to bypass intended access restrictions and modify network configurations by using a Netlink socket for the (1) stdout or (2) stderr of a setuid program.",
        "id": 435
    },
    {
        "cve_id": "CVE-2014-9922",
        "code_before_change": "static struct dentry *ecryptfs_mount(struct file_system_type *fs_type, int flags,\n\t\t\tconst char *dev_name, void *raw_data)\n{\n\tstruct super_block *s;\n\tstruct ecryptfs_sb_info *sbi;\n\tstruct ecryptfs_dentry_info *root_info;\n\tconst char *err = \"Getting sb failed\";\n\tstruct inode *inode;\n\tstruct path path;\n\tuid_t check_ruid;\n\tint rc;\n\n\tsbi = kmem_cache_zalloc(ecryptfs_sb_info_cache, GFP_KERNEL);\n\tif (!sbi) {\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trc = ecryptfs_parse_options(sbi, raw_data, &check_ruid);\n\tif (rc) {\n\t\terr = \"Error parsing options\";\n\t\tgoto out;\n\t}\n\n\ts = sget(fs_type, NULL, set_anon_super, flags, NULL);\n\tif (IS_ERR(s)) {\n\t\trc = PTR_ERR(s);\n\t\tgoto out;\n\t}\n\n\trc = bdi_setup_and_register(&sbi->bdi, \"ecryptfs\", BDI_CAP_MAP_COPY);\n\tif (rc)\n\t\tgoto out1;\n\n\tecryptfs_set_superblock_private(s, sbi);\n\ts->s_bdi = &sbi->bdi;\n\n\t/* ->kill_sb() will take care of sbi after that point */\n\tsbi = NULL;\n\ts->s_op = &ecryptfs_sops;\n\ts->s_d_op = &ecryptfs_dops;\n\n\terr = \"Reading sb failed\";\n\trc = kern_path(dev_name, LOOKUP_FOLLOW | LOOKUP_DIRECTORY, &path);\n\tif (rc) {\n\t\tecryptfs_printk(KERN_WARNING, \"kern_path() failed\\n\");\n\t\tgoto out1;\n\t}\n\tif (path.dentry->d_sb->s_type == &ecryptfs_fs_type) {\n\t\trc = -EINVAL;\n\t\tprintk(KERN_ERR \"Mount on filesystem of type \"\n\t\t\t\"eCryptfs explicitly disallowed due to \"\n\t\t\t\"known incompatibilities\\n\");\n\t\tgoto out_free;\n\t}\n\n\tif (check_ruid && !uid_eq(path.dentry->d_inode->i_uid, current_uid())) {\n\t\trc = -EPERM;\n\t\tprintk(KERN_ERR \"Mount of device (uid: %d) not owned by \"\n\t\t       \"requested user (uid: %d)\\n\",\n\t\t\ti_uid_read(path.dentry->d_inode),\n\t\t\tfrom_kuid(&init_user_ns, current_uid()));\n\t\tgoto out_free;\n\t}\n\n\tecryptfs_set_superblock_lower(s, path.dentry->d_sb);\n\n\t/**\n\t * Set the POSIX ACL flag based on whether they're enabled in the lower\n\t * mount. Force a read-only eCryptfs mount if the lower mount is ro.\n\t * Allow a ro eCryptfs mount even when the lower mount is rw.\n\t */\n\ts->s_flags = flags & ~MS_POSIXACL;\n\ts->s_flags |= path.dentry->d_sb->s_flags & (MS_RDONLY | MS_POSIXACL);\n\n\ts->s_maxbytes = path.dentry->d_sb->s_maxbytes;\n\ts->s_blocksize = path.dentry->d_sb->s_blocksize;\n\ts->s_magic = ECRYPTFS_SUPER_MAGIC;\n\n\tinode = ecryptfs_get_inode(path.dentry->d_inode, s);\n\trc = PTR_ERR(inode);\n\tif (IS_ERR(inode))\n\t\tgoto out_free;\n\n\ts->s_root = d_make_root(inode);\n\tif (!s->s_root) {\n\t\trc = -ENOMEM;\n\t\tgoto out_free;\n\t}\n\n\trc = -ENOMEM;\n\troot_info = kmem_cache_zalloc(ecryptfs_dentry_info_cache, GFP_KERNEL);\n\tif (!root_info)\n\t\tgoto out_free;\n\n\t/* ->kill_sb() will take care of root_info */\n\tecryptfs_set_dentry_private(s->s_root, root_info);\n\troot_info->lower_path = path;\n\n\ts->s_flags |= MS_ACTIVE;\n\treturn dget(s->s_root);\n\nout_free:\n\tpath_put(&path);\nout1:\n\tdeactivate_locked_super(s);\nout:\n\tif (sbi) {\n\t\tecryptfs_destroy_mount_crypt_stat(&sbi->mount_crypt_stat);\n\t\tkmem_cache_free(ecryptfs_sb_info_cache, sbi);\n\t}\n\tprintk(KERN_ERR \"%s; rc = [%d]\\n\", err, rc);\n\treturn ERR_PTR(rc);\n}",
        "code_after_change": "static struct dentry *ecryptfs_mount(struct file_system_type *fs_type, int flags,\n\t\t\tconst char *dev_name, void *raw_data)\n{\n\tstruct super_block *s;\n\tstruct ecryptfs_sb_info *sbi;\n\tstruct ecryptfs_dentry_info *root_info;\n\tconst char *err = \"Getting sb failed\";\n\tstruct inode *inode;\n\tstruct path path;\n\tuid_t check_ruid;\n\tint rc;\n\n\tsbi = kmem_cache_zalloc(ecryptfs_sb_info_cache, GFP_KERNEL);\n\tif (!sbi) {\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trc = ecryptfs_parse_options(sbi, raw_data, &check_ruid);\n\tif (rc) {\n\t\terr = \"Error parsing options\";\n\t\tgoto out;\n\t}\n\n\ts = sget(fs_type, NULL, set_anon_super, flags, NULL);\n\tif (IS_ERR(s)) {\n\t\trc = PTR_ERR(s);\n\t\tgoto out;\n\t}\n\n\trc = bdi_setup_and_register(&sbi->bdi, \"ecryptfs\", BDI_CAP_MAP_COPY);\n\tif (rc)\n\t\tgoto out1;\n\n\tecryptfs_set_superblock_private(s, sbi);\n\ts->s_bdi = &sbi->bdi;\n\n\t/* ->kill_sb() will take care of sbi after that point */\n\tsbi = NULL;\n\ts->s_op = &ecryptfs_sops;\n\ts->s_d_op = &ecryptfs_dops;\n\n\terr = \"Reading sb failed\";\n\trc = kern_path(dev_name, LOOKUP_FOLLOW | LOOKUP_DIRECTORY, &path);\n\tif (rc) {\n\t\tecryptfs_printk(KERN_WARNING, \"kern_path() failed\\n\");\n\t\tgoto out1;\n\t}\n\tif (path.dentry->d_sb->s_type == &ecryptfs_fs_type) {\n\t\trc = -EINVAL;\n\t\tprintk(KERN_ERR \"Mount on filesystem of type \"\n\t\t\t\"eCryptfs explicitly disallowed due to \"\n\t\t\t\"known incompatibilities\\n\");\n\t\tgoto out_free;\n\t}\n\n\tif (check_ruid && !uid_eq(path.dentry->d_inode->i_uid, current_uid())) {\n\t\trc = -EPERM;\n\t\tprintk(KERN_ERR \"Mount of device (uid: %d) not owned by \"\n\t\t       \"requested user (uid: %d)\\n\",\n\t\t\ti_uid_read(path.dentry->d_inode),\n\t\t\tfrom_kuid(&init_user_ns, current_uid()));\n\t\tgoto out_free;\n\t}\n\n\tecryptfs_set_superblock_lower(s, path.dentry->d_sb);\n\n\t/**\n\t * Set the POSIX ACL flag based on whether they're enabled in the lower\n\t * mount. Force a read-only eCryptfs mount if the lower mount is ro.\n\t * Allow a ro eCryptfs mount even when the lower mount is rw.\n\t */\n\ts->s_flags = flags & ~MS_POSIXACL;\n\ts->s_flags |= path.dentry->d_sb->s_flags & (MS_RDONLY | MS_POSIXACL);\n\n\ts->s_maxbytes = path.dentry->d_sb->s_maxbytes;\n\ts->s_blocksize = path.dentry->d_sb->s_blocksize;\n\ts->s_magic = ECRYPTFS_SUPER_MAGIC;\n\ts->s_stack_depth = path.dentry->d_sb->s_stack_depth + 1;\n\n\trc = -EINVAL;\n\tif (s->s_stack_depth > FILESYSTEM_MAX_STACK_DEPTH) {\n\t\tpr_err(\"eCryptfs: maximum fs stacking depth exceeded\\n\");\n\t\tgoto out_free;\n\t}\n\n\tinode = ecryptfs_get_inode(path.dentry->d_inode, s);\n\trc = PTR_ERR(inode);\n\tif (IS_ERR(inode))\n\t\tgoto out_free;\n\n\ts->s_root = d_make_root(inode);\n\tif (!s->s_root) {\n\t\trc = -ENOMEM;\n\t\tgoto out_free;\n\t}\n\n\trc = -ENOMEM;\n\troot_info = kmem_cache_zalloc(ecryptfs_dentry_info_cache, GFP_KERNEL);\n\tif (!root_info)\n\t\tgoto out_free;\n\n\t/* ->kill_sb() will take care of root_info */\n\tecryptfs_set_dentry_private(s->s_root, root_info);\n\troot_info->lower_path = path;\n\n\ts->s_flags |= MS_ACTIVE;\n\treturn dget(s->s_root);\n\nout_free:\n\tpath_put(&path);\nout1:\n\tdeactivate_locked_super(s);\nout:\n\tif (sbi) {\n\t\tecryptfs_destroy_mount_crypt_stat(&sbi->mount_crypt_stat);\n\t\tkmem_cache_free(ecryptfs_sb_info_cache, sbi);\n\t}\n\tprintk(KERN_ERR \"%s; rc = [%d]\\n\", err, rc);\n\treturn ERR_PTR(rc);\n}",
        "patch": "--- code before\n+++ code after\n@@ -76,6 +76,13 @@\n \ts->s_maxbytes = path.dentry->d_sb->s_maxbytes;\n \ts->s_blocksize = path.dentry->d_sb->s_blocksize;\n \ts->s_magic = ECRYPTFS_SUPER_MAGIC;\n+\ts->s_stack_depth = path.dentry->d_sb->s_stack_depth + 1;\n+\n+\trc = -EINVAL;\n+\tif (s->s_stack_depth > FILESYSTEM_MAX_STACK_DEPTH) {\n+\t\tpr_err(\"eCryptfs: maximum fs stacking depth exceeded\\n\");\n+\t\tgoto out_free;\n+\t}\n \n \tinode = ecryptfs_get_inode(path.dentry->d_inode, s);\n \trc = PTR_ERR(inode);",
        "function_modified_lines": {
            "added": [
                "\ts->s_stack_depth = path.dentry->d_sb->s_stack_depth + 1;",
                "",
                "\trc = -EINVAL;",
                "\tif (s->s_stack_depth > FILESYSTEM_MAX_STACK_DEPTH) {",
                "\t\tpr_err(\"eCryptfs: maximum fs stacking depth exceeded\\n\");",
                "\t\tgoto out_free;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The eCryptfs subsystem in the Linux kernel before 3.18 allows local users to gain privileges via a large filesystem stack that includes an overlayfs layer, related to fs/ecryptfs/main.c and fs/overlayfs/super.c.",
        "id": 712
    },
    {
        "cve_id": "CVE-2014-9870",
        "code_before_change": "asmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}",
        "code_after_change": "asmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value[0] = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -51,7 +51,7 @@\n \t\treturn regs->ARM_r0;\n \n \tcase NR(set_tls):\n-\t\tthread->tp_value = regs->ARM_r0;\n+\t\tthread->tp_value[0] = regs->ARM_r0;\n \t\tif (tls_emu)\n \t\t\treturn 0;\n \t\tif (has_tls_reg) {",
        "function_modified_lines": {
            "added": [
                "\t\tthread->tp_value[0] = regs->ARM_r0;"
            ],
            "deleted": [
                "\t\tthread->tp_value = regs->ARM_r0;"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Linux kernel before 3.11 on ARM platforms, as used in Android before 2016-08-05 on Nexus 5 and 7 (2013) devices, does not properly consider user-space access to the TPIDRURW register, which allows local users to gain privileges via a crafted application, aka Android internal bug 28749743 and Qualcomm internal bug CR561044.",
        "id": 704
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static int tcp_v6_send_synack(const struct sock *sk, struct dst_entry *dst,\n\t\t\t      struct flowi *fl,\n\t\t\t      struct request_sock *req,\n\t\t\t      struct tcp_fastopen_cookie *foc,\n\t\t\t      bool attach_req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct flowi6 *fl6 = &fl->u.ip6;\n\tstruct sk_buff *skb;\n\tint err = -ENOMEM;\n\n\t/* First, grab a route. */\n\tif (!dst && (dst = inet6_csk_route_req(sk, fl6, req,\n\t\t\t\t\t       IPPROTO_TCP)) == NULL)\n\t\tgoto done;\n\n\tskb = tcp_make_synack(sk, dst, req, foc, attach_req);\n\n\tif (skb) {\n\t\t__tcp_v6_send_check(skb, &ireq->ir_v6_loc_addr,\n\t\t\t\t    &ireq->ir_v6_rmt_addr);\n\n\t\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\t\tif (np->repflow && ireq->pktopts)\n\t\t\tfl6->flowlabel = ip6_flowlabel(ipv6_hdr(ireq->pktopts));\n\n\t\terr = ip6_xmit(sk, skb, fl6, np->opt, np->tclass);\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\treturn err;\n}",
        "code_after_change": "static int tcp_v6_send_synack(const struct sock *sk, struct dst_entry *dst,\n\t\t\t      struct flowi *fl,\n\t\t\t      struct request_sock *req,\n\t\t\t      struct tcp_fastopen_cookie *foc,\n\t\t\t      bool attach_req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct flowi6 *fl6 = &fl->u.ip6;\n\tstruct sk_buff *skb;\n\tint err = -ENOMEM;\n\n\t/* First, grab a route. */\n\tif (!dst && (dst = inet6_csk_route_req(sk, fl6, req,\n\t\t\t\t\t       IPPROTO_TCP)) == NULL)\n\t\tgoto done;\n\n\tskb = tcp_make_synack(sk, dst, req, foc, attach_req);\n\n\tif (skb) {\n\t\t__tcp_v6_send_check(skb, &ireq->ir_v6_loc_addr,\n\t\t\t\t    &ireq->ir_v6_rmt_addr);\n\n\t\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\t\tif (np->repflow && ireq->pktopts)\n\t\t\tfl6->flowlabel = ip6_flowlabel(ipv6_hdr(ireq->pktopts));\n\n\t\terr = ip6_xmit(sk, skb, fl6, rcu_dereference(np->opt),\n\t\t\t       np->tclass);\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -25,7 +25,8 @@\n \t\tif (np->repflow && ireq->pktopts)\n \t\t\tfl6->flowlabel = ip6_flowlabel(ipv6_hdr(ireq->pktopts));\n \n-\t\terr = ip6_xmit(sk, skb, fl6, np->opt, np->tclass);\n+\t\terr = ip6_xmit(sk, skb, fl6, rcu_dereference(np->opt),\n+\t\t\t       np->tclass);\n \t\terr = net_xmit_eval(err);\n \t}\n ",
        "function_modified_lines": {
            "added": [
                "\t\terr = ip6_xmit(sk, skb, fl6, rcu_dereference(np->opt),",
                "\t\t\t       np->tclass);"
            ],
            "deleted": [
                "\t\terr = ip6_xmit(sk, skb, fl6, np->opt, np->tclass);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1005
    },
    {
        "cve_id": "CVE-2013-1956",
        "code_before_change": "int create_user_ns(struct cred *new)\n{\n\tstruct user_namespace *ns, *parent_ns = new->user_ns;\n\tkuid_t owner = new->euid;\n\tkgid_t group = new->egid;\n\tint ret;\n\n\t/* The creator needs a mapping in the parent user namespace\n\t * or else we won't be able to reasonably tell userspace who\n\t * created a user_namespace.\n\t */\n\tif (!kuid_has_mapping(parent_ns, owner) ||\n\t    !kgid_has_mapping(parent_ns, group))\n\t\treturn -EPERM;\n\n\tns = kmem_cache_zalloc(user_ns_cachep, GFP_KERNEL);\n\tif (!ns)\n\t\treturn -ENOMEM;\n\n\tret = proc_alloc_inum(&ns->proc_inum);\n\tif (ret) {\n\t\tkmem_cache_free(user_ns_cachep, ns);\n\t\treturn ret;\n\t}\n\n\tatomic_set(&ns->count, 1);\n\t/* Leave the new->user_ns reference with the new user namespace. */\n\tns->parent = parent_ns;\n\tns->owner = owner;\n\tns->group = group;\n\n\tset_cred_user_ns(new, ns);\n\n\treturn 0;\n}",
        "code_after_change": "int create_user_ns(struct cred *new)\n{\n\tstruct user_namespace *ns, *parent_ns = new->user_ns;\n\tkuid_t owner = new->euid;\n\tkgid_t group = new->egid;\n\tint ret;\n\n\t/*\n\t * Verify that we can not violate the policy of which files\n\t * may be accessed that is specified by the root directory,\n\t * by verifing that the root directory is at the root of the\n\t * mount namespace which allows all files to be accessed.\n\t */\n\tif (current_chrooted())\n\t\treturn -EPERM;\n\n\t/* The creator needs a mapping in the parent user namespace\n\t * or else we won't be able to reasonably tell userspace who\n\t * created a user_namespace.\n\t */\n\tif (!kuid_has_mapping(parent_ns, owner) ||\n\t    !kgid_has_mapping(parent_ns, group))\n\t\treturn -EPERM;\n\n\tns = kmem_cache_zalloc(user_ns_cachep, GFP_KERNEL);\n\tif (!ns)\n\t\treturn -ENOMEM;\n\n\tret = proc_alloc_inum(&ns->proc_inum);\n\tif (ret) {\n\t\tkmem_cache_free(user_ns_cachep, ns);\n\t\treturn ret;\n\t}\n\n\tatomic_set(&ns->count, 1);\n\t/* Leave the new->user_ns reference with the new user namespace. */\n\tns->parent = parent_ns;\n\tns->owner = owner;\n\tns->group = group;\n\n\tset_cred_user_ns(new, ns);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,6 +4,15 @@\n \tkuid_t owner = new->euid;\n \tkgid_t group = new->egid;\n \tint ret;\n+\n+\t/*\n+\t * Verify that we can not violate the policy of which files\n+\t * may be accessed that is specified by the root directory,\n+\t * by verifing that the root directory is at the root of the\n+\t * mount namespace which allows all files to be accessed.\n+\t */\n+\tif (current_chrooted())\n+\t\treturn -EPERM;\n \n \t/* The creator needs a mapping in the parent user namespace\n \t * or else we won't be able to reasonably tell userspace who",
        "function_modified_lines": {
            "added": [
                "",
                "\t/*",
                "\t * Verify that we can not violate the policy of which files",
                "\t * may be accessed that is specified by the root directory,",
                "\t * by verifing that the root directory is at the root of the",
                "\t * mount namespace which allows all files to be accessed.",
                "\t */",
                "\tif (current_chrooted())",
                "\t\treturn -EPERM;"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The create_user_ns function in kernel/user_namespace.c in the Linux kernel before 3.8.6 does not check whether a chroot directory exists that differs from the namespace root directory, which allows local users to bypass intended filesystem restrictions via a crafted clone system call.",
        "id": 210
    },
    {
        "cve_id": "CVE-2016-9644",
        "code_before_change": "int __init early_fixup_exception(unsigned long *ip)\n{\n\tconst struct exception_table_entry *fixup;\n\tunsigned long new_ip;\n\n\tfixup = search_exception_tables(*ip);\n\tif (fixup) {\n\t\tnew_ip = ex_fixup_addr(fixup);\n\n\t\tif (fixup->fixup - fixup->insn >= 0x7ffffff0 - 4) {\n\t\t\t/* uaccess handling not supported during early boot */\n\t\t\treturn 0;\n\t\t}\n\n\t\t*ip = new_ip;\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}",
        "code_after_change": "int __init early_fixup_exception(unsigned long *ip)\n{\n\tconst struct exception_table_entry *e;\n\tunsigned long new_ip;\n\tex_handler_t handler;\n\n\te = search_exception_tables(*ip);\n\tif (!e)\n\t\treturn 0;\n\n\tnew_ip  = ex_fixup_addr(e);\n\thandler = ex_fixup_handler(e);\n\n\t/* special handling not supported during early boot */\n\tif (handler != ex_handler_default)\n\t\treturn 0;\n\n\t*ip = new_ip;\n\treturn 1;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,20 +1,20 @@\n int __init early_fixup_exception(unsigned long *ip)\n {\n-\tconst struct exception_table_entry *fixup;\n+\tconst struct exception_table_entry *e;\n \tunsigned long new_ip;\n+\tex_handler_t handler;\n \n-\tfixup = search_exception_tables(*ip);\n-\tif (fixup) {\n-\t\tnew_ip = ex_fixup_addr(fixup);\n+\te = search_exception_tables(*ip);\n+\tif (!e)\n+\t\treturn 0;\n \n-\t\tif (fixup->fixup - fixup->insn >= 0x7ffffff0 - 4) {\n-\t\t\t/* uaccess handling not supported during early boot */\n-\t\t\treturn 0;\n-\t\t}\n+\tnew_ip  = ex_fixup_addr(e);\n+\thandler = ex_fixup_handler(e);\n \n-\t\t*ip = new_ip;\n-\t\treturn 1;\n-\t}\n+\t/* special handling not supported during early boot */\n+\tif (handler != ex_handler_default)\n+\t\treturn 0;\n \n-\treturn 0;\n+\t*ip = new_ip;\n+\treturn 1;\n }",
        "function_modified_lines": {
            "added": [
                "\tconst struct exception_table_entry *e;",
                "\tex_handler_t handler;",
                "\te = search_exception_tables(*ip);",
                "\tif (!e)",
                "\t\treturn 0;",
                "\tnew_ip  = ex_fixup_addr(e);",
                "\thandler = ex_fixup_handler(e);",
                "\t/* special handling not supported during early boot */",
                "\tif (handler != ex_handler_default)",
                "\t\treturn 0;",
                "\t*ip = new_ip;",
                "\treturn 1;"
            ],
            "deleted": [
                "\tconst struct exception_table_entry *fixup;",
                "\tfixup = search_exception_tables(*ip);",
                "\tif (fixup) {",
                "\t\tnew_ip = ex_fixup_addr(fixup);",
                "\t\tif (fixup->fixup - fixup->insn >= 0x7ffffff0 - 4) {",
                "\t\t\t/* uaccess handling not supported during early boot */",
                "\t\t\treturn 0;",
                "\t\t}",
                "\t\t*ip = new_ip;",
                "\t\treturn 1;",
                "\t}",
                "\treturn 0;"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The __get_user_asm_ex macro in arch/x86/include/asm/uaccess.h in the Linux kernel 4.4.22 through 4.4.28 contains extended asm statements that are incompatible with the exception table, which allows local users to obtain root access on non-SMEP platforms via a crafted application.  NOTE: this vulnerability exists because of incorrect backporting of the CVE-2016-9178 patch to older kernels.",
        "id": 1155
    },
    {
        "cve_id": "CVE-2014-0181",
        "code_before_change": "static inline void dnrmg_receive_user_skb(struct sk_buff *skb)\n{\n\tstruct nlmsghdr *nlh = nlmsg_hdr(skb);\n\n\tif (nlh->nlmsg_len < sizeof(*nlh) || skb->len < nlh->nlmsg_len)\n\t\treturn;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\tRCV_SKB_FAIL(-EPERM);\n\n\t/* Eventually we might send routing messages too */\n\n\tRCV_SKB_FAIL(-EINVAL);\n}",
        "code_after_change": "static inline void dnrmg_receive_user_skb(struct sk_buff *skb)\n{\n\tstruct nlmsghdr *nlh = nlmsg_hdr(skb);\n\n\tif (nlh->nlmsg_len < sizeof(*nlh) || skb->len < nlh->nlmsg_len)\n\t\treturn;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\tRCV_SKB_FAIL(-EPERM);\n\n\t/* Eventually we might send routing messages too */\n\n\tRCV_SKB_FAIL(-EINVAL);\n}",
        "patch": "--- code before\n+++ code after\n@@ -5,7 +5,7 @@\n \tif (nlh->nlmsg_len < sizeof(*nlh) || skb->len < nlh->nlmsg_len)\n \t\treturn;\n \n-\tif (!capable(CAP_NET_ADMIN))\n+\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n \t\tRCV_SKB_FAIL(-EPERM);\n \n \t/* Eventually we might send routing messages too */",
        "function_modified_lines": {
            "added": [
                "\tif (!netlink_capable(skb, CAP_NET_ADMIN))"
            ],
            "deleted": [
                "\tif (!capable(CAP_NET_ADMIN))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Netlink implementation in the Linux kernel through 3.14.1 does not provide a mechanism for authorizing socket operations based on the opener of a socket, which allows local users to bypass intended access restrictions and modify network configurations by using a Netlink socket for the (1) stdout or (2) stderr of a setuid program.",
        "id": 448
    },
    {
        "cve_id": "CVE-2013-1858",
        "code_before_change": "static int userns_install(struct nsproxy *nsproxy, void *ns)\n{\n\tstruct user_namespace *user_ns = ns;\n\tstruct cred *cred;\n\n\t/* Don't allow gaining capabilities by reentering\n\t * the same user namespace.\n\t */\n\tif (user_ns == current_user_ns())\n\t\treturn -EINVAL;\n\n\t/* Threaded processes may not enter a different user namespace */\n\tif (atomic_read(&current->mm->mm_users) > 1)\n\t\treturn -EINVAL;\n\n\tif (!ns_capable(user_ns, CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tcred = prepare_creds();\n\tif (!cred)\n\t\treturn -ENOMEM;\n\n\tput_user_ns(cred->user_ns);\n\tset_cred_user_ns(cred, get_user_ns(user_ns));\n\n\treturn commit_creds(cred);\n}",
        "code_after_change": "static int userns_install(struct nsproxy *nsproxy, void *ns)\n{\n\tstruct user_namespace *user_ns = ns;\n\tstruct cred *cred;\n\n\t/* Don't allow gaining capabilities by reentering\n\t * the same user namespace.\n\t */\n\tif (user_ns == current_user_ns())\n\t\treturn -EINVAL;\n\n\t/* Threaded processes may not enter a different user namespace */\n\tif (atomic_read(&current->mm->mm_users) > 1)\n\t\treturn -EINVAL;\n\n\tif (current->fs->users != 1)\n\t\treturn -EINVAL;\n\n\tif (!ns_capable(user_ns, CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tcred = prepare_creds();\n\tif (!cred)\n\t\treturn -ENOMEM;\n\n\tput_user_ns(cred->user_ns);\n\tset_cred_user_ns(cred, get_user_ns(user_ns));\n\n\treturn commit_creds(cred);\n}",
        "patch": "--- code before\n+++ code after\n@@ -13,6 +13,9 @@\n \tif (atomic_read(&current->mm->mm_users) > 1)\n \t\treturn -EINVAL;\n \n+\tif (current->fs->users != 1)\n+\t\treturn -EINVAL;\n+\n \tif (!ns_capable(user_ns, CAP_SYS_ADMIN))\n \t\treturn -EPERM;\n ",
        "function_modified_lines": {
            "added": [
                "\tif (current->fs->users != 1)",
                "\t\treturn -EINVAL;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The clone system-call implementation in the Linux kernel before 3.8.3 does not properly handle a combination of the CLONE_NEWUSER and CLONE_FS flags, which allows local users to gain privileges by calling chroot and leveraging the sharing of the / directory between a parent process and a child process.",
        "id": 200
    },
    {
        "cve_id": "CVE-2016-3157",
        "code_before_change": "__visible __notrace_funcgraph struct task_struct *\n__switch_to(struct task_struct *prev_p, struct task_struct *next_p)\n{\n\tstruct thread_struct *prev = &prev_p->thread;\n\tstruct thread_struct *next = &next_p->thread;\n\tstruct fpu *prev_fpu = &prev->fpu;\n\tstruct fpu *next_fpu = &next->fpu;\n\tint cpu = smp_processor_id();\n\tstruct tss_struct *tss = &per_cpu(cpu_tss, cpu);\n\tunsigned fsindex, gsindex;\n\tfpu_switch_t fpu_switch;\n\n\tfpu_switch = switch_fpu_prepare(prev_fpu, next_fpu, cpu);\n\n\t/* We must save %fs and %gs before load_TLS() because\n\t * %fs and %gs may be cleared by load_TLS().\n\t *\n\t * (e.g. xen_load_tls())\n\t */\n\tsavesegment(fs, fsindex);\n\tsavesegment(gs, gsindex);\n\n\t/*\n\t * Load TLS before restoring any segments so that segment loads\n\t * reference the correct GDT entries.\n\t */\n\tload_TLS(next, cpu);\n\n\t/*\n\t * Leave lazy mode, flushing any hypercalls made here.  This\n\t * must be done after loading TLS entries in the GDT but before\n\t * loading segments that might reference them, and and it must\n\t * be done before fpu__restore(), so the TS bit is up to\n\t * date.\n\t */\n\tarch_end_context_switch(next_p);\n\n\t/* Switch DS and ES.\n\t *\n\t * Reading them only returns the selectors, but writing them (if\n\t * nonzero) loads the full descriptor from the GDT or LDT.  The\n\t * LDT for next is loaded in switch_mm, and the GDT is loaded\n\t * above.\n\t *\n\t * We therefore need to write new values to the segment\n\t * registers on every context switch unless both the new and old\n\t * values are zero.\n\t *\n\t * Note that we don't need to do anything for CS and SS, as\n\t * those are saved and restored as part of pt_regs.\n\t */\n\tsavesegment(es, prev->es);\n\tif (unlikely(next->es | prev->es))\n\t\tloadsegment(es, next->es);\n\n\tsavesegment(ds, prev->ds);\n\tif (unlikely(next->ds | prev->ds))\n\t\tloadsegment(ds, next->ds);\n\n\t/*\n\t * Switch FS and GS.\n\t *\n\t * These are even more complicated than DS and ES: they have\n\t * 64-bit bases are that controlled by arch_prctl.  Those bases\n\t * only differ from the values in the GDT or LDT if the selector\n\t * is 0.\n\t *\n\t * Loading the segment register resets the hidden base part of\n\t * the register to 0 or the value from the GDT / LDT.  If the\n\t * next base address zero, writing 0 to the segment register is\n\t * much faster than using wrmsr to explicitly zero the base.\n\t *\n\t * The thread_struct.fs and thread_struct.gs values are 0\n\t * if the fs and gs bases respectively are not overridden\n\t * from the values implied by fsindex and gsindex.  They\n\t * are nonzero, and store the nonzero base addresses, if\n\t * the bases are overridden.\n\t *\n\t * (fs != 0 && fsindex != 0) || (gs != 0 && gsindex != 0) should\n\t * be impossible.\n\t *\n\t * Therefore we need to reload the segment registers if either\n\t * the old or new selector is nonzero, and we need to override\n\t * the base address if next thread expects it to be overridden.\n\t *\n\t * This code is unnecessarily slow in the case where the old and\n\t * new indexes are zero and the new base is nonzero -- it will\n\t * unnecessarily write 0 to the selector before writing the new\n\t * base address.\n\t *\n\t * Note: This all depends on arch_prctl being the only way that\n\t * user code can override the segment base.  Once wrfsbase and\n\t * wrgsbase are enabled, most of this code will need to change.\n\t */\n\tif (unlikely(fsindex | next->fsindex | prev->fs)) {\n\t\tloadsegment(fs, next->fsindex);\n\n\t\t/*\n\t\t * If user code wrote a nonzero value to FS, then it also\n\t\t * cleared the overridden base address.\n\t\t *\n\t\t * XXX: if user code wrote 0 to FS and cleared the base\n\t\t * address itself, we won't notice and we'll incorrectly\n\t\t * restore the prior base address next time we reschdule\n\t\t * the process.\n\t\t */\n\t\tif (fsindex)\n\t\t\tprev->fs = 0;\n\t}\n\tif (next->fs)\n\t\twrmsrl(MSR_FS_BASE, next->fs);\n\tprev->fsindex = fsindex;\n\n\tif (unlikely(gsindex | next->gsindex | prev->gs)) {\n\t\tload_gs_index(next->gsindex);\n\n\t\t/* This works (and fails) the same way as fsindex above. */\n\t\tif (gsindex)\n\t\t\tprev->gs = 0;\n\t}\n\tif (next->gs)\n\t\twrmsrl(MSR_KERNEL_GS_BASE, next->gs);\n\tprev->gsindex = gsindex;\n\n\tswitch_fpu_finish(next_fpu, fpu_switch);\n\n\t/*\n\t * Switch the PDA and FPU contexts.\n\t */\n\tthis_cpu_write(current_task, next_p);\n\n\t/* Reload esp0 and ss1.  This changes current_thread_info(). */\n\tload_sp0(tss, next);\n\n\t/*\n\t * Now maybe reload the debug registers and handle I/O bitmaps\n\t */\n\tif (unlikely(task_thread_info(next_p)->flags & _TIF_WORK_CTXSW_NEXT ||\n\t\t     task_thread_info(prev_p)->flags & _TIF_WORK_CTXSW_PREV))\n\t\t__switch_to_xtra(prev_p, next_p, tss);\n\n\tif (static_cpu_has_bug(X86_BUG_SYSRET_SS_ATTRS)) {\n\t\t/*\n\t\t * AMD CPUs have a misfeature: SYSRET sets the SS selector but\n\t\t * does not update the cached descriptor.  As a result, if we\n\t\t * do SYSRET while SS is NULL, we'll end up in user mode with\n\t\t * SS apparently equal to __USER_DS but actually unusable.\n\t\t *\n\t\t * The straightforward workaround would be to fix it up just\n\t\t * before SYSRET, but that would slow down the system call\n\t\t * fast paths.  Instead, we ensure that SS is never NULL in\n\t\t * system call context.  We do this by replacing NULL SS\n\t\t * selectors at every context switch.  SYSCALL sets up a valid\n\t\t * SS, so the only way to get NULL is to re-enter the kernel\n\t\t * from CPL 3 through an interrupt.  Since that can't happen\n\t\t * in the same task as a running syscall, we are guaranteed to\n\t\t * context switch between every interrupt vector entry and a\n\t\t * subsequent SYSRET.\n\t\t *\n\t\t * We read SS first because SS reads are much faster than\n\t\t * writes.  Out of caution, we force SS to __KERNEL_DS even if\n\t\t * it previously had a different non-NULL value.\n\t\t */\n\t\tunsigned short ss_sel;\n\t\tsavesegment(ss, ss_sel);\n\t\tif (ss_sel != __KERNEL_DS)\n\t\t\tloadsegment(ss, __KERNEL_DS);\n\t}\n\n\treturn prev_p;\n}",
        "code_after_change": "__visible __notrace_funcgraph struct task_struct *\n__switch_to(struct task_struct *prev_p, struct task_struct *next_p)\n{\n\tstruct thread_struct *prev = &prev_p->thread;\n\tstruct thread_struct *next = &next_p->thread;\n\tstruct fpu *prev_fpu = &prev->fpu;\n\tstruct fpu *next_fpu = &next->fpu;\n\tint cpu = smp_processor_id();\n\tstruct tss_struct *tss = &per_cpu(cpu_tss, cpu);\n\tunsigned fsindex, gsindex;\n\tfpu_switch_t fpu_switch;\n\n\tfpu_switch = switch_fpu_prepare(prev_fpu, next_fpu, cpu);\n\n\t/* We must save %fs and %gs before load_TLS() because\n\t * %fs and %gs may be cleared by load_TLS().\n\t *\n\t * (e.g. xen_load_tls())\n\t */\n\tsavesegment(fs, fsindex);\n\tsavesegment(gs, gsindex);\n\n\t/*\n\t * Load TLS before restoring any segments so that segment loads\n\t * reference the correct GDT entries.\n\t */\n\tload_TLS(next, cpu);\n\n\t/*\n\t * Leave lazy mode, flushing any hypercalls made here.  This\n\t * must be done after loading TLS entries in the GDT but before\n\t * loading segments that might reference them, and and it must\n\t * be done before fpu__restore(), so the TS bit is up to\n\t * date.\n\t */\n\tarch_end_context_switch(next_p);\n\n\t/* Switch DS and ES.\n\t *\n\t * Reading them only returns the selectors, but writing them (if\n\t * nonzero) loads the full descriptor from the GDT or LDT.  The\n\t * LDT for next is loaded in switch_mm, and the GDT is loaded\n\t * above.\n\t *\n\t * We therefore need to write new values to the segment\n\t * registers on every context switch unless both the new and old\n\t * values are zero.\n\t *\n\t * Note that we don't need to do anything for CS and SS, as\n\t * those are saved and restored as part of pt_regs.\n\t */\n\tsavesegment(es, prev->es);\n\tif (unlikely(next->es | prev->es))\n\t\tloadsegment(es, next->es);\n\n\tsavesegment(ds, prev->ds);\n\tif (unlikely(next->ds | prev->ds))\n\t\tloadsegment(ds, next->ds);\n\n\t/*\n\t * Switch FS and GS.\n\t *\n\t * These are even more complicated than DS and ES: they have\n\t * 64-bit bases are that controlled by arch_prctl.  Those bases\n\t * only differ from the values in the GDT or LDT if the selector\n\t * is 0.\n\t *\n\t * Loading the segment register resets the hidden base part of\n\t * the register to 0 or the value from the GDT / LDT.  If the\n\t * next base address zero, writing 0 to the segment register is\n\t * much faster than using wrmsr to explicitly zero the base.\n\t *\n\t * The thread_struct.fs and thread_struct.gs values are 0\n\t * if the fs and gs bases respectively are not overridden\n\t * from the values implied by fsindex and gsindex.  They\n\t * are nonzero, and store the nonzero base addresses, if\n\t * the bases are overridden.\n\t *\n\t * (fs != 0 && fsindex != 0) || (gs != 0 && gsindex != 0) should\n\t * be impossible.\n\t *\n\t * Therefore we need to reload the segment registers if either\n\t * the old or new selector is nonzero, and we need to override\n\t * the base address if next thread expects it to be overridden.\n\t *\n\t * This code is unnecessarily slow in the case where the old and\n\t * new indexes are zero and the new base is nonzero -- it will\n\t * unnecessarily write 0 to the selector before writing the new\n\t * base address.\n\t *\n\t * Note: This all depends on arch_prctl being the only way that\n\t * user code can override the segment base.  Once wrfsbase and\n\t * wrgsbase are enabled, most of this code will need to change.\n\t */\n\tif (unlikely(fsindex | next->fsindex | prev->fs)) {\n\t\tloadsegment(fs, next->fsindex);\n\n\t\t/*\n\t\t * If user code wrote a nonzero value to FS, then it also\n\t\t * cleared the overridden base address.\n\t\t *\n\t\t * XXX: if user code wrote 0 to FS and cleared the base\n\t\t * address itself, we won't notice and we'll incorrectly\n\t\t * restore the prior base address next time we reschdule\n\t\t * the process.\n\t\t */\n\t\tif (fsindex)\n\t\t\tprev->fs = 0;\n\t}\n\tif (next->fs)\n\t\twrmsrl(MSR_FS_BASE, next->fs);\n\tprev->fsindex = fsindex;\n\n\tif (unlikely(gsindex | next->gsindex | prev->gs)) {\n\t\tload_gs_index(next->gsindex);\n\n\t\t/* This works (and fails) the same way as fsindex above. */\n\t\tif (gsindex)\n\t\t\tprev->gs = 0;\n\t}\n\tif (next->gs)\n\t\twrmsrl(MSR_KERNEL_GS_BASE, next->gs);\n\tprev->gsindex = gsindex;\n\n\tswitch_fpu_finish(next_fpu, fpu_switch);\n\n\t/*\n\t * Switch the PDA and FPU contexts.\n\t */\n\tthis_cpu_write(current_task, next_p);\n\n\t/* Reload esp0 and ss1.  This changes current_thread_info(). */\n\tload_sp0(tss, next);\n\n\t/*\n\t * Now maybe reload the debug registers and handle I/O bitmaps\n\t */\n\tif (unlikely(task_thread_info(next_p)->flags & _TIF_WORK_CTXSW_NEXT ||\n\t\t     task_thread_info(prev_p)->flags & _TIF_WORK_CTXSW_PREV))\n\t\t__switch_to_xtra(prev_p, next_p, tss);\n\n#ifdef CONFIG_XEN\n\t/*\n\t * On Xen PV, IOPL bits in pt_regs->flags have no effect, and\n\t * current_pt_regs()->flags may not match the current task's\n\t * intended IOPL.  We need to switch it manually.\n\t */\n\tif (unlikely(static_cpu_has(X86_FEATURE_XENPV) &&\n\t\t     prev->iopl != next->iopl))\n\t\txen_set_iopl_mask(next->iopl);\n#endif\n\n\tif (static_cpu_has_bug(X86_BUG_SYSRET_SS_ATTRS)) {\n\t\t/*\n\t\t * AMD CPUs have a misfeature: SYSRET sets the SS selector but\n\t\t * does not update the cached descriptor.  As a result, if we\n\t\t * do SYSRET while SS is NULL, we'll end up in user mode with\n\t\t * SS apparently equal to __USER_DS but actually unusable.\n\t\t *\n\t\t * The straightforward workaround would be to fix it up just\n\t\t * before SYSRET, but that would slow down the system call\n\t\t * fast paths.  Instead, we ensure that SS is never NULL in\n\t\t * system call context.  We do this by replacing NULL SS\n\t\t * selectors at every context switch.  SYSCALL sets up a valid\n\t\t * SS, so the only way to get NULL is to re-enter the kernel\n\t\t * from CPL 3 through an interrupt.  Since that can't happen\n\t\t * in the same task as a running syscall, we are guaranteed to\n\t\t * context switch between every interrupt vector entry and a\n\t\t * subsequent SYSRET.\n\t\t *\n\t\t * We read SS first because SS reads are much faster than\n\t\t * writes.  Out of caution, we force SS to __KERNEL_DS even if\n\t\t * it previously had a different non-NULL value.\n\t\t */\n\t\tunsigned short ss_sel;\n\t\tsavesegment(ss, ss_sel);\n\t\tif (ss_sel != __KERNEL_DS)\n\t\t\tloadsegment(ss, __KERNEL_DS);\n\t}\n\n\treturn prev_p;\n}",
        "patch": "--- code before\n+++ code after\n@@ -139,6 +139,17 @@\n \t\t     task_thread_info(prev_p)->flags & _TIF_WORK_CTXSW_PREV))\n \t\t__switch_to_xtra(prev_p, next_p, tss);\n \n+#ifdef CONFIG_XEN\n+\t/*\n+\t * On Xen PV, IOPL bits in pt_regs->flags have no effect, and\n+\t * current_pt_regs()->flags may not match the current task's\n+\t * intended IOPL.  We need to switch it manually.\n+\t */\n+\tif (unlikely(static_cpu_has(X86_FEATURE_XENPV) &&\n+\t\t     prev->iopl != next->iopl))\n+\t\txen_set_iopl_mask(next->iopl);\n+#endif\n+\n \tif (static_cpu_has_bug(X86_BUG_SYSRET_SS_ATTRS)) {\n \t\t/*\n \t\t * AMD CPUs have a misfeature: SYSRET sets the SS selector but",
        "function_modified_lines": {
            "added": [
                "#ifdef CONFIG_XEN",
                "\t/*",
                "\t * On Xen PV, IOPL bits in pt_regs->flags have no effect, and",
                "\t * current_pt_regs()->flags may not match the current task's",
                "\t * intended IOPL.  We need to switch it manually.",
                "\t */",
                "\tif (unlikely(static_cpu_has(X86_FEATURE_XENPV) &&",
                "\t\t     prev->iopl != next->iopl))",
                "\t\txen_set_iopl_mask(next->iopl);",
                "#endif",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The __switch_to function in arch/x86/kernel/process_64.c in the Linux kernel does not properly context-switch IOPL on 64-bit PV Xen guests, which allows local guest OS users to gain privileges, cause a denial of service (guest OS crash), or obtain sensitive information by leveraging I/O port access.",
        "id": 985
    },
    {
        "cve_id": "CVE-2016-3841",
        "code_before_change": "static int do_ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, int __user *optlen, unsigned int flags)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tint len;\n\tint val;\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_getsockopt(sk, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tswitch (optname) {\n\tcase IPV6_ADDRFORM:\n\t\tif (sk->sk_protocol != IPPROTO_UDP &&\n\t\t    sk->sk_protocol != IPPROTO_UDPLITE &&\n\t\t    sk->sk_protocol != IPPROTO_TCP)\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -ENOTCONN;\n\t\tval = sk->sk_family;\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter gsf;\n\t\tint err;\n\n\t\tif (len < GROUP_FILTER_SIZE(0))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&gsf, optval, GROUP_FILTER_SIZE(0)))\n\t\t\treturn -EFAULT;\n\t\tif (gsf.gf_group.ss_family != AF_INET6)\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tlock_sock(sk);\n\t\terr = ip6_mc_msfget(sk, &gsf,\n\t\t\t(struct group_filter __user *)optval, optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct msghdr msg;\n\t\tstruct sk_buff *skb;\n\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\treturn -ENOPROTOOPT;\n\n\t\tmsg.msg_control = optval;\n\t\tmsg.msg_controllen = len;\n\t\tmsg.msg_flags = flags;\n\n\t\tlock_sock(sk);\n\t\tskb = np->pktoptions;\n\t\tif (skb)\n\t\t\tip6_datagram_recv_ctl(sk, &msg, skb);\n\t\trelease_sock(sk);\n\t\tif (!skb) {\n\t\t\tif (np->rxopt.bits.rxinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr : np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxhlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxtclass) {\n\t\t\t\tint tclass = (int)ip6_tclass(np->rcv_flowinfo);\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_TCLASS, sizeof(tclass), &tclass);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxoinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr :\n\t\t\t\t\t\t\t\t     np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxohlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxflow) {\n\t\t\t\t__be32 flowinfo = np->rcv_flowinfo;\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_FLOWINFO, sizeof(flowinfo), &flowinfo);\n\t\t\t}\n\t\t}\n\t\tlen -= msg.msg_controllen;\n\t\treturn put_user(len, optlen);\n\t}\n\tcase IPV6_MTU:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tval = 0;\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tval = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!val)\n\t\t\treturn -ENOTCONN;\n\t\tbreak;\n\t}\n\n\tcase IPV6_V6ONLY:\n\t\tval = sk->sk_ipv6only;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tval = np->rxopt.bits.rxinfo;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tval = np->rxopt.bits.rxoinfo;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tval = np->rxopt.bits.rxhlim;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tval = np->rxopt.bits.rxohlim;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tval = np->rxopt.bits.srcrt;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tval = np->rxopt.bits.osrcrt;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\n\t\tlock_sock(sk);\n\t\tlen = ipv6_getsockopt_sticky(sk, np->opt,\n\t\t\t\t\t     optname, optval, len);\n\t\trelease_sock(sk);\n\t\t/* check if ipv6_getsockopt_sticky() returns err code */\n\t\tif (len < 0)\n\t\t\treturn len;\n\t\treturn put_user(len, optlen);\n\t}\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tval = np->rxopt.bits.hopopts;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tval = np->rxopt.bits.ohopopts;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tval = np->rxopt.bits.dstopts;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tval = np->rxopt.bits.odstopts;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tval = np->tclass;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tval = np->rxopt.bits.rxtclass;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tval = np->rxopt.bits.rxflow;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tval = np->rxopt.bits.rxpmtu;\n\t\tbreak;\n\n\tcase IPV6_PATHMTU:\n\t{\n\t\tstruct dst_entry *dst;\n\t\tstruct ip6_mtuinfo mtuinfo;\n\n\t\tif (len < sizeof(mtuinfo))\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(mtuinfo);\n\t\tmemset(&mtuinfo, 0, sizeof(mtuinfo));\n\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tmtuinfo.ip6m_mtu = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!mtuinfo.ip6m_mtu)\n\t\t\treturn -ENOTCONN;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &mtuinfo, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_TRANSPARENT:\n\t\tval = inet_sk(sk)->transparent;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tval = np->rxopt.bits.rxorigdstaddr;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_HOPS:\n\tcase IPV6_MULTICAST_HOPS:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tif (optname == IPV6_UNICAST_HOPS)\n\t\t\tval = np->hop_limit;\n\t\telse\n\t\t\tval = np->mcast_hops;\n\n\t\tif (val < 0) {\n\t\t\trcu_read_lock();\n\t\t\tdst = __sk_dst_get(sk);\n\t\t\tif (dst)\n\t\t\t\tval = ip6_dst_hoplimit(dst);\n\t\t\trcu_read_unlock();\n\t\t}\n\n\t\tif (val < 0)\n\t\t\tval = sock_net(sk)->ipv6.devconf_all->hop_limit;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tval = np->mc_loop;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_IF:\n\t\tval = np->mcast_oif;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t\tval = (__force int)htonl((__u32) np->ucast_oif);\n\t\tbreak;\n\n\tcase IPV6_MTU_DISCOVER:\n\t\tval = np->pmtudisc;\n\t\tbreak;\n\n\tcase IPV6_RECVERR:\n\t\tval = np->recverr;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO_SEND:\n\t\tval = np->sndflow;\n\t\tbreak;\n\n\tcase IPV6_FLOWLABEL_MGR:\n\t{\n\t\tstruct in6_flowlabel_req freq;\n\t\tint flags;\n\n\t\tif (len < sizeof(freq))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_user(&freq, optval, sizeof(freq)))\n\t\t\treturn -EFAULT;\n\n\t\tif (freq.flr_action != IPV6_FL_A_GET)\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(freq);\n\t\tflags = freq.flr_flags;\n\n\t\tmemset(&freq, 0, sizeof(freq));\n\n\t\tval = ipv6_flowlabel_opt_get(sk, &freq, flags);\n\t\tif (val < 0)\n\t\t\treturn val;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &freq, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tval = 0;\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_TMP)\n\t\t\tval |= IPV6_PREFER_SRC_TMP;\n\t\telse if (np->srcprefs & IPV6_PREFER_SRC_PUBLIC)\n\t\t\tval |= IPV6_PREFER_SRC_PUBLIC;\n\t\telse {\n\t\t\t/* XXX: should we return system default? */\n\t\t\tval |= IPV6_PREFER_SRC_PUBTMP_DEFAULT;\n\t\t}\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_COA)\n\t\t\tval |= IPV6_PREFER_SRC_COA;\n\t\telse\n\t\t\tval |= IPV6_PREFER_SRC_HOME;\n\t\tbreak;\n\n\tcase IPV6_MINHOPCOUNT:\n\t\tval = np->min_hopcount;\n\t\tbreak;\n\n\tcase IPV6_DONTFRAG:\n\t\tval = np->dontfrag;\n\t\tbreak;\n\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tval = np->autoflowlabel;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\tlen = min_t(unsigned int, sizeof(int), len);\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
        "code_after_change": "static int do_ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, int __user *optlen, unsigned int flags)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tint len;\n\tint val;\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_getsockopt(sk, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tswitch (optname) {\n\tcase IPV6_ADDRFORM:\n\t\tif (sk->sk_protocol != IPPROTO_UDP &&\n\t\t    sk->sk_protocol != IPPROTO_UDPLITE &&\n\t\t    sk->sk_protocol != IPPROTO_TCP)\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -ENOTCONN;\n\t\tval = sk->sk_family;\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter gsf;\n\t\tint err;\n\n\t\tif (len < GROUP_FILTER_SIZE(0))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&gsf, optval, GROUP_FILTER_SIZE(0)))\n\t\t\treturn -EFAULT;\n\t\tif (gsf.gf_group.ss_family != AF_INET6)\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tlock_sock(sk);\n\t\terr = ip6_mc_msfget(sk, &gsf,\n\t\t\t(struct group_filter __user *)optval, optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct msghdr msg;\n\t\tstruct sk_buff *skb;\n\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\treturn -ENOPROTOOPT;\n\n\t\tmsg.msg_control = optval;\n\t\tmsg.msg_controllen = len;\n\t\tmsg.msg_flags = flags;\n\n\t\tlock_sock(sk);\n\t\tskb = np->pktoptions;\n\t\tif (skb)\n\t\t\tip6_datagram_recv_ctl(sk, &msg, skb);\n\t\trelease_sock(sk);\n\t\tif (!skb) {\n\t\t\tif (np->rxopt.bits.rxinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr : np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxhlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxtclass) {\n\t\t\t\tint tclass = (int)ip6_tclass(np->rcv_flowinfo);\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_TCLASS, sizeof(tclass), &tclass);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxoinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr :\n\t\t\t\t\t\t\t\t     np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxohlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxflow) {\n\t\t\t\t__be32 flowinfo = np->rcv_flowinfo;\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_FLOWINFO, sizeof(flowinfo), &flowinfo);\n\t\t\t}\n\t\t}\n\t\tlen -= msg.msg_controllen;\n\t\treturn put_user(len, optlen);\n\t}\n\tcase IPV6_MTU:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tval = 0;\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tval = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!val)\n\t\t\treturn -ENOTCONN;\n\t\tbreak;\n\t}\n\n\tcase IPV6_V6ONLY:\n\t\tval = sk->sk_ipv6only;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tval = np->rxopt.bits.rxinfo;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tval = np->rxopt.bits.rxoinfo;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tval = np->rxopt.bits.rxhlim;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tval = np->rxopt.bits.rxohlim;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tval = np->rxopt.bits.srcrt;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tval = np->rxopt.bits.osrcrt;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\tlock_sock(sk);\n\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\t\tlen = ipv6_getsockopt_sticky(sk, opt, optname, optval, len);\n\t\trelease_sock(sk);\n\t\t/* check if ipv6_getsockopt_sticky() returns err code */\n\t\tif (len < 0)\n\t\t\treturn len;\n\t\treturn put_user(len, optlen);\n\t}\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tval = np->rxopt.bits.hopopts;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tval = np->rxopt.bits.ohopopts;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tval = np->rxopt.bits.dstopts;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tval = np->rxopt.bits.odstopts;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tval = np->tclass;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tval = np->rxopt.bits.rxtclass;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tval = np->rxopt.bits.rxflow;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tval = np->rxopt.bits.rxpmtu;\n\t\tbreak;\n\n\tcase IPV6_PATHMTU:\n\t{\n\t\tstruct dst_entry *dst;\n\t\tstruct ip6_mtuinfo mtuinfo;\n\n\t\tif (len < sizeof(mtuinfo))\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(mtuinfo);\n\t\tmemset(&mtuinfo, 0, sizeof(mtuinfo));\n\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tmtuinfo.ip6m_mtu = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!mtuinfo.ip6m_mtu)\n\t\t\treturn -ENOTCONN;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &mtuinfo, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_TRANSPARENT:\n\t\tval = inet_sk(sk)->transparent;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tval = np->rxopt.bits.rxorigdstaddr;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_HOPS:\n\tcase IPV6_MULTICAST_HOPS:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tif (optname == IPV6_UNICAST_HOPS)\n\t\t\tval = np->hop_limit;\n\t\telse\n\t\t\tval = np->mcast_hops;\n\n\t\tif (val < 0) {\n\t\t\trcu_read_lock();\n\t\t\tdst = __sk_dst_get(sk);\n\t\t\tif (dst)\n\t\t\t\tval = ip6_dst_hoplimit(dst);\n\t\t\trcu_read_unlock();\n\t\t}\n\n\t\tif (val < 0)\n\t\t\tval = sock_net(sk)->ipv6.devconf_all->hop_limit;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tval = np->mc_loop;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_IF:\n\t\tval = np->mcast_oif;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t\tval = (__force int)htonl((__u32) np->ucast_oif);\n\t\tbreak;\n\n\tcase IPV6_MTU_DISCOVER:\n\t\tval = np->pmtudisc;\n\t\tbreak;\n\n\tcase IPV6_RECVERR:\n\t\tval = np->recverr;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO_SEND:\n\t\tval = np->sndflow;\n\t\tbreak;\n\n\tcase IPV6_FLOWLABEL_MGR:\n\t{\n\t\tstruct in6_flowlabel_req freq;\n\t\tint flags;\n\n\t\tif (len < sizeof(freq))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_user(&freq, optval, sizeof(freq)))\n\t\t\treturn -EFAULT;\n\n\t\tif (freq.flr_action != IPV6_FL_A_GET)\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(freq);\n\t\tflags = freq.flr_flags;\n\n\t\tmemset(&freq, 0, sizeof(freq));\n\n\t\tval = ipv6_flowlabel_opt_get(sk, &freq, flags);\n\t\tif (val < 0)\n\t\t\treturn val;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &freq, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tval = 0;\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_TMP)\n\t\t\tval |= IPV6_PREFER_SRC_TMP;\n\t\telse if (np->srcprefs & IPV6_PREFER_SRC_PUBLIC)\n\t\t\tval |= IPV6_PREFER_SRC_PUBLIC;\n\t\telse {\n\t\t\t/* XXX: should we return system default? */\n\t\t\tval |= IPV6_PREFER_SRC_PUBTMP_DEFAULT;\n\t\t}\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_COA)\n\t\t\tval |= IPV6_PREFER_SRC_COA;\n\t\telse\n\t\t\tval |= IPV6_PREFER_SRC_HOME;\n\t\tbreak;\n\n\tcase IPV6_MINHOPCOUNT:\n\t\tval = np->min_hopcount;\n\t\tbreak;\n\n\tcase IPV6_DONTFRAG:\n\t\tval = np->dontfrag;\n\t\tbreak;\n\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tval = np->autoflowlabel;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\tlen = min_t(unsigned int, sizeof(int), len);\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -141,10 +141,11 @@\n \tcase IPV6_RTHDR:\n \tcase IPV6_DSTOPTS:\n \t{\n+\t\tstruct ipv6_txoptions *opt;\n \n \t\tlock_sock(sk);\n-\t\tlen = ipv6_getsockopt_sticky(sk, np->opt,\n-\t\t\t\t\t     optname, optval, len);\n+\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n+\t\tlen = ipv6_getsockopt_sticky(sk, opt, optname, optval, len);\n \t\trelease_sock(sk);\n \t\t/* check if ipv6_getsockopt_sticky() returns err code */\n \t\tif (len < 0)",
        "function_modified_lines": {
            "added": [
                "\t\tstruct ipv6_txoptions *opt;",
                "\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));",
                "\t\tlen = ipv6_getsockopt_sticky(sk, opt, optname, optval, len);"
            ],
            "deleted": [
                "\t\tlen = ipv6_getsockopt_sticky(sk, np->opt,",
                "\t\t\t\t\t     optname, optval, len);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.",
        "id": 1002
    },
    {
        "cve_id": "CVE-2016-9120",
        "code_before_change": "void ion_free(struct ion_client *client, struct ion_handle *handle)\n{\n\tbool valid_handle;\n\n\tBUG_ON(client != handle->client);\n\n\tmutex_lock(&client->lock);\n\tvalid_handle = ion_handle_validate(client, handle);\n\n\tif (!valid_handle) {\n\t\tWARN(1, \"%s: invalid handle passed to free.\\n\", __func__);\n\t\tmutex_unlock(&client->lock);\n\t\treturn;\n\t}\n\tmutex_unlock(&client->lock);\n\tion_handle_put(handle);\n}",
        "code_after_change": "void ion_free(struct ion_client *client, struct ion_handle *handle)\n{\n\tBUG_ON(client != handle->client);\n\n\tmutex_lock(&client->lock);\n\tion_free_nolock(client, handle);\n\tmutex_unlock(&client->lock);\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,17 +1,8 @@\n void ion_free(struct ion_client *client, struct ion_handle *handle)\n {\n-\tbool valid_handle;\n-\n \tBUG_ON(client != handle->client);\n \n \tmutex_lock(&client->lock);\n-\tvalid_handle = ion_handle_validate(client, handle);\n-\n-\tif (!valid_handle) {\n-\t\tWARN(1, \"%s: invalid handle passed to free.\\n\", __func__);\n-\t\tmutex_unlock(&client->lock);\n-\t\treturn;\n-\t}\n+\tion_free_nolock(client, handle);\n \tmutex_unlock(&client->lock);\n-\tion_handle_put(handle);\n }",
        "function_modified_lines": {
            "added": [
                "\tion_free_nolock(client, handle);"
            ],
            "deleted": [
                "\tbool valid_handle;",
                "",
                "\tvalid_handle = ion_handle_validate(client, handle);",
                "",
                "\tif (!valid_handle) {",
                "\t\tWARN(1, \"%s: invalid handle passed to free.\\n\", __func__);",
                "\t\tmutex_unlock(&client->lock);",
                "\t\treturn;",
                "\t}",
                "\tion_handle_put(handle);"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-416"
        ],
        "cve_description": "Race condition in the ion_ioctl function in drivers/staging/android/ion/ion.c in the Linux kernel before 4.6 allows local users to gain privileges or cause a denial of service (use-after-free) by calling ION_IOC_FREE on two CPUs at the same time.",
        "id": 1139
    },
    {
        "cve_id": "CVE-2016-6786",
        "code_before_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tstruct perf_event_context *gctx = group_leader->ctx;\n\n\t\tmutex_lock(&gctx->mutex);\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\n\tif (move_group) {\n\t\tsynchronize_rcu();\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
        "code_after_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\tif (move_group) {\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group) {\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -6,7 +6,7 @@\n \tstruct perf_event *group_leader = NULL, *output_event = NULL;\n \tstruct perf_event *event, *sibling;\n \tstruct perf_event_attr attr;\n-\tstruct perf_event_context *ctx;\n+\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n \tstruct file *event_file = NULL;\n \tstruct fd group = {NULL, 0};\n \tstruct task_struct *task = NULL;\n@@ -204,9 +204,14 @@\n \t}\n \n \tif (move_group) {\n-\t\tstruct perf_event_context *gctx = group_leader->ctx;\n-\n-\t\tmutex_lock(&gctx->mutex);\n+\t\tgctx = group_leader->ctx;\n+\n+\t\t/*\n+\t\t * See perf_event_ctx_lock() for comments on the details\n+\t\t * of swizzling perf_event::ctx.\n+\t\t */\n+\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n+\n \t\tperf_remove_from_context(group_leader, false);\n \n \t\t/*\n@@ -221,15 +226,19 @@\n \t\t\tperf_event__state_init(sibling);\n \t\t\tput_ctx(gctx);\n \t\t}\n-\t\tmutex_unlock(&gctx->mutex);\n-\t\tput_ctx(gctx);\n+\t} else {\n+\t\tmutex_lock(&ctx->mutex);\n \t}\n \n \tWARN_ON_ONCE(ctx->parent_ctx);\n-\tmutex_lock(&ctx->mutex);\n \n \tif (move_group) {\n+\t\t/*\n+\t\t * Wait for everybody to stop referencing the events through\n+\t\t * the old lists, before installing it on new lists.\n+\t\t */\n \t\tsynchronize_rcu();\n+\n \t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n \t\tget_ctx(ctx);\n \t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n@@ -241,6 +250,11 @@\n \n \tperf_install_in_context(ctx, event, event->cpu);\n \tperf_unpin_context(ctx);\n+\n+\tif (move_group) {\n+\t\tmutex_unlock(&gctx->mutex);\n+\t\tput_ctx(gctx);\n+\t}\n \tmutex_unlock(&ctx->mutex);\n \n \tput_online_cpus();",
        "function_modified_lines": {
            "added": [
                "\tstruct perf_event_context *ctx, *uninitialized_var(gctx);",
                "\t\tgctx = group_leader->ctx;",
                "",
                "\t\t/*",
                "\t\t * See perf_event_ctx_lock() for comments on the details",
                "\t\t * of swizzling perf_event::ctx.",
                "\t\t */",
                "\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);",
                "",
                "\t} else {",
                "\t\tmutex_lock(&ctx->mutex);",
                "\t\t/*",
                "\t\t * Wait for everybody to stop referencing the events through",
                "\t\t * the old lists, before installing it on new lists.",
                "\t\t */",
                "",
                "",
                "\tif (move_group) {",
                "\t\tmutex_unlock(&gctx->mutex);",
                "\t\tput_ctx(gctx);",
                "\t}"
            ],
            "deleted": [
                "\tstruct perf_event_context *ctx;",
                "\t\tstruct perf_event_context *gctx = group_leader->ctx;",
                "",
                "\t\tmutex_lock(&gctx->mutex);",
                "\t\tmutex_unlock(&gctx->mutex);",
                "\t\tput_ctx(gctx);",
                "\tmutex_lock(&ctx->mutex);"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "kernel/events/core.c in the performance subsystem in the Linux kernel before 4.0 mismanages locks during certain migrations, which allows local users to gain privileges via a crafted application, aka Android internal bug 30955111.",
        "id": 1083
    },
    {
        "cve_id": "CVE-2016-6786",
        "code_before_change": "int perf_event_refresh(struct perf_event *event, int refresh)\n{\n\t/*\n\t * not supported on inherited events\n\t */\n\tif (event->attr.inherit || !is_sampling_event(event))\n\t\treturn -EINVAL;\n\n\tatomic_add(refresh, &event->event_limit);\n\tperf_event_enable(event);\n\n\treturn 0;\n}",
        "code_after_change": "int perf_event_refresh(struct perf_event *event, int refresh)\n{\n\tstruct perf_event_context *ctx;\n\tint ret;\n\n\tctx = perf_event_ctx_lock(event);\n\tret = _perf_event_refresh(event, refresh);\n\tperf_event_ctx_unlock(event, ctx);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -1,13 +1,11 @@\n int perf_event_refresh(struct perf_event *event, int refresh)\n {\n-\t/*\n-\t * not supported on inherited events\n-\t */\n-\tif (event->attr.inherit || !is_sampling_event(event))\n-\t\treturn -EINVAL;\n+\tstruct perf_event_context *ctx;\n+\tint ret;\n \n-\tatomic_add(refresh, &event->event_limit);\n-\tperf_event_enable(event);\n+\tctx = perf_event_ctx_lock(event);\n+\tret = _perf_event_refresh(event, refresh);\n+\tperf_event_ctx_unlock(event, ctx);\n \n-\treturn 0;\n+\treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct perf_event_context *ctx;",
                "\tint ret;",
                "\tctx = perf_event_ctx_lock(event);",
                "\tret = _perf_event_refresh(event, refresh);",
                "\tperf_event_ctx_unlock(event, ctx);",
                "\treturn ret;"
            ],
            "deleted": [
                "\t/*",
                "\t * not supported on inherited events",
                "\t */",
                "\tif (event->attr.inherit || !is_sampling_event(event))",
                "\t\treturn -EINVAL;",
                "\tatomic_add(refresh, &event->event_limit);",
                "\tperf_event_enable(event);",
                "\treturn 0;"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "kernel/events/core.c in the performance subsystem in the Linux kernel before 4.0 mismanages locks during certain migrations, which allows local users to gain privileges via a crafted application, aka Android internal bug 30955111.",
        "id": 1082
    },
    {
        "cve_id": "CVE-2014-9922",
        "code_before_change": "static int ovl_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct path lowerpath;\n\tstruct path upperpath;\n\tstruct path workpath;\n\tstruct inode *root_inode;\n\tstruct dentry *root_dentry;\n\tstruct ovl_entry *oe;\n\tstruct ovl_fs *ufs;\n\tstruct kstatfs statfs;\n\tint err;\n\n\terr = -ENOMEM;\n\tufs = kzalloc(sizeof(struct ovl_fs), GFP_KERNEL);\n\tif (!ufs)\n\t\tgoto out;\n\n\terr = ovl_parse_opt((char *) data, &ufs->config);\n\tif (err)\n\t\tgoto out_free_config;\n\n\t/* FIXME: workdir is not needed for a R/O mount */\n\terr = -EINVAL;\n\tif (!ufs->config.upperdir || !ufs->config.lowerdir ||\n\t    !ufs->config.workdir) {\n\t\tpr_err(\"overlayfs: missing upperdir or lowerdir or workdir\\n\");\n\t\tgoto out_free_config;\n\t}\n\n\terr = -ENOMEM;\n\toe = ovl_alloc_entry();\n\tif (oe == NULL)\n\t\tgoto out_free_config;\n\n\terr = ovl_mount_dir(ufs->config.upperdir, &upperpath);\n\tif (err)\n\t\tgoto out_free_oe;\n\n\terr = ovl_mount_dir(ufs->config.lowerdir, &lowerpath);\n\tif (err)\n\t\tgoto out_put_upperpath;\n\n\terr = ovl_mount_dir(ufs->config.workdir, &workpath);\n\tif (err)\n\t\tgoto out_put_lowerpath;\n\n\terr = -EINVAL;\n\tif (!S_ISDIR(upperpath.dentry->d_inode->i_mode) ||\n\t    !S_ISDIR(lowerpath.dentry->d_inode->i_mode) ||\n\t    !S_ISDIR(workpath.dentry->d_inode->i_mode)) {\n\t\tpr_err(\"overlayfs: upperdir or lowerdir or workdir not a directory\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (upperpath.mnt != workpath.mnt) {\n\t\tpr_err(\"overlayfs: workdir and upperdir must reside under the same mount\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\tif (!ovl_workdir_ok(workpath.dentry, upperpath.dentry)) {\n\t\tpr_err(\"overlayfs: workdir and upperdir must be separate subtrees\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (!ovl_is_allowed_fs_type(upperpath.dentry)) {\n\t\tpr_err(\"overlayfs: filesystem of upperdir is not supported\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (!ovl_is_allowed_fs_type(lowerpath.dentry)) {\n\t\tpr_err(\"overlayfs: filesystem of lowerdir is not supported\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\terr = vfs_statfs(&lowerpath, &statfs);\n\tif (err) {\n\t\tpr_err(\"overlayfs: statfs failed on lowerpath\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\tufs->lower_namelen = statfs.f_namelen;\n\n\tufs->upper_mnt = clone_private_mount(&upperpath);\n\terr = PTR_ERR(ufs->upper_mnt);\n\tif (IS_ERR(ufs->upper_mnt)) {\n\t\tpr_err(\"overlayfs: failed to clone upperpath\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tufs->lower_mnt = clone_private_mount(&lowerpath);\n\terr = PTR_ERR(ufs->lower_mnt);\n\tif (IS_ERR(ufs->lower_mnt)) {\n\t\tpr_err(\"overlayfs: failed to clone lowerpath\\n\");\n\t\tgoto out_put_upper_mnt;\n\t}\n\n\tufs->workdir = ovl_workdir_create(ufs->upper_mnt, workpath.dentry);\n\terr = PTR_ERR(ufs->workdir);\n\tif (IS_ERR(ufs->workdir)) {\n\t\tpr_err(\"overlayfs: failed to create directory %s/%s\\n\",\n\t\t       ufs->config.workdir, OVL_WORKDIR_NAME);\n\t\tgoto out_put_lower_mnt;\n\t}\n\n\t/*\n\t * Make lower_mnt R/O.  That way fchmod/fchown on lower file\n\t * will fail instead of modifying lower fs.\n\t */\n\tufs->lower_mnt->mnt_flags |= MNT_READONLY;\n\n\t/* If the upper fs is r/o, we mark overlayfs r/o too */\n\tif (ufs->upper_mnt->mnt_sb->s_flags & MS_RDONLY)\n\t\tsb->s_flags |= MS_RDONLY;\n\n\tsb->s_d_op = &ovl_dentry_operations;\n\n\terr = -ENOMEM;\n\troot_inode = ovl_new_inode(sb, S_IFDIR, oe);\n\tif (!root_inode)\n\t\tgoto out_put_workdir;\n\n\troot_dentry = d_make_root(root_inode);\n\tif (!root_dentry)\n\t\tgoto out_put_workdir;\n\n\tmntput(upperpath.mnt);\n\tmntput(lowerpath.mnt);\n\tpath_put(&workpath);\n\n\toe->__upperdentry = upperpath.dentry;\n\toe->lowerdentry = lowerpath.dentry;\n\n\troot_dentry->d_fsdata = oe;\n\n\tsb->s_magic = OVERLAYFS_SUPER_MAGIC;\n\tsb->s_op = &ovl_super_operations;\n\tsb->s_root = root_dentry;\n\tsb->s_fs_info = ufs;\n\n\treturn 0;\n\nout_put_workdir:\n\tdput(ufs->workdir);\nout_put_lower_mnt:\n\tmntput(ufs->lower_mnt);\nout_put_upper_mnt:\n\tmntput(ufs->upper_mnt);\nout_put_workpath:\n\tpath_put(&workpath);\nout_put_lowerpath:\n\tpath_put(&lowerpath);\nout_put_upperpath:\n\tpath_put(&upperpath);\nout_free_oe:\n\tkfree(oe);\nout_free_config:\n\tkfree(ufs->config.lowerdir);\n\tkfree(ufs->config.upperdir);\n\tkfree(ufs->config.workdir);\n\tkfree(ufs);\nout:\n\treturn err;\n}",
        "code_after_change": "static int ovl_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct path lowerpath;\n\tstruct path upperpath;\n\tstruct path workpath;\n\tstruct inode *root_inode;\n\tstruct dentry *root_dentry;\n\tstruct ovl_entry *oe;\n\tstruct ovl_fs *ufs;\n\tstruct kstatfs statfs;\n\tint err;\n\n\terr = -ENOMEM;\n\tufs = kzalloc(sizeof(struct ovl_fs), GFP_KERNEL);\n\tif (!ufs)\n\t\tgoto out;\n\n\terr = ovl_parse_opt((char *) data, &ufs->config);\n\tif (err)\n\t\tgoto out_free_config;\n\n\t/* FIXME: workdir is not needed for a R/O mount */\n\terr = -EINVAL;\n\tif (!ufs->config.upperdir || !ufs->config.lowerdir ||\n\t    !ufs->config.workdir) {\n\t\tpr_err(\"overlayfs: missing upperdir or lowerdir or workdir\\n\");\n\t\tgoto out_free_config;\n\t}\n\n\terr = -ENOMEM;\n\toe = ovl_alloc_entry();\n\tif (oe == NULL)\n\t\tgoto out_free_config;\n\n\terr = ovl_mount_dir(ufs->config.upperdir, &upperpath);\n\tif (err)\n\t\tgoto out_free_oe;\n\n\terr = ovl_mount_dir(ufs->config.lowerdir, &lowerpath);\n\tif (err)\n\t\tgoto out_put_upperpath;\n\n\terr = ovl_mount_dir(ufs->config.workdir, &workpath);\n\tif (err)\n\t\tgoto out_put_lowerpath;\n\n\terr = -EINVAL;\n\tif (!S_ISDIR(upperpath.dentry->d_inode->i_mode) ||\n\t    !S_ISDIR(lowerpath.dentry->d_inode->i_mode) ||\n\t    !S_ISDIR(workpath.dentry->d_inode->i_mode)) {\n\t\tpr_err(\"overlayfs: upperdir or lowerdir or workdir not a directory\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (upperpath.mnt != workpath.mnt) {\n\t\tpr_err(\"overlayfs: workdir and upperdir must reside under the same mount\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\tif (!ovl_workdir_ok(workpath.dentry, upperpath.dentry)) {\n\t\tpr_err(\"overlayfs: workdir and upperdir must be separate subtrees\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (!ovl_is_allowed_fs_type(upperpath.dentry)) {\n\t\tpr_err(\"overlayfs: filesystem of upperdir is not supported\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (!ovl_is_allowed_fs_type(lowerpath.dentry)) {\n\t\tpr_err(\"overlayfs: filesystem of lowerdir is not supported\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\terr = vfs_statfs(&lowerpath, &statfs);\n\tif (err) {\n\t\tpr_err(\"overlayfs: statfs failed on lowerpath\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\tufs->lower_namelen = statfs.f_namelen;\n\n\tsb->s_stack_depth = max(upperpath.mnt->mnt_sb->s_stack_depth,\n\t\t\t\tlowerpath.mnt->mnt_sb->s_stack_depth) + 1;\n\n\terr = -EINVAL;\n\tif (sb->s_stack_depth > FILESYSTEM_MAX_STACK_DEPTH) {\n\t\tpr_err(\"overlayfs: maximum fs stacking depth exceeded\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tufs->upper_mnt = clone_private_mount(&upperpath);\n\terr = PTR_ERR(ufs->upper_mnt);\n\tif (IS_ERR(ufs->upper_mnt)) {\n\t\tpr_err(\"overlayfs: failed to clone upperpath\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tufs->lower_mnt = clone_private_mount(&lowerpath);\n\terr = PTR_ERR(ufs->lower_mnt);\n\tif (IS_ERR(ufs->lower_mnt)) {\n\t\tpr_err(\"overlayfs: failed to clone lowerpath\\n\");\n\t\tgoto out_put_upper_mnt;\n\t}\n\n\tufs->workdir = ovl_workdir_create(ufs->upper_mnt, workpath.dentry);\n\terr = PTR_ERR(ufs->workdir);\n\tif (IS_ERR(ufs->workdir)) {\n\t\tpr_err(\"overlayfs: failed to create directory %s/%s\\n\",\n\t\t       ufs->config.workdir, OVL_WORKDIR_NAME);\n\t\tgoto out_put_lower_mnt;\n\t}\n\n\t/*\n\t * Make lower_mnt R/O.  That way fchmod/fchown on lower file\n\t * will fail instead of modifying lower fs.\n\t */\n\tufs->lower_mnt->mnt_flags |= MNT_READONLY;\n\n\t/* If the upper fs is r/o, we mark overlayfs r/o too */\n\tif (ufs->upper_mnt->mnt_sb->s_flags & MS_RDONLY)\n\t\tsb->s_flags |= MS_RDONLY;\n\n\tsb->s_d_op = &ovl_dentry_operations;\n\n\terr = -ENOMEM;\n\troot_inode = ovl_new_inode(sb, S_IFDIR, oe);\n\tif (!root_inode)\n\t\tgoto out_put_workdir;\n\n\troot_dentry = d_make_root(root_inode);\n\tif (!root_dentry)\n\t\tgoto out_put_workdir;\n\n\tmntput(upperpath.mnt);\n\tmntput(lowerpath.mnt);\n\tpath_put(&workpath);\n\n\toe->__upperdentry = upperpath.dentry;\n\toe->lowerdentry = lowerpath.dentry;\n\n\troot_dentry->d_fsdata = oe;\n\n\tsb->s_magic = OVERLAYFS_SUPER_MAGIC;\n\tsb->s_op = &ovl_super_operations;\n\tsb->s_root = root_dentry;\n\tsb->s_fs_info = ufs;\n\n\treturn 0;\n\nout_put_workdir:\n\tdput(ufs->workdir);\nout_put_lower_mnt:\n\tmntput(ufs->lower_mnt);\nout_put_upper_mnt:\n\tmntput(ufs->upper_mnt);\nout_put_workpath:\n\tpath_put(&workpath);\nout_put_lowerpath:\n\tpath_put(&lowerpath);\nout_put_upperpath:\n\tpath_put(&upperpath);\nout_free_oe:\n\tkfree(oe);\nout_free_config:\n\tkfree(ufs->config.lowerdir);\n\tkfree(ufs->config.upperdir);\n\tkfree(ufs->config.workdir);\n\tkfree(ufs);\nout:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -77,6 +77,15 @@\n \t\tgoto out_put_workpath;\n \t}\n \tufs->lower_namelen = statfs.f_namelen;\n+\n+\tsb->s_stack_depth = max(upperpath.mnt->mnt_sb->s_stack_depth,\n+\t\t\t\tlowerpath.mnt->mnt_sb->s_stack_depth) + 1;\n+\n+\terr = -EINVAL;\n+\tif (sb->s_stack_depth > FILESYSTEM_MAX_STACK_DEPTH) {\n+\t\tpr_err(\"overlayfs: maximum fs stacking depth exceeded\\n\");\n+\t\tgoto out_put_workpath;\n+\t}\n \n \tufs->upper_mnt = clone_private_mount(&upperpath);\n \terr = PTR_ERR(ufs->upper_mnt);",
        "function_modified_lines": {
            "added": [
                "",
                "\tsb->s_stack_depth = max(upperpath.mnt->mnt_sb->s_stack_depth,",
                "\t\t\t\tlowerpath.mnt->mnt_sb->s_stack_depth) + 1;",
                "",
                "\terr = -EINVAL;",
                "\tif (sb->s_stack_depth > FILESYSTEM_MAX_STACK_DEPTH) {",
                "\t\tpr_err(\"overlayfs: maximum fs stacking depth exceeded\\n\");",
                "\t\tgoto out_put_workpath;",
                "\t}"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The eCryptfs subsystem in the Linux kernel before 3.18 allows local users to gain privileges via a large filesystem stack that includes an overlayfs layer, related to fs/ecryptfs/main.c and fs/overlayfs/super.c.",
        "id": 713
    },
    {
        "cve_id": "CVE-2014-9870",
        "code_before_change": "int\ncopy_thread(unsigned long clone_flags, unsigned long stack_start,\n\t    unsigned long stk_sz, struct task_struct *p)\n{\n\tstruct thread_info *thread = task_thread_info(p);\n\tstruct pt_regs *childregs = task_pt_regs(p);\n\n\tmemset(&thread->cpu_context, 0, sizeof(struct cpu_context_save));\n\n\tif (likely(!(p->flags & PF_KTHREAD))) {\n\t\t*childregs = *current_pt_regs();\n\t\tchildregs->ARM_r0 = 0;\n\t\tif (stack_start)\n\t\t\tchildregs->ARM_sp = stack_start;\n\t} else {\n\t\tmemset(childregs, 0, sizeof(struct pt_regs));\n\t\tthread->cpu_context.r4 = stk_sz;\n\t\tthread->cpu_context.r5 = stack_start;\n\t\tchildregs->ARM_cpsr = SVC_MODE;\n\t}\n\tthread->cpu_context.pc = (unsigned long)ret_from_fork;\n\tthread->cpu_context.sp = (unsigned long)childregs;\n\n\tclear_ptrace_hw_breakpoint(p);\n\n\tif (clone_flags & CLONE_SETTLS)\n\t\tthread->tp_value = childregs->ARM_r3;\n\n\tthread_notify(THREAD_NOTIFY_COPY, thread);\n\n\treturn 0;\n}",
        "code_after_change": "int\ncopy_thread(unsigned long clone_flags, unsigned long stack_start,\n\t    unsigned long stk_sz, struct task_struct *p)\n{\n\tstruct thread_info *thread = task_thread_info(p);\n\tstruct pt_regs *childregs = task_pt_regs(p);\n\n\tmemset(&thread->cpu_context, 0, sizeof(struct cpu_context_save));\n\n\tif (likely(!(p->flags & PF_KTHREAD))) {\n\t\t*childregs = *current_pt_regs();\n\t\tchildregs->ARM_r0 = 0;\n\t\tif (stack_start)\n\t\t\tchildregs->ARM_sp = stack_start;\n\t} else {\n\t\tmemset(childregs, 0, sizeof(struct pt_regs));\n\t\tthread->cpu_context.r4 = stk_sz;\n\t\tthread->cpu_context.r5 = stack_start;\n\t\tchildregs->ARM_cpsr = SVC_MODE;\n\t}\n\tthread->cpu_context.pc = (unsigned long)ret_from_fork;\n\tthread->cpu_context.sp = (unsigned long)childregs;\n\n\tclear_ptrace_hw_breakpoint(p);\n\n\tif (clone_flags & CLONE_SETTLS)\n\t\tthread->tp_value[0] = childregs->ARM_r3;\n\tthread->tp_value[1] = get_tpuser();\n\n\tthread_notify(THREAD_NOTIFY_COPY, thread);\n\n\treturn 0;\n}",
        "patch": "--- code before\n+++ code after\n@@ -24,7 +24,8 @@\n \tclear_ptrace_hw_breakpoint(p);\n \n \tif (clone_flags & CLONE_SETTLS)\n-\t\tthread->tp_value = childregs->ARM_r3;\n+\t\tthread->tp_value[0] = childregs->ARM_r3;\n+\tthread->tp_value[1] = get_tpuser();\n \n \tthread_notify(THREAD_NOTIFY_COPY, thread);\n ",
        "function_modified_lines": {
            "added": [
                "\t\tthread->tp_value[0] = childregs->ARM_r3;",
                "\tthread->tp_value[1] = get_tpuser();"
            ],
            "deleted": [
                "\t\tthread->tp_value = childregs->ARM_r3;"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Linux kernel before 3.11 on ARM platforms, as used in Android before 2016-08-05 on Nexus 5 and 7 (2013) devices, does not properly consider user-space access to the TPIDRURW register, which allows local users to gain privileges via a crafted application, aka Android internal bug 28749743 and Qualcomm internal bug CR561044.",
        "id": 702
    },
    {
        "cve_id": "CVE-2013-1858",
        "code_before_change": " */\nSYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)\n{\n\tstruct fs_struct *fs, *new_fs = NULL;\n\tstruct files_struct *fd, *new_fd = NULL;\n\tstruct cred *new_cred = NULL;\n\tstruct nsproxy *new_nsproxy = NULL;\n\tint do_sysvsem = 0;\n\tint err;\n\n\t/*\n\t * If unsharing a user namespace must also unshare the thread.\n\t */\n\tif (unshare_flags & CLONE_NEWUSER)\n\t\tunshare_flags |= CLONE_THREAD;\n\t/*\n\t * If unsharing a pid namespace must also unshare the thread.\n\t */\n\tif (unshare_flags & CLONE_NEWPID)\n\t\tunshare_flags |= CLONE_THREAD;\n\t/*\n\t * If unsharing a thread from a thread group, must also unshare vm.\n\t */\n\tif (unshare_flags & CLONE_THREAD)\n\t\tunshare_flags |= CLONE_VM;\n\t/*\n\t * If unsharing vm, must also unshare signal handlers.\n\t */\n\tif (unshare_flags & CLONE_VM)\n\t\tunshare_flags |= CLONE_SIGHAND;\n\t/*\n\t * If unsharing namespace, must also unshare filesystem information.\n\t */\n\tif (unshare_flags & CLONE_NEWNS)\n\t\tunshare_flags |= CLONE_FS;\n\n\terr = check_unshare_flags(unshare_flags);\n\tif (err)\n\t\tgoto bad_unshare_out;\n\t/*\n\t * CLONE_NEWIPC must also detach from the undolist: after switching\n\t * to a new ipc namespace, the semaphore arrays from the old\n\t * namespace are unreachable.\n\t */\n\tif (unshare_flags & (CLONE_NEWIPC|CLONE_SYSVSEM))\n\t\tdo_sysvsem = 1;\n\terr = unshare_fs(unshare_flags, &new_fs);\n\tif (err)\n\t\tgoto bad_unshare_out;\n\terr = unshare_fd(unshare_flags, &new_fd);\n\tif (err)\n\t\tgoto bad_unshare_cleanup_fs;\n\terr = unshare_userns(unshare_flags, &new_cred);\n\tif (err)\n\t\tgoto bad_unshare_cleanup_fd;\n\terr = unshare_nsproxy_namespaces(unshare_flags, &new_nsproxy,\n\t\t\t\t\t new_cred, new_fs);\n\tif (err)\n\t\tgoto bad_unshare_cleanup_cred;\n\n\tif (new_fs || new_fd || do_sysvsem || new_cred || new_nsproxy) {\n\t\tif (do_sysvsem) {\n\t\t\t/*\n\t\t\t * CLONE_SYSVSEM is equivalent to sys_exit().\n\t\t\t */\n\t\t\texit_sem(current);\n\t\t}\n\n\t\tif (new_nsproxy)\n\t\t\tswitch_task_namespaces(current, new_nsproxy);\n\n\t\ttask_lock(current);\n\n\t\tif (new_fs) {\n\t\t\tfs = current->fs;\n\t\t\tspin_lock(&fs->lock);\n\t\t\tcurrent->fs = new_fs;\n\t\t\tif (--fs->users)\n\t\t\t\tnew_fs = NULL;\n\t\t\telse\n\t\t\t\tnew_fs = fs;\n\t\t\tspin_unlock(&fs->lock);\n\t\t}\n\n\t\tif (new_fd) {\n\t\t\tfd = current->files;\n\t\t\tcurrent->files = new_fd;\n\t\t\tnew_fd = fd;\n\t\t}\n\n\t\ttask_unlock(current);\n\n\t\tif (new_cred) {\n\t\t\t/* Install the new user namespace */\n\t\t\tcommit_creds(new_cred);\n\t\t\tnew_cred = NULL;\n\t\t}\n\t}\n\nbad_unshare_cleanup_cred:\n\tif (new_cred)\n\t\tput_cred(new_cred);\nbad_unshare_cleanup_fd:\n\tif (new_fd)\n\t\tput_files_struct(new_fd);\n\nbad_unshare_cleanup_fs:\n\tif (new_fs)\n\t\tfree_fs_struct(new_fs);\n\nbad_unshare_out:\n\treturn err;\n}",
        "code_after_change": " */\nSYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)\n{\n\tstruct fs_struct *fs, *new_fs = NULL;\n\tstruct files_struct *fd, *new_fd = NULL;\n\tstruct cred *new_cred = NULL;\n\tstruct nsproxy *new_nsproxy = NULL;\n\tint do_sysvsem = 0;\n\tint err;\n\n\t/*\n\t * If unsharing a user namespace must also unshare the thread.\n\t */\n\tif (unshare_flags & CLONE_NEWUSER)\n\t\tunshare_flags |= CLONE_THREAD | CLONE_FS;\n\t/*\n\t * If unsharing a pid namespace must also unshare the thread.\n\t */\n\tif (unshare_flags & CLONE_NEWPID)\n\t\tunshare_flags |= CLONE_THREAD;\n\t/*\n\t * If unsharing a thread from a thread group, must also unshare vm.\n\t */\n\tif (unshare_flags & CLONE_THREAD)\n\t\tunshare_flags |= CLONE_VM;\n\t/*\n\t * If unsharing vm, must also unshare signal handlers.\n\t */\n\tif (unshare_flags & CLONE_VM)\n\t\tunshare_flags |= CLONE_SIGHAND;\n\t/*\n\t * If unsharing namespace, must also unshare filesystem information.\n\t */\n\tif (unshare_flags & CLONE_NEWNS)\n\t\tunshare_flags |= CLONE_FS;\n\n\terr = check_unshare_flags(unshare_flags);\n\tif (err)\n\t\tgoto bad_unshare_out;\n\t/*\n\t * CLONE_NEWIPC must also detach from the undolist: after switching\n\t * to a new ipc namespace, the semaphore arrays from the old\n\t * namespace are unreachable.\n\t */\n\tif (unshare_flags & (CLONE_NEWIPC|CLONE_SYSVSEM))\n\t\tdo_sysvsem = 1;\n\terr = unshare_fs(unshare_flags, &new_fs);\n\tif (err)\n\t\tgoto bad_unshare_out;\n\terr = unshare_fd(unshare_flags, &new_fd);\n\tif (err)\n\t\tgoto bad_unshare_cleanup_fs;\n\terr = unshare_userns(unshare_flags, &new_cred);\n\tif (err)\n\t\tgoto bad_unshare_cleanup_fd;\n\terr = unshare_nsproxy_namespaces(unshare_flags, &new_nsproxy,\n\t\t\t\t\t new_cred, new_fs);\n\tif (err)\n\t\tgoto bad_unshare_cleanup_cred;\n\n\tif (new_fs || new_fd || do_sysvsem || new_cred || new_nsproxy) {\n\t\tif (do_sysvsem) {\n\t\t\t/*\n\t\t\t * CLONE_SYSVSEM is equivalent to sys_exit().\n\t\t\t */\n\t\t\texit_sem(current);\n\t\t}\n\n\t\tif (new_nsproxy)\n\t\t\tswitch_task_namespaces(current, new_nsproxy);\n\n\t\ttask_lock(current);\n\n\t\tif (new_fs) {\n\t\t\tfs = current->fs;\n\t\t\tspin_lock(&fs->lock);\n\t\t\tcurrent->fs = new_fs;\n\t\t\tif (--fs->users)\n\t\t\t\tnew_fs = NULL;\n\t\t\telse\n\t\t\t\tnew_fs = fs;\n\t\t\tspin_unlock(&fs->lock);\n\t\t}\n\n\t\tif (new_fd) {\n\t\t\tfd = current->files;\n\t\t\tcurrent->files = new_fd;\n\t\t\tnew_fd = fd;\n\t\t}\n\n\t\ttask_unlock(current);\n\n\t\tif (new_cred) {\n\t\t\t/* Install the new user namespace */\n\t\t\tcommit_creds(new_cred);\n\t\t\tnew_cred = NULL;\n\t\t}\n\t}\n\nbad_unshare_cleanup_cred:\n\tif (new_cred)\n\t\tput_cred(new_cred);\nbad_unshare_cleanup_fd:\n\tif (new_fd)\n\t\tput_files_struct(new_fd);\n\nbad_unshare_cleanup_fs:\n\tif (new_fs)\n\t\tfree_fs_struct(new_fs);\n\nbad_unshare_out:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -12,7 +12,7 @@\n \t * If unsharing a user namespace must also unshare the thread.\n \t */\n \tif (unshare_flags & CLONE_NEWUSER)\n-\t\tunshare_flags |= CLONE_THREAD;\n+\t\tunshare_flags |= CLONE_THREAD | CLONE_FS;\n \t/*\n \t * If unsharing a pid namespace must also unshare the thread.\n \t */",
        "function_modified_lines": {
            "added": [
                "\t\tunshare_flags |= CLONE_THREAD | CLONE_FS;"
            ],
            "deleted": [
                "\t\tunshare_flags |= CLONE_THREAD;"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The clone system-call implementation in the Linux kernel before 3.8.3 does not properly handle a combination of the CLONE_NEWUSER and CLONE_FS flags, which allows local users to gain privileges by calling chroot and leveraging the sharing of the / directory between a parent process and a child process.",
        "id": 199
    },
    {
        "cve_id": "CVE-2016-6786",
        "code_before_change": "static ssize_t\nperf_read(struct file *file, char __user *buf, size_t count, loff_t *ppos)\n{\n\tstruct perf_event *event = file->private_data;\n\n\treturn perf_read_hw(event, buf, count);\n}",
        "code_after_change": "static ssize_t\nperf_read(struct file *file, char __user *buf, size_t count, loff_t *ppos)\n{\n\tstruct perf_event *event = file->private_data;\n\tstruct perf_event_context *ctx;\n\tint ret;\n\n\tctx = perf_event_ctx_lock(event);\n\tret = perf_read_hw(event, buf, count);\n\tperf_event_ctx_unlock(event, ctx);\n\n\treturn ret;\n}",
        "patch": "--- code before\n+++ code after\n@@ -2,6 +2,12 @@\n perf_read(struct file *file, char __user *buf, size_t count, loff_t *ppos)\n {\n \tstruct perf_event *event = file->private_data;\n+\tstruct perf_event_context *ctx;\n+\tint ret;\n \n-\treturn perf_read_hw(event, buf, count);\n+\tctx = perf_event_ctx_lock(event);\n+\tret = perf_read_hw(event, buf, count);\n+\tperf_event_ctx_unlock(event, ctx);\n+\n+\treturn ret;\n }",
        "function_modified_lines": {
            "added": [
                "\tstruct perf_event_context *ctx;",
                "\tint ret;",
                "\tctx = perf_event_ctx_lock(event);",
                "\tret = perf_read_hw(event, buf, count);",
                "\tperf_event_ctx_unlock(event, ctx);",
                "",
                "\treturn ret;"
            ],
            "deleted": [
                "\treturn perf_read_hw(event, buf, count);"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "kernel/events/core.c in the performance subsystem in the Linux kernel before 4.0 mismanages locks during certain migrations, which allows local users to gain privileges via a crafted application, aka Android internal bug 30955111.",
        "id": 1087
    },
    {
        "cve_id": "CVE-2016-8632",
        "code_before_change": "static int tipc_l2_device_event(struct notifier_block *nb, unsigned long evt,\n\t\t\t\tvoid *ptr)\n{\n\tstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\n\tstruct net *net = dev_net(dev);\n\tstruct tipc_bearer *b;\n\n\tb = rtnl_dereference(dev->tipc_ptr);\n\tif (!b)\n\t\treturn NOTIFY_DONE;\n\n\tb->mtu = dev->mtu;\n\n\tswitch (evt) {\n\tcase NETDEV_CHANGE:\n\t\tif (netif_carrier_ok(dev))\n\t\t\tbreak;\n\tcase NETDEV_UP:\n\t\ttest_and_set_bit_lock(0, &b->up);\n\t\tbreak;\n\tcase NETDEV_GOING_DOWN:\n\t\tclear_bit_unlock(0, &b->up);\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_CHANGEMTU:\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_CHANGEADDR:\n\t\tb->media->raw2addr(b, &b->addr,\n\t\t\t\t   (char *)dev->dev_addr);\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_UNREGISTER:\n\tcase NETDEV_CHANGENAME:\n\t\tbearer_disable(dev_net(dev), b);\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}",
        "code_after_change": "static int tipc_l2_device_event(struct notifier_block *nb, unsigned long evt,\n\t\t\t\tvoid *ptr)\n{\n\tstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\n\tstruct net *net = dev_net(dev);\n\tstruct tipc_bearer *b;\n\n\tb = rtnl_dereference(dev->tipc_ptr);\n\tif (!b)\n\t\treturn NOTIFY_DONE;\n\n\tswitch (evt) {\n\tcase NETDEV_CHANGE:\n\t\tif (netif_carrier_ok(dev))\n\t\t\tbreak;\n\tcase NETDEV_UP:\n\t\ttest_and_set_bit_lock(0, &b->up);\n\t\tbreak;\n\tcase NETDEV_GOING_DOWN:\n\t\tclear_bit_unlock(0, &b->up);\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_CHANGEMTU:\n\t\tif (tipc_mtu_bad(dev, 0)) {\n\t\t\tbearer_disable(net, b);\n\t\t\tbreak;\n\t\t}\n\t\tb->mtu = dev->mtu;\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_CHANGEADDR:\n\t\tb->media->raw2addr(b, &b->addr,\n\t\t\t\t   (char *)dev->dev_addr);\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_UNREGISTER:\n\tcase NETDEV_CHANGENAME:\n\t\tbearer_disable(dev_net(dev), b);\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,8 +8,6 @@\n \tb = rtnl_dereference(dev->tipc_ptr);\n \tif (!b)\n \t\treturn NOTIFY_DONE;\n-\n-\tb->mtu = dev->mtu;\n \n \tswitch (evt) {\n \tcase NETDEV_CHANGE:\n@@ -23,6 +21,11 @@\n \t\ttipc_reset_bearer(net, b);\n \t\tbreak;\n \tcase NETDEV_CHANGEMTU:\n+\t\tif (tipc_mtu_bad(dev, 0)) {\n+\t\t\tbearer_disable(net, b);\n+\t\t\tbreak;\n+\t\t}\n+\t\tb->mtu = dev->mtu;\n \t\ttipc_reset_bearer(net, b);\n \t\tbreak;\n \tcase NETDEV_CHANGEADDR:",
        "function_modified_lines": {
            "added": [
                "\t\tif (tipc_mtu_bad(dev, 0)) {",
                "\t\t\tbearer_disable(net, b);",
                "\t\t\tbreak;",
                "\t\t}",
                "\t\tb->mtu = dev->mtu;"
            ],
            "deleted": [
                "",
                "\tb->mtu = dev->mtu;"
            ]
        },
        "cwe": [
            "CWE-264",
            "CWE-119"
        ],
        "cve_description": "The tipc_msg_build function in net/tipc/msg.c in the Linux kernel through 4.8.11 does not validate the relationship between the minimum fragment length and the maximum packet size, which allows local users to gain privileges or cause a denial of service (heap-based buffer overflow) by leveraging the CAP_NET_ADMIN capability.",
        "id": 1120
    },
    {
        "cve_id": "CVE-2016-4440",
        "code_before_change": "static void vmx_set_msr_bitmap(struct kvm_vcpu *vcpu)\n{\n\tunsigned long *msr_bitmap;\n\n\tif (is_guest_mode(vcpu))\n\t\tmsr_bitmap = vmx_msr_bitmap_nested;\n\telse if (vcpu->arch.apic_base & X2APIC_ENABLE) {\n\t\tif (is_long_mode(vcpu))\n\t\t\tmsr_bitmap = vmx_msr_bitmap_longmode_x2apic;\n\t\telse\n\t\t\tmsr_bitmap = vmx_msr_bitmap_legacy_x2apic;\n\t} else {\n\t\tif (is_long_mode(vcpu))\n\t\t\tmsr_bitmap = vmx_msr_bitmap_longmode;\n\t\telse\n\t\t\tmsr_bitmap = vmx_msr_bitmap_legacy;\n\t}\n\n\tvmcs_write64(MSR_BITMAP, __pa(msr_bitmap));\n}",
        "code_after_change": "static void vmx_set_msr_bitmap(struct kvm_vcpu *vcpu)\n{\n\tunsigned long *msr_bitmap;\n\n\tif (is_guest_mode(vcpu))\n\t\tmsr_bitmap = vmx_msr_bitmap_nested;\n\telse if (cpu_has_secondary_exec_ctrls() &&\n\t\t (vmcs_read32(SECONDARY_VM_EXEC_CONTROL) &\n\t\t  SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE)) {\n\t\tif (is_long_mode(vcpu))\n\t\t\tmsr_bitmap = vmx_msr_bitmap_longmode_x2apic;\n\t\telse\n\t\t\tmsr_bitmap = vmx_msr_bitmap_legacy_x2apic;\n\t} else {\n\t\tif (is_long_mode(vcpu))\n\t\t\tmsr_bitmap = vmx_msr_bitmap_longmode;\n\t\telse\n\t\t\tmsr_bitmap = vmx_msr_bitmap_legacy;\n\t}\n\n\tvmcs_write64(MSR_BITMAP, __pa(msr_bitmap));\n}",
        "patch": "--- code before\n+++ code after\n@@ -4,7 +4,9 @@\n \n \tif (is_guest_mode(vcpu))\n \t\tmsr_bitmap = vmx_msr_bitmap_nested;\n-\telse if (vcpu->arch.apic_base & X2APIC_ENABLE) {\n+\telse if (cpu_has_secondary_exec_ctrls() &&\n+\t\t (vmcs_read32(SECONDARY_VM_EXEC_CONTROL) &\n+\t\t  SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE)) {\n \t\tif (is_long_mode(vcpu))\n \t\t\tmsr_bitmap = vmx_msr_bitmap_longmode_x2apic;\n \t\telse",
        "function_modified_lines": {
            "added": [
                "\telse if (cpu_has_secondary_exec_ctrls() &&",
                "\t\t (vmcs_read32(SECONDARY_VM_EXEC_CONTROL) &",
                "\t\t  SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE)) {"
            ],
            "deleted": [
                "\telse if (vcpu->arch.apic_base & X2APIC_ENABLE) {"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "arch/x86/kvm/vmx.c in the Linux kernel through 4.6.3 mishandles the APICv on/off state, which allows guest OS users to obtain direct APIC MSR access on the host OS, and consequently cause a denial of service (host OS crash) or possibly execute arbitrary code on the host OS, via x2APIC mode.",
        "id": 1015
    },
    {
        "cve_id": "CVE-2016-9644",
        "code_before_change": "static void\ndo_file(char const *const fname)\n{\n\ttable_sort_t custom_sort;\n\tElf32_Ehdr *ehdr = mmap_file(fname);\n\n\tehdr_curr = ehdr;\n\tswitch (ehdr->e_ident[EI_DATA]) {\n\tdefault:\n\t\tfprintf(stderr, \"unrecognized ELF data encoding %d: %s\\n\",\n\t\t\tehdr->e_ident[EI_DATA], fname);\n\t\tfail_file();\n\t\tbreak;\n\tcase ELFDATA2LSB:\n\t\tr = rle;\n\t\tr2 = r2le;\n\t\tr8 = r8le;\n\t\tw = wle;\n\t\tw2 = w2le;\n\t\tw8 = w8le;\n\t\tbreak;\n\tcase ELFDATA2MSB:\n\t\tr = rbe;\n\t\tr2 = r2be;\n\t\tr8 = r8be;\n\t\tw = wbe;\n\t\tw2 = w2be;\n\t\tw8 = w8be;\n\t\tbreak;\n\t}  /* end switch */\n\tif (memcmp(ELFMAG, ehdr->e_ident, SELFMAG) != 0\n\t||  r2(&ehdr->e_type) != ET_EXEC\n\t||  ehdr->e_ident[EI_VERSION] != EV_CURRENT) {\n\t\tfprintf(stderr, \"unrecognized ET_EXEC file %s\\n\", fname);\n\t\tfail_file();\n\t}\n\n\tcustom_sort = NULL;\n\tswitch (r2(&ehdr->e_machine)) {\n\tdefault:\n\t\tfprintf(stderr, \"unrecognized e_machine %d %s\\n\",\n\t\t\tr2(&ehdr->e_machine), fname);\n\t\tfail_file();\n\t\tbreak;\n\tcase EM_386:\n\tcase EM_X86_64:\n\tcase EM_S390:\n\t\tcustom_sort = sort_relative_table;\n\t\tbreak;\n\tcase EM_ARCOMPACT:\n\tcase EM_ARCV2:\n\tcase EM_ARM:\n\tcase EM_AARCH64:\n\tcase EM_MICROBLAZE:\n\tcase EM_MIPS:\n\tcase EM_XTENSA:\n\t\tbreak;\n\t}  /* end switch */\n\n\tswitch (ehdr->e_ident[EI_CLASS]) {\n\tdefault:\n\t\tfprintf(stderr, \"unrecognized ELF class %d %s\\n\",\n\t\t\tehdr->e_ident[EI_CLASS], fname);\n\t\tfail_file();\n\t\tbreak;\n\tcase ELFCLASS32:\n\t\tif (r2(&ehdr->e_ehsize) != sizeof(Elf32_Ehdr)\n\t\t||  r2(&ehdr->e_shentsize) != sizeof(Elf32_Shdr)) {\n\t\t\tfprintf(stderr,\n\t\t\t\t\"unrecognized ET_EXEC file: %s\\n\", fname);\n\t\t\tfail_file();\n\t\t}\n\t\tdo32(ehdr, fname, custom_sort);\n\t\tbreak;\n\tcase ELFCLASS64: {\n\t\tElf64_Ehdr *const ghdr = (Elf64_Ehdr *)ehdr;\n\t\tif (r2(&ghdr->e_ehsize) != sizeof(Elf64_Ehdr)\n\t\t||  r2(&ghdr->e_shentsize) != sizeof(Elf64_Shdr)) {\n\t\t\tfprintf(stderr,\n\t\t\t\t\"unrecognized ET_EXEC file: %s\\n\", fname);\n\t\t\tfail_file();\n\t\t}\n\t\tdo64(ghdr, fname, custom_sort);\n\t\tbreak;\n\t}\n\t}  /* end switch */\n\n\tcleanup();\n}",
        "code_after_change": "static void\ndo_file(char const *const fname)\n{\n\ttable_sort_t custom_sort;\n\tElf32_Ehdr *ehdr = mmap_file(fname);\n\n\tehdr_curr = ehdr;\n\tswitch (ehdr->e_ident[EI_DATA]) {\n\tdefault:\n\t\tfprintf(stderr, \"unrecognized ELF data encoding %d: %s\\n\",\n\t\t\tehdr->e_ident[EI_DATA], fname);\n\t\tfail_file();\n\t\tbreak;\n\tcase ELFDATA2LSB:\n\t\tr = rle;\n\t\tr2 = r2le;\n\t\tr8 = r8le;\n\t\tw = wle;\n\t\tw2 = w2le;\n\t\tw8 = w8le;\n\t\tbreak;\n\tcase ELFDATA2MSB:\n\t\tr = rbe;\n\t\tr2 = r2be;\n\t\tr8 = r8be;\n\t\tw = wbe;\n\t\tw2 = w2be;\n\t\tw8 = w8be;\n\t\tbreak;\n\t}  /* end switch */\n\tif (memcmp(ELFMAG, ehdr->e_ident, SELFMAG) != 0\n\t||  r2(&ehdr->e_type) != ET_EXEC\n\t||  ehdr->e_ident[EI_VERSION] != EV_CURRENT) {\n\t\tfprintf(stderr, \"unrecognized ET_EXEC file %s\\n\", fname);\n\t\tfail_file();\n\t}\n\n\tcustom_sort = NULL;\n\tswitch (r2(&ehdr->e_machine)) {\n\tdefault:\n\t\tfprintf(stderr, \"unrecognized e_machine %d %s\\n\",\n\t\t\tr2(&ehdr->e_machine), fname);\n\t\tfail_file();\n\t\tbreak;\n\tcase EM_386:\n\tcase EM_X86_64:\n\t\tcustom_sort = x86_sort_relative_table;\n\t\tbreak;\n\n\tcase EM_S390:\n\t\tcustom_sort = sort_relative_table;\n\t\tbreak;\n\tcase EM_ARCOMPACT:\n\tcase EM_ARCV2:\n\tcase EM_ARM:\n\tcase EM_AARCH64:\n\tcase EM_MICROBLAZE:\n\tcase EM_MIPS:\n\tcase EM_XTENSA:\n\t\tbreak;\n\t}  /* end switch */\n\n\tswitch (ehdr->e_ident[EI_CLASS]) {\n\tdefault:\n\t\tfprintf(stderr, \"unrecognized ELF class %d %s\\n\",\n\t\t\tehdr->e_ident[EI_CLASS], fname);\n\t\tfail_file();\n\t\tbreak;\n\tcase ELFCLASS32:\n\t\tif (r2(&ehdr->e_ehsize) != sizeof(Elf32_Ehdr)\n\t\t||  r2(&ehdr->e_shentsize) != sizeof(Elf32_Shdr)) {\n\t\t\tfprintf(stderr,\n\t\t\t\t\"unrecognized ET_EXEC file: %s\\n\", fname);\n\t\t\tfail_file();\n\t\t}\n\t\tdo32(ehdr, fname, custom_sort);\n\t\tbreak;\n\tcase ELFCLASS64: {\n\t\tElf64_Ehdr *const ghdr = (Elf64_Ehdr *)ehdr;\n\t\tif (r2(&ghdr->e_ehsize) != sizeof(Elf64_Ehdr)\n\t\t||  r2(&ghdr->e_shentsize) != sizeof(Elf64_Shdr)) {\n\t\t\tfprintf(stderr,\n\t\t\t\t\"unrecognized ET_EXEC file: %s\\n\", fname);\n\t\t\tfail_file();\n\t\t}\n\t\tdo64(ghdr, fname, custom_sort);\n\t\tbreak;\n\t}\n\t}  /* end switch */\n\n\tcleanup();\n}",
        "patch": "--- code before\n+++ code after\n@@ -44,6 +44,9 @@\n \t\tbreak;\n \tcase EM_386:\n \tcase EM_X86_64:\n+\t\tcustom_sort = x86_sort_relative_table;\n+\t\tbreak;\n+\n \tcase EM_S390:\n \t\tcustom_sort = sort_relative_table;\n \t\tbreak;",
        "function_modified_lines": {
            "added": [
                "\t\tcustom_sort = x86_sort_relative_table;",
                "\t\tbreak;",
                ""
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The __get_user_asm_ex macro in arch/x86/include/asm/uaccess.h in the Linux kernel 4.4.22 through 4.4.28 contains extended asm statements that are incompatible with the exception table, which allows local users to obtain root access on non-SMEP platforms via a crafted application.  NOTE: this vulnerability exists because of incorrect backporting of the CVE-2016-9178 patch to older kernels.",
        "id": 1158
    },
    {
        "cve_id": "CVE-2014-0181",
        "code_before_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
        "code_after_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
        "patch": "--- code before\n+++ code after\n@@ -8,7 +8,7 @@\n \tstruct dn_ifaddr __rcu **ifap;\n \tint err = -EINVAL;\n \n-\tif (!capable(CAP_NET_ADMIN))\n+\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n \t\treturn -EPERM;\n \n \tif (!net_eq(net, &init_net))",
        "function_modified_lines": {
            "added": [
                "\tif (!netlink_capable(skb, CAP_NET_ADMIN))"
            ],
            "deleted": [
                "\tif (!capable(CAP_NET_ADMIN))"
            ]
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The Netlink implementation in the Linux kernel through 3.14.1 does not provide a mechanism for authorizing socket operations based on the opener of a socket, which allows local users to bypass intended access restrictions and modify network configurations by using a Netlink socket for the (1) stdout or (2) stderr of a setuid program.",
        "id": 444
    },
    {
        "cve_id": "CVE-2014-8159",
        "code_before_change": "struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,\n\t\t\t    size_t size, int access, int dmasync)\n{\n\tstruct ib_umem *umem;\n\tstruct page **page_list;\n\tstruct vm_area_struct **vma_list;\n\tunsigned long locked;\n\tunsigned long lock_limit;\n\tunsigned long cur_base;\n\tunsigned long npages;\n\tint ret;\n\tint i;\n\tDEFINE_DMA_ATTRS(attrs);\n\tstruct scatterlist *sg, *sg_list_start;\n\tint need_release = 0;\n\n\tif (dmasync)\n\t\tdma_set_attr(DMA_ATTR_WRITE_BARRIER, &attrs);\n\n\tif (!can_do_mlock())\n\t\treturn ERR_PTR(-EPERM);\n\n\tumem = kzalloc(sizeof *umem, GFP_KERNEL);\n\tif (!umem)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tumem->context   = context;\n\tumem->length    = size;\n\tumem->address   = addr;\n\tumem->page_size = PAGE_SIZE;\n\tumem->pid       = get_task_pid(current, PIDTYPE_PID);\n\t/*\n\t * We ask for writable memory if any of the following\n\t * access flags are set.  \"Local write\" and \"remote write\"\n\t * obviously require write access.  \"Remote atomic\" can do\n\t * things like fetch and add, which will modify memory, and\n\t * \"MW bind\" can change permissions by binding a window.\n\t */\n\tumem->writable  = !!(access &\n\t\t(IB_ACCESS_LOCAL_WRITE   | IB_ACCESS_REMOTE_WRITE |\n\t\t IB_ACCESS_REMOTE_ATOMIC | IB_ACCESS_MW_BIND));\n\n\tif (access & IB_ACCESS_ON_DEMAND) {\n\t\tret = ib_umem_odp_get(context, umem);\n\t\tif (ret) {\n\t\t\tkfree(umem);\n\t\t\treturn ERR_PTR(ret);\n\t\t}\n\t\treturn umem;\n\t}\n\n\tumem->odp_data = NULL;\n\n\t/* We assume the memory is from hugetlb until proved otherwise */\n\tumem->hugetlb   = 1;\n\n\tpage_list = (struct page **) __get_free_page(GFP_KERNEL);\n\tif (!page_list) {\n\t\tkfree(umem);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t/*\n\t * if we can't alloc the vma_list, it's not so bad;\n\t * just assume the memory is not hugetlb memory\n\t */\n\tvma_list = (struct vm_area_struct **) __get_free_page(GFP_KERNEL);\n\tif (!vma_list)\n\t\tumem->hugetlb = 0;\n\n\tnpages = ib_umem_num_pages(umem);\n\n\tdown_write(&current->mm->mmap_sem);\n\n\tlocked     = npages + current->mm->pinned_vm;\n\tlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\n\n\tif ((locked > lock_limit) && !capable(CAP_IPC_LOCK)) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tcur_base = addr & PAGE_MASK;\n\n\tif (npages == 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = sg_alloc_table(&umem->sg_head, npages, GFP_KERNEL);\n\tif (ret)\n\t\tgoto out;\n\n\tneed_release = 1;\n\tsg_list_start = umem->sg_head.sgl;\n\n\twhile (npages) {\n\t\tret = get_user_pages(current, current->mm, cur_base,\n\t\t\t\t     min_t(unsigned long, npages,\n\t\t\t\t\t   PAGE_SIZE / sizeof (struct page *)),\n\t\t\t\t     1, !umem->writable, page_list, vma_list);\n\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\n\t\tumem->npages += ret;\n\t\tcur_base += ret * PAGE_SIZE;\n\t\tnpages   -= ret;\n\n\t\tfor_each_sg(sg_list_start, sg, ret, i) {\n\t\t\tif (vma_list && !is_vm_hugetlb_page(vma_list[i]))\n\t\t\t\tumem->hugetlb = 0;\n\n\t\t\tsg_set_page(sg, page_list[i], PAGE_SIZE, 0);\n\t\t}\n\n\t\t/* preparing for next loop */\n\t\tsg_list_start = sg;\n\t}\n\n\tumem->nmap = ib_dma_map_sg_attrs(context->device,\n\t\t\t\t  umem->sg_head.sgl,\n\t\t\t\t  umem->npages,\n\t\t\t\t  DMA_BIDIRECTIONAL,\n\t\t\t\t  &attrs);\n\n\tif (umem->nmap <= 0) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tret = 0;\n\nout:\n\tif (ret < 0) {\n\t\tif (need_release)\n\t\t\t__ib_umem_release(context->device, umem, 0);\n\t\tput_pid(umem->pid);\n\t\tkfree(umem);\n\t} else\n\t\tcurrent->mm->pinned_vm = locked;\n\n\tup_write(&current->mm->mmap_sem);\n\tif (vma_list)\n\t\tfree_page((unsigned long) vma_list);\n\tfree_page((unsigned long) page_list);\n\n\treturn ret < 0 ? ERR_PTR(ret) : umem;\n}",
        "code_after_change": "struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,\n\t\t\t    size_t size, int access, int dmasync)\n{\n\tstruct ib_umem *umem;\n\tstruct page **page_list;\n\tstruct vm_area_struct **vma_list;\n\tunsigned long locked;\n\tunsigned long lock_limit;\n\tunsigned long cur_base;\n\tunsigned long npages;\n\tint ret;\n\tint i;\n\tDEFINE_DMA_ATTRS(attrs);\n\tstruct scatterlist *sg, *sg_list_start;\n\tint need_release = 0;\n\n\tif (dmasync)\n\t\tdma_set_attr(DMA_ATTR_WRITE_BARRIER, &attrs);\n\n\t/*\n\t * If the combination of the addr and size requested for this memory\n\t * region causes an integer overflow, return error.\n\t */\n\tif ((PAGE_ALIGN(addr + size) <= size) ||\n\t    (PAGE_ALIGN(addr + size) <= addr))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!can_do_mlock())\n\t\treturn ERR_PTR(-EPERM);\n\n\tumem = kzalloc(sizeof *umem, GFP_KERNEL);\n\tif (!umem)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tumem->context   = context;\n\tumem->length    = size;\n\tumem->address   = addr;\n\tumem->page_size = PAGE_SIZE;\n\tumem->pid       = get_task_pid(current, PIDTYPE_PID);\n\t/*\n\t * We ask for writable memory if any of the following\n\t * access flags are set.  \"Local write\" and \"remote write\"\n\t * obviously require write access.  \"Remote atomic\" can do\n\t * things like fetch and add, which will modify memory, and\n\t * \"MW bind\" can change permissions by binding a window.\n\t */\n\tumem->writable  = !!(access &\n\t\t(IB_ACCESS_LOCAL_WRITE   | IB_ACCESS_REMOTE_WRITE |\n\t\t IB_ACCESS_REMOTE_ATOMIC | IB_ACCESS_MW_BIND));\n\n\tif (access & IB_ACCESS_ON_DEMAND) {\n\t\tret = ib_umem_odp_get(context, umem);\n\t\tif (ret) {\n\t\t\tkfree(umem);\n\t\t\treturn ERR_PTR(ret);\n\t\t}\n\t\treturn umem;\n\t}\n\n\tumem->odp_data = NULL;\n\n\t/* We assume the memory is from hugetlb until proved otherwise */\n\tumem->hugetlb   = 1;\n\n\tpage_list = (struct page **) __get_free_page(GFP_KERNEL);\n\tif (!page_list) {\n\t\tkfree(umem);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t/*\n\t * if we can't alloc the vma_list, it's not so bad;\n\t * just assume the memory is not hugetlb memory\n\t */\n\tvma_list = (struct vm_area_struct **) __get_free_page(GFP_KERNEL);\n\tif (!vma_list)\n\t\tumem->hugetlb = 0;\n\n\tnpages = ib_umem_num_pages(umem);\n\n\tdown_write(&current->mm->mmap_sem);\n\n\tlocked     = npages + current->mm->pinned_vm;\n\tlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\n\n\tif ((locked > lock_limit) && !capable(CAP_IPC_LOCK)) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tcur_base = addr & PAGE_MASK;\n\n\tif (npages == 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = sg_alloc_table(&umem->sg_head, npages, GFP_KERNEL);\n\tif (ret)\n\t\tgoto out;\n\n\tneed_release = 1;\n\tsg_list_start = umem->sg_head.sgl;\n\n\twhile (npages) {\n\t\tret = get_user_pages(current, current->mm, cur_base,\n\t\t\t\t     min_t(unsigned long, npages,\n\t\t\t\t\t   PAGE_SIZE / sizeof (struct page *)),\n\t\t\t\t     1, !umem->writable, page_list, vma_list);\n\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\n\t\tumem->npages += ret;\n\t\tcur_base += ret * PAGE_SIZE;\n\t\tnpages   -= ret;\n\n\t\tfor_each_sg(sg_list_start, sg, ret, i) {\n\t\t\tif (vma_list && !is_vm_hugetlb_page(vma_list[i]))\n\t\t\t\tumem->hugetlb = 0;\n\n\t\t\tsg_set_page(sg, page_list[i], PAGE_SIZE, 0);\n\t\t}\n\n\t\t/* preparing for next loop */\n\t\tsg_list_start = sg;\n\t}\n\n\tumem->nmap = ib_dma_map_sg_attrs(context->device,\n\t\t\t\t  umem->sg_head.sgl,\n\t\t\t\t  umem->npages,\n\t\t\t\t  DMA_BIDIRECTIONAL,\n\t\t\t\t  &attrs);\n\n\tif (umem->nmap <= 0) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tret = 0;\n\nout:\n\tif (ret < 0) {\n\t\tif (need_release)\n\t\t\t__ib_umem_release(context->device, umem, 0);\n\t\tput_pid(umem->pid);\n\t\tkfree(umem);\n\t} else\n\t\tcurrent->mm->pinned_vm = locked;\n\n\tup_write(&current->mm->mmap_sem);\n\tif (vma_list)\n\t\tfree_page((unsigned long) vma_list);\n\tfree_page((unsigned long) page_list);\n\n\treturn ret < 0 ? ERR_PTR(ret) : umem;\n}",
        "patch": "--- code before\n+++ code after\n@@ -16,6 +16,14 @@\n \n \tif (dmasync)\n \t\tdma_set_attr(DMA_ATTR_WRITE_BARRIER, &attrs);\n+\n+\t/*\n+\t * If the combination of the addr and size requested for this memory\n+\t * region causes an integer overflow, return error.\n+\t */\n+\tif ((PAGE_ALIGN(addr + size) <= size) ||\n+\t    (PAGE_ALIGN(addr + size) <= addr))\n+\t\treturn ERR_PTR(-EINVAL);\n \n \tif (!can_do_mlock())\n \t\treturn ERR_PTR(-EPERM);",
        "function_modified_lines": {
            "added": [
                "",
                "\t/*",
                "\t * If the combination of the addr and size requested for this memory",
                "\t * region causes an integer overflow, return error.",
                "\t */",
                "\tif ((PAGE_ALIGN(addr + size) <= size) ||",
                "\t    (PAGE_ALIGN(addr + size) <= addr))",
                "\t\treturn ERR_PTR(-EINVAL);"
            ],
            "deleted": []
        },
        "cwe": [
            "CWE-264"
        ],
        "cve_description": "The InfiniBand (IB) implementation in the Linux kernel package before 2.6.32-504.12.2 on Red Hat Enterprise Linux (RHEL) 6 does not properly restrict use of User Verbs for registration of memory regions, which allows local users to access arbitrary physical memory locations, and consequently cause a denial of service (system crash) or gain privileges, by leveraging permissions on a uverbs device under /dev/infiniband/.",
        "id": 606
    }
]